#include "kernels_RVV_4x30_fp32.h"



#include <stdio.h>
#include <stdlib.h>

#include <riscv_vector.h>


// gemm_RVV_1x10_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][10, 1] @DRAM
// )
void gemm_RVV_1x10_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
}

// gemm_RVV_1x10_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][10, 1] @DRAM
// )
void gemm_RVV_1x10_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
}

// gemm_RVV_1x11_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][11, 1] @DRAM
// )
void gemm_RVV_1x11_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
}

// gemm_RVV_1x11_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][11, 1] @DRAM
// )
void gemm_RVV_1x11_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
}

// gemm_RVV_1x12_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][12, 1] @DRAM
// )
void gemm_RVV_1x12_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
}

// gemm_RVV_1x12_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][12, 1] @DRAM
// )
void gemm_RVV_1x12_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
}

// gemm_RVV_1x13_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][13, 1] @DRAM
// )
void gemm_RVV_1x13_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
}

// gemm_RVV_1x13_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][13, 1] @DRAM
// )
void gemm_RVV_1x13_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
}

// gemm_RVV_1x14_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][14, 1] @DRAM
// )
void gemm_RVV_1x14_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
}

// gemm_RVV_1x14_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][14, 1] @DRAM
// )
void gemm_RVV_1x14_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
}

// gemm_RVV_1x15_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 15] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][15, 1] @DRAM
// )
void gemm_RVV_1x15_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
}

// gemm_RVV_1x15_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 15] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][15, 1] @DRAM
// )
void gemm_RVV_1x15_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
}

// gemm_RVV_1x16_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 16] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][16, 1] @DRAM
// )
void gemm_RVV_1x16_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
}

// gemm_RVV_1x16_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 16] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][16, 1] @DRAM
// )
void gemm_RVV_1x16_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
}

// gemm_RVV_1x17_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 17] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][17, 1] @DRAM
// )
void gemm_RVV_1x17_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
}

// gemm_RVV_1x17_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 17] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][17, 1] @DRAM
// )
void gemm_RVV_1x17_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
}

// gemm_RVV_1x18_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 18] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][18, 1] @DRAM
// )
void gemm_RVV_1x18_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
}

// gemm_RVV_1x18_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 18] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][18, 1] @DRAM
// )
void gemm_RVV_1x18_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
}

// gemm_RVV_1x19_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 19] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][19, 1] @DRAM
// )
void gemm_RVV_1x19_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
}

// gemm_RVV_1x19_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 19] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][19, 1] @DRAM
// )
void gemm_RVV_1x19_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
}

// gemm_RVV_1x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 1] @DRAM
// )
void gemm_RVV_1x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
}

// gemm_RVV_1x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 1] @DRAM
// )
void gemm_RVV_1x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
}

// gemm_RVV_1x20_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 20] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][20, 1] @DRAM
// )
void gemm_RVV_1x20_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
}

// gemm_RVV_1x20_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 20] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][20, 1] @DRAM
// )
void gemm_RVV_1x20_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
}

// gemm_RVV_1x21_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 21] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][21, 1] @DRAM
// )
void gemm_RVV_1x21_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
}

// gemm_RVV_1x21_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 21] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][21, 1] @DRAM
// )
void gemm_RVV_1x21_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
}

// gemm_RVV_1x22_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 22] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][22, 1] @DRAM
// )
void gemm_RVV_1x22_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
}

// gemm_RVV_1x22_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 22] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][22, 1] @DRAM
// )
void gemm_RVV_1x22_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
}

// gemm_RVV_1x23_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 23] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][23, 1] @DRAM
// )
void gemm_RVV_1x23_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
}

// gemm_RVV_1x23_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 23] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][23, 1] @DRAM
// )
void gemm_RVV_1x23_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(1));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
}

// gemm_RVV_1x24_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 24] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][24, 1] @DRAM
// )
void gemm_RVV_1x24_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
}

// gemm_RVV_1x24_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 24] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][24, 1] @DRAM
// )
void gemm_RVV_1x24_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(1));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(1));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
}

// gemm_RVV_1x25_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 25] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][25, 1] @DRAM
// )
void gemm_RVV_1x25_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
}

// gemm_RVV_1x25_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 25] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][25, 1] @DRAM
// )
void gemm_RVV_1x25_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(1));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(1));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(1));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
}

// gemm_RVV_1x26_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 26] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][26, 1] @DRAM
// )
void gemm_RVV_1x26_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
}

// gemm_RVV_1x26_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 26] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][26, 1] @DRAM
// )
void gemm_RVV_1x26_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(1));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(1));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(1));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(1));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
}

// gemm_RVV_1x27_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 27] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][27, 1] @DRAM
// )
void gemm_RVV_1x27_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(1));
}

// gemm_RVV_1x27_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 27] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][27, 1] @DRAM
// )
void gemm_RVV_1x27_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(1));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(1));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(1));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(1));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(1));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(1));
}

// gemm_RVV_1x28_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 28] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][28, 1] @DRAM
// )
void gemm_RVV_1x28_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(1));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(1));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(1));
}

// gemm_RVV_1x28_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 28] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][28, 1] @DRAM
// )
void gemm_RVV_1x28_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(1));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(1));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(1));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(1));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(1));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(1));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(1));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(1));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(1));
}

// gemm_RVV_1x29_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 29] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][29, 1] @DRAM
// )
void gemm_RVV_1x29_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_28 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(1));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(1));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(1));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(1));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(1));
}

// gemm_RVV_1x29_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 29] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][29, 1] @DRAM
// )
void gemm_RVV_1x29_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(1));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(1));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(1));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(1));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(1));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(1));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(1));
C_reg_28 = __riscv_vle32_v_f32m1(&C.data[(28) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(1));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(1));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(1));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(1));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(1));
}

// gemm_RVV_1x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 1] @DRAM
// )
void gemm_RVV_1x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
}

// gemm_RVV_1x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 1] @DRAM
// )
void gemm_RVV_1x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
}

// gemm_RVV_1x30_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 30] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][30, 1] @DRAM
// )
void gemm_RVV_1x30_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
vfloat32m1_t C_reg_29;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_28 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_29 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(1));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(1));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(1));
  C_reg_29 = __riscv_vfmacc_vf_f32m1(C_reg_29, B.data[(k) * (B.strides[0]) + 29], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(1));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(1));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(1));
__riscv_vse32_v_f32m1(&C.data[(29) * (C.strides[0])], C_reg_29,(1));
}

// gemm_RVV_1x30_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 30] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][30, 1] @DRAM
// )
void gemm_RVV_1x30_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
vfloat32m1_t C_reg_29;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(1));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(1));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(1));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(1));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(1));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(1));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(1));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(1));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(1));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(1));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(1));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(1));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(1));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(1));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(1));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(1));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(1));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(1));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(1));
C_reg_28 = __riscv_vle32_v_f32m1(&C.data[(28) * (C.strides[0])],(1));
C_reg_29 = __riscv_vle32_v_f32m1(&C.data[(29) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(1));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(1));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(1));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(1));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(1));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(1));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(1));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(1));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(1));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(1));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(1));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(1));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(1));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(1));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(1));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(1));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(1));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(1));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(1));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(1));
  C_reg_29 = __riscv_vfmacc_vf_f32m1(C_reg_29, B.data[(k) * (B.strides[0]) + 29], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(1));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(1));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(1));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(1));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(1));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(1));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(1));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(1));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(1));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(1));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(1));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(1));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(1));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(1));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(1));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(1));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(1));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(1));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(1));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(1));
__riscv_vse32_v_f32m1(&C.data[(29) * (C.strides[0])], C_reg_29,(1));
}

// gemm_RVV_1x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 1] @DRAM
// )
void gemm_RVV_1x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
}

// gemm_RVV_1x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 1] @DRAM
// )
void gemm_RVV_1x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
}

// gemm_RVV_1x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 1] @DRAM
// )
void gemm_RVV_1x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
}

// gemm_RVV_1x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 1] @DRAM
// )
void gemm_RVV_1x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
}

// gemm_RVV_1x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 1] @DRAM
// )
void gemm_RVV_1x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
}

// gemm_RVV_1x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 1] @DRAM
// )
void gemm_RVV_1x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
}

// gemm_RVV_1x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 1] @DRAM
// )
void gemm_RVV_1x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
}

// gemm_RVV_1x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 1] @DRAM
// )
void gemm_RVV_1x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
}

// gemm_RVV_1x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 1] @DRAM
// )
void gemm_RVV_1x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
}

// gemm_RVV_1x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 1] @DRAM
// )
void gemm_RVV_1x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
}

// gemm_RVV_1x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 1] @DRAM
// )
void gemm_RVV_1x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
}

// gemm_RVV_1x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 1] @DRAM
// )
void gemm_RVV_1x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
}

// gemm_RVV_1x9_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][9, 1] @DRAM
// )
void gemm_RVV_1x9_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
}

// gemm_RVV_1x9_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][9, 1] @DRAM
// )
void gemm_RVV_1x9_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(1));
}

// gemm_RVV_2x10_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][10, 2] @DRAM
// )
void gemm_RVV_2x10_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
}

// gemm_RVV_2x10_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][10, 2] @DRAM
// )
void gemm_RVV_2x10_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
}

// gemm_RVV_2x11_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][11, 2] @DRAM
// )
void gemm_RVV_2x11_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
}

// gemm_RVV_2x11_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][11, 2] @DRAM
// )
void gemm_RVV_2x11_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
}

// gemm_RVV_2x12_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][12, 2] @DRAM
// )
void gemm_RVV_2x12_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
}

// gemm_RVV_2x12_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][12, 2] @DRAM
// )
void gemm_RVV_2x12_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
}

// gemm_RVV_2x13_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][13, 2] @DRAM
// )
void gemm_RVV_2x13_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
}

// gemm_RVV_2x13_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][13, 2] @DRAM
// )
void gemm_RVV_2x13_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
}

// gemm_RVV_2x14_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][14, 2] @DRAM
// )
void gemm_RVV_2x14_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
}

// gemm_RVV_2x14_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][14, 2] @DRAM
// )
void gemm_RVV_2x14_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
}

// gemm_RVV_2x15_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 15] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][15, 2] @DRAM
// )
void gemm_RVV_2x15_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
}

// gemm_RVV_2x15_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 15] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][15, 2] @DRAM
// )
void gemm_RVV_2x15_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
}

// gemm_RVV_2x16_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 16] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][16, 2] @DRAM
// )
void gemm_RVV_2x16_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
}

// gemm_RVV_2x16_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 16] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][16, 2] @DRAM
// )
void gemm_RVV_2x16_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
}

// gemm_RVV_2x17_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 17] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][17, 2] @DRAM
// )
void gemm_RVV_2x17_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
}

// gemm_RVV_2x17_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 17] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][17, 2] @DRAM
// )
void gemm_RVV_2x17_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
}

// gemm_RVV_2x18_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 18] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][18, 2] @DRAM
// )
void gemm_RVV_2x18_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
}

// gemm_RVV_2x18_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 18] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][18, 2] @DRAM
// )
void gemm_RVV_2x18_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
}

// gemm_RVV_2x19_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 19] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][19, 2] @DRAM
// )
void gemm_RVV_2x19_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
}

// gemm_RVV_2x19_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 19] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][19, 2] @DRAM
// )
void gemm_RVV_2x19_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
}

// gemm_RVV_2x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 2] @DRAM
// )
void gemm_RVV_2x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
}

// gemm_RVV_2x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 2] @DRAM
// )
void gemm_RVV_2x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
}

// gemm_RVV_2x20_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 20] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][20, 2] @DRAM
// )
void gemm_RVV_2x20_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
}

// gemm_RVV_2x20_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 20] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][20, 2] @DRAM
// )
void gemm_RVV_2x20_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
}

// gemm_RVV_2x21_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 21] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][21, 2] @DRAM
// )
void gemm_RVV_2x21_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
}

// gemm_RVV_2x21_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 21] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][21, 2] @DRAM
// )
void gemm_RVV_2x21_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
}

// gemm_RVV_2x22_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 22] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][22, 2] @DRAM
// )
void gemm_RVV_2x22_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
}

// gemm_RVV_2x22_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 22] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][22, 2] @DRAM
// )
void gemm_RVV_2x22_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
}

// gemm_RVV_2x23_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 23] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][23, 2] @DRAM
// )
void gemm_RVV_2x23_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
}

// gemm_RVV_2x23_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 23] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][23, 2] @DRAM
// )
void gemm_RVV_2x23_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(2));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
}

// gemm_RVV_2x24_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 24] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][24, 2] @DRAM
// )
void gemm_RVV_2x24_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
}

// gemm_RVV_2x24_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 24] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][24, 2] @DRAM
// )
void gemm_RVV_2x24_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(2));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(2));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
}

// gemm_RVV_2x25_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 25] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][25, 2] @DRAM
// )
void gemm_RVV_2x25_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
}

// gemm_RVV_2x25_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 25] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][25, 2] @DRAM
// )
void gemm_RVV_2x25_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(2));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(2));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(2));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
}

// gemm_RVV_2x26_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 26] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][26, 2] @DRAM
// )
void gemm_RVV_2x26_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
}

// gemm_RVV_2x26_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 26] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][26, 2] @DRAM
// )
void gemm_RVV_2x26_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(2));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(2));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(2));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(2));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
}

// gemm_RVV_2x27_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 27] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][27, 2] @DRAM
// )
void gemm_RVV_2x27_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(2));
}

// gemm_RVV_2x27_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 27] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][27, 2] @DRAM
// )
void gemm_RVV_2x27_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(2));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(2));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(2));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(2));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(2));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(2));
}

// gemm_RVV_2x28_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 28] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][28, 2] @DRAM
// )
void gemm_RVV_2x28_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(2));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(2));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(2));
}

// gemm_RVV_2x28_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 28] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][28, 2] @DRAM
// )
void gemm_RVV_2x28_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(2));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(2));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(2));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(2));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(2));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(2));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(2));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(2));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(2));
}

// gemm_RVV_2x29_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 29] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][29, 2] @DRAM
// )
void gemm_RVV_2x29_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_28 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(2));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(2));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(2));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(2));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(2));
}

// gemm_RVV_2x29_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 29] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][29, 2] @DRAM
// )
void gemm_RVV_2x29_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(2));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(2));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(2));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(2));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(2));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(2));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(2));
C_reg_28 = __riscv_vle32_v_f32m1(&C.data[(28) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(2));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(2));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(2));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(2));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(2));
}

// gemm_RVV_2x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 2] @DRAM
// )
void gemm_RVV_2x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
}

// gemm_RVV_2x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 2] @DRAM
// )
void gemm_RVV_2x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
}

// gemm_RVV_2x30_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 30] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][30, 2] @DRAM
// )
void gemm_RVV_2x30_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
vfloat32m1_t C_reg_29;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_28 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_29 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(2));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(2));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(2));
  C_reg_29 = __riscv_vfmacc_vf_f32m1(C_reg_29, B.data[(k) * (B.strides[0]) + 29], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(2));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(2));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(2));
__riscv_vse32_v_f32m1(&C.data[(29) * (C.strides[0])], C_reg_29,(2));
}

// gemm_RVV_2x30_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 30] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][30, 2] @DRAM
// )
void gemm_RVV_2x30_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
vfloat32m1_t C_reg_29;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(2));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(2));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(2));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(2));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(2));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(2));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(2));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(2));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(2));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(2));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(2));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(2));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(2));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(2));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(2));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(2));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(2));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(2));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(2));
C_reg_28 = __riscv_vle32_v_f32m1(&C.data[(28) * (C.strides[0])],(2));
C_reg_29 = __riscv_vle32_v_f32m1(&C.data[(29) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(2));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(2));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(2));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(2));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(2));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(2));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(2));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(2));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(2));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(2));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(2));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(2));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(2));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(2));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(2));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(2));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(2));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(2));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(2));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(2));
  C_reg_29 = __riscv_vfmacc_vf_f32m1(C_reg_29, B.data[(k) * (B.strides[0]) + 29], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(2));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(2));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(2));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(2));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(2));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(2));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(2));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(2));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(2));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(2));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(2));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(2));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(2));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(2));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(2));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(2));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(2));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(2));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(2));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(2));
__riscv_vse32_v_f32m1(&C.data[(29) * (C.strides[0])], C_reg_29,(2));
}

// gemm_RVV_2x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 2] @DRAM
// )
void gemm_RVV_2x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
}

// gemm_RVV_2x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 2] @DRAM
// )
void gemm_RVV_2x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
}

// gemm_RVV_2x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 2] @DRAM
// )
void gemm_RVV_2x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
}

// gemm_RVV_2x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 2] @DRAM
// )
void gemm_RVV_2x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
}

// gemm_RVV_2x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 2] @DRAM
// )
void gemm_RVV_2x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
}

// gemm_RVV_2x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 2] @DRAM
// )
void gemm_RVV_2x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
}

// gemm_RVV_2x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 2] @DRAM
// )
void gemm_RVV_2x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
}

// gemm_RVV_2x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 2] @DRAM
// )
void gemm_RVV_2x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
}

// gemm_RVV_2x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 2] @DRAM
// )
void gemm_RVV_2x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
}

// gemm_RVV_2x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 2] @DRAM
// )
void gemm_RVV_2x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
}

// gemm_RVV_2x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 2] @DRAM
// )
void gemm_RVV_2x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
}

// gemm_RVV_2x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 2] @DRAM
// )
void gemm_RVV_2x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
}

// gemm_RVV_2x9_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][9, 2] @DRAM
// )
void gemm_RVV_2x9_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
}

// gemm_RVV_2x9_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][9, 2] @DRAM
// )
void gemm_RVV_2x9_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(2));
}

// gemm_RVV_3x10_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][10, 3] @DRAM
// )
void gemm_RVV_3x10_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
}

// gemm_RVV_3x10_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][10, 3] @DRAM
// )
void gemm_RVV_3x10_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
}

// gemm_RVV_3x11_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][11, 3] @DRAM
// )
void gemm_RVV_3x11_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
}

// gemm_RVV_3x11_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][11, 3] @DRAM
// )
void gemm_RVV_3x11_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
}

// gemm_RVV_3x12_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][12, 3] @DRAM
// )
void gemm_RVV_3x12_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
}

// gemm_RVV_3x12_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][12, 3] @DRAM
// )
void gemm_RVV_3x12_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
}

// gemm_RVV_3x13_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][13, 3] @DRAM
// )
void gemm_RVV_3x13_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
}

// gemm_RVV_3x13_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][13, 3] @DRAM
// )
void gemm_RVV_3x13_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
}

// gemm_RVV_3x14_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][14, 3] @DRAM
// )
void gemm_RVV_3x14_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
}

// gemm_RVV_3x14_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][14, 3] @DRAM
// )
void gemm_RVV_3x14_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
}

// gemm_RVV_3x15_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 15] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][15, 3] @DRAM
// )
void gemm_RVV_3x15_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
}

// gemm_RVV_3x15_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 15] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][15, 3] @DRAM
// )
void gemm_RVV_3x15_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
}

// gemm_RVV_3x16_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 16] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][16, 3] @DRAM
// )
void gemm_RVV_3x16_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
}

// gemm_RVV_3x16_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 16] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][16, 3] @DRAM
// )
void gemm_RVV_3x16_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
}

// gemm_RVV_3x17_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 17] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][17, 3] @DRAM
// )
void gemm_RVV_3x17_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
}

// gemm_RVV_3x17_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 17] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][17, 3] @DRAM
// )
void gemm_RVV_3x17_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
}

// gemm_RVV_3x18_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 18] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][18, 3] @DRAM
// )
void gemm_RVV_3x18_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
}

// gemm_RVV_3x18_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 18] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][18, 3] @DRAM
// )
void gemm_RVV_3x18_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
}

// gemm_RVV_3x19_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 19] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][19, 3] @DRAM
// )
void gemm_RVV_3x19_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
}

// gemm_RVV_3x19_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 19] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][19, 3] @DRAM
// )
void gemm_RVV_3x19_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
}

// gemm_RVV_3x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 3] @DRAM
// )
void gemm_RVV_3x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
}

// gemm_RVV_3x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 3] @DRAM
// )
void gemm_RVV_3x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
}

// gemm_RVV_3x20_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 20] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][20, 3] @DRAM
// )
void gemm_RVV_3x20_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
}

// gemm_RVV_3x20_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 20] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][20, 3] @DRAM
// )
void gemm_RVV_3x20_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
}

// gemm_RVV_3x21_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 21] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][21, 3] @DRAM
// )
void gemm_RVV_3x21_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
}

// gemm_RVV_3x21_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 21] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][21, 3] @DRAM
// )
void gemm_RVV_3x21_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
}

// gemm_RVV_3x22_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 22] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][22, 3] @DRAM
// )
void gemm_RVV_3x22_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
}

// gemm_RVV_3x22_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 22] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][22, 3] @DRAM
// )
void gemm_RVV_3x22_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
}

// gemm_RVV_3x23_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 23] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][23, 3] @DRAM
// )
void gemm_RVV_3x23_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
}

// gemm_RVV_3x23_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 23] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][23, 3] @DRAM
// )
void gemm_RVV_3x23_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(3));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
}

// gemm_RVV_3x24_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 24] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][24, 3] @DRAM
// )
void gemm_RVV_3x24_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
}

// gemm_RVV_3x24_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 24] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][24, 3] @DRAM
// )
void gemm_RVV_3x24_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(3));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(3));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
}

// gemm_RVV_3x25_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 25] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][25, 3] @DRAM
// )
void gemm_RVV_3x25_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
}

// gemm_RVV_3x25_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 25] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][25, 3] @DRAM
// )
void gemm_RVV_3x25_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(3));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(3));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(3));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
}

// gemm_RVV_3x26_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 26] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][26, 3] @DRAM
// )
void gemm_RVV_3x26_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
}

// gemm_RVV_3x26_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 26] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][26, 3] @DRAM
// )
void gemm_RVV_3x26_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(3));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(3));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(3));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(3));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
}

// gemm_RVV_3x27_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 27] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][27, 3] @DRAM
// )
void gemm_RVV_3x27_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(3));
}

// gemm_RVV_3x27_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 27] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][27, 3] @DRAM
// )
void gemm_RVV_3x27_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(3));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(3));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(3));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(3));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(3));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(3));
}

// gemm_RVV_3x28_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 28] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][28, 3] @DRAM
// )
void gemm_RVV_3x28_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(3));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(3));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(3));
}

// gemm_RVV_3x28_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 28] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][28, 3] @DRAM
// )
void gemm_RVV_3x28_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(3));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(3));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(3));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(3));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(3));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(3));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(3));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(3));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(3));
}

// gemm_RVV_3x29_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 29] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][29, 3] @DRAM
// )
void gemm_RVV_3x29_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_28 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(3));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(3));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(3));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(3));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(3));
}

// gemm_RVV_3x29_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 29] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][29, 3] @DRAM
// )
void gemm_RVV_3x29_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(3));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(3));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(3));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(3));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(3));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(3));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(3));
C_reg_28 = __riscv_vle32_v_f32m1(&C.data[(28) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(3));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(3));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(3));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(3));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(3));
}

// gemm_RVV_3x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 3] @DRAM
// )
void gemm_RVV_3x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
}

// gemm_RVV_3x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 3] @DRAM
// )
void gemm_RVV_3x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
}

// gemm_RVV_3x30_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 30] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][30, 3] @DRAM
// )
void gemm_RVV_3x30_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
vfloat32m1_t C_reg_29;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_28 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_29 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(3));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(3));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(3));
  C_reg_29 = __riscv_vfmacc_vf_f32m1(C_reg_29, B.data[(k) * (B.strides[0]) + 29], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(3));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(3));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(3));
__riscv_vse32_v_f32m1(&C.data[(29) * (C.strides[0])], C_reg_29,(3));
}

// gemm_RVV_3x30_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 30] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][30, 3] @DRAM
// )
void gemm_RVV_3x30_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
vfloat32m1_t C_reg_29;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(3));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(3));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(3));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(3));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(3));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(3));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(3));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(3));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(3));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(3));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(3));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(3));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(3));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(3));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(3));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(3));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(3));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(3));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(3));
C_reg_28 = __riscv_vle32_v_f32m1(&C.data[(28) * (C.strides[0])],(3));
C_reg_29 = __riscv_vle32_v_f32m1(&C.data[(29) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(3));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(3));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(3));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(3));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(3));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(3));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(3));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(3));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(3));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(3));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(3));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(3));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(3));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(3));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(3));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(3));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(3));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(3));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(3));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(3));
  C_reg_29 = __riscv_vfmacc_vf_f32m1(C_reg_29, B.data[(k) * (B.strides[0]) + 29], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(3));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(3));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(3));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(3));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(3));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(3));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(3));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(3));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(3));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(3));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(3));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(3));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(3));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(3));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(3));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(3));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(3));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(3));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(3));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(3));
__riscv_vse32_v_f32m1(&C.data[(29) * (C.strides[0])], C_reg_29,(3));
}

// gemm_RVV_3x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 3] @DRAM
// )
void gemm_RVV_3x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
}

// gemm_RVV_3x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 3] @DRAM
// )
void gemm_RVV_3x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
}

// gemm_RVV_3x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 3] @DRAM
// )
void gemm_RVV_3x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
}

// gemm_RVV_3x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 3] @DRAM
// )
void gemm_RVV_3x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
}

// gemm_RVV_3x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 3] @DRAM
// )
void gemm_RVV_3x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
}

// gemm_RVV_3x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 3] @DRAM
// )
void gemm_RVV_3x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
}

// gemm_RVV_3x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 3] @DRAM
// )
void gemm_RVV_3x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
}

// gemm_RVV_3x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 3] @DRAM
// )
void gemm_RVV_3x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
}

// gemm_RVV_3x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 3] @DRAM
// )
void gemm_RVV_3x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
}

// gemm_RVV_3x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 3] @DRAM
// )
void gemm_RVV_3x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
}

// gemm_RVV_3x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 3] @DRAM
// )
void gemm_RVV_3x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
}

// gemm_RVV_3x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 3] @DRAM
// )
void gemm_RVV_3x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
}

// gemm_RVV_3x9_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][9, 3] @DRAM
// )
void gemm_RVV_3x9_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
}

// gemm_RVV_3x9_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][9, 3] @DRAM
// )
void gemm_RVV_3x9_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(3));
}

// gemm_RVV_4x10_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][10, 4] @DRAM
// )
void gemm_RVV_4x10_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
}

// gemm_RVV_4x10_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][10, 4] @DRAM
// )
void gemm_RVV_4x10_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
}

// gemm_RVV_4x11_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][11, 4] @DRAM
// )
void gemm_RVV_4x11_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
}

// gemm_RVV_4x11_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][11, 4] @DRAM
// )
void gemm_RVV_4x11_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
}

// gemm_RVV_4x12_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][12, 4] @DRAM
// )
void gemm_RVV_4x12_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
}

// gemm_RVV_4x12_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][12, 4] @DRAM
// )
void gemm_RVV_4x12_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
}

// gemm_RVV_4x13_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][13, 4] @DRAM
// )
void gemm_RVV_4x13_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
}

// gemm_RVV_4x13_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][13, 4] @DRAM
// )
void gemm_RVV_4x13_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
}

// gemm_RVV_4x14_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][14, 4] @DRAM
// )
void gemm_RVV_4x14_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
}

// gemm_RVV_4x14_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][14, 4] @DRAM
// )
void gemm_RVV_4x14_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
}

// gemm_RVV_4x15_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 15] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][15, 4] @DRAM
// )
void gemm_RVV_4x15_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
}

// gemm_RVV_4x15_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 15] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][15, 4] @DRAM
// )
void gemm_RVV_4x15_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
}

// gemm_RVV_4x16_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 16] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][16, 4] @DRAM
// )
void gemm_RVV_4x16_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
}

// gemm_RVV_4x16_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 16] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][16, 4] @DRAM
// )
void gemm_RVV_4x16_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
}

// gemm_RVV_4x17_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 17] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][17, 4] @DRAM
// )
void gemm_RVV_4x17_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
}

// gemm_RVV_4x17_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 17] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][17, 4] @DRAM
// )
void gemm_RVV_4x17_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
}

// gemm_RVV_4x18_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 18] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][18, 4] @DRAM
// )
void gemm_RVV_4x18_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
}

// gemm_RVV_4x18_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 18] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][18, 4] @DRAM
// )
void gemm_RVV_4x18_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
}

// gemm_RVV_4x19_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 19] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][19, 4] @DRAM
// )
void gemm_RVV_4x19_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
}

// gemm_RVV_4x19_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 19] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][19, 4] @DRAM
// )
void gemm_RVV_4x19_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
}

// gemm_RVV_4x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 4] @DRAM
// )
void gemm_RVV_4x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
}

// gemm_RVV_4x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 4] @DRAM
// )
void gemm_RVV_4x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
}

// gemm_RVV_4x20_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 20] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][20, 4] @DRAM
// )
void gemm_RVV_4x20_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
}

// gemm_RVV_4x20_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 20] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][20, 4] @DRAM
// )
void gemm_RVV_4x20_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
}

// gemm_RVV_4x21_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 21] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][21, 4] @DRAM
// )
void gemm_RVV_4x21_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
}

// gemm_RVV_4x21_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 21] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][21, 4] @DRAM
// )
void gemm_RVV_4x21_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
}

// gemm_RVV_4x22_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 22] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][22, 4] @DRAM
// )
void gemm_RVV_4x22_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
}

// gemm_RVV_4x22_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 22] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][22, 4] @DRAM
// )
void gemm_RVV_4x22_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
}

// gemm_RVV_4x23_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 23] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][23, 4] @DRAM
// )
void gemm_RVV_4x23_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
}

// gemm_RVV_4x23_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 23] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][23, 4] @DRAM
// )
void gemm_RVV_4x23_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(4));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
}

// gemm_RVV_4x24_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 24] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][24, 4] @DRAM
// )
void gemm_RVV_4x24_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
}

// gemm_RVV_4x24_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 24] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][24, 4] @DRAM
// )
void gemm_RVV_4x24_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(4));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(4));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
}

// gemm_RVV_4x25_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 25] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][25, 4] @DRAM
// )
void gemm_RVV_4x25_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
}

// gemm_RVV_4x25_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 25] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][25, 4] @DRAM
// )
void gemm_RVV_4x25_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(4));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(4));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(4));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
}

// gemm_RVV_4x26_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 26] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][26, 4] @DRAM
// )
void gemm_RVV_4x26_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
}

// gemm_RVV_4x26_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 26] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][26, 4] @DRAM
// )
void gemm_RVV_4x26_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(4));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(4));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(4));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(4));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
}

// gemm_RVV_4x27_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 27] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][27, 4] @DRAM
// )
void gemm_RVV_4x27_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(4));
}

// gemm_RVV_4x27_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 27] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][27, 4] @DRAM
// )
void gemm_RVV_4x27_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(4));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(4));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(4));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(4));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(4));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(4));
}

// gemm_RVV_4x28_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 28] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][28, 4] @DRAM
// )
void gemm_RVV_4x28_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(4));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(4));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(4));
}

// gemm_RVV_4x28_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 28] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][28, 4] @DRAM
// )
void gemm_RVV_4x28_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(4));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(4));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(4));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(4));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(4));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(4));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(4));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(4));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(4));
}

// gemm_RVV_4x29_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 29] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][29, 4] @DRAM
// )
void gemm_RVV_4x29_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_28 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(4));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(4));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(4));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(4));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(4));
}

// gemm_RVV_4x29_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 29] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][29, 4] @DRAM
// )
void gemm_RVV_4x29_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(4));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(4));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(4));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(4));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(4));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(4));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(4));
C_reg_28 = __riscv_vle32_v_f32m1(&C.data[(28) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(4));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(4));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(4));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(4));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(4));
}

// gemm_RVV_4x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 4] @DRAM
// )
void gemm_RVV_4x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
}

// gemm_RVV_4x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 4] @DRAM
// )
void gemm_RVV_4x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
}

// gemm_RVV_4x30_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 30] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][30, 4] @DRAM
// )
void gemm_RVV_4x30_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
vfloat32m1_t C_reg_29;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_9 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_10 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_11 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_12 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_13 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_14 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_15 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_16 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_17 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_18 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_19 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_20 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_21 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_22 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_23 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_24 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_25 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_26 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_27 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_28 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_29 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(4));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(4));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(4));
  C_reg_29 = __riscv_vfmacc_vf_f32m1(C_reg_29, B.data[(k) * (B.strides[0]) + 29], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(4));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(4));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(4));
__riscv_vse32_v_f32m1(&C.data[(29) * (C.strides[0])], C_reg_29,(4));
}

// gemm_RVV_4x30_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 30] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][30, 4] @DRAM
// )
void gemm_RVV_4x30_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
vfloat32m1_t C_reg_9;
vfloat32m1_t C_reg_10;
vfloat32m1_t C_reg_11;
vfloat32m1_t C_reg_12;
vfloat32m1_t C_reg_13;
vfloat32m1_t C_reg_14;
vfloat32m1_t C_reg_15;
vfloat32m1_t C_reg_16;
vfloat32m1_t C_reg_17;
vfloat32m1_t C_reg_18;
vfloat32m1_t C_reg_19;
vfloat32m1_t C_reg_20;
vfloat32m1_t C_reg_21;
vfloat32m1_t C_reg_22;
vfloat32m1_t C_reg_23;
vfloat32m1_t C_reg_24;
vfloat32m1_t C_reg_25;
vfloat32m1_t C_reg_26;
vfloat32m1_t C_reg_27;
vfloat32m1_t C_reg_28;
vfloat32m1_t C_reg_29;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
C_reg_9 = __riscv_vle32_v_f32m1(&C.data[(9) * (C.strides[0])],(4));
C_reg_10 = __riscv_vle32_v_f32m1(&C.data[(10) * (C.strides[0])],(4));
C_reg_11 = __riscv_vle32_v_f32m1(&C.data[(11) * (C.strides[0])],(4));
C_reg_12 = __riscv_vle32_v_f32m1(&C.data[(12) * (C.strides[0])],(4));
C_reg_13 = __riscv_vle32_v_f32m1(&C.data[(13) * (C.strides[0])],(4));
C_reg_14 = __riscv_vle32_v_f32m1(&C.data[(14) * (C.strides[0])],(4));
C_reg_15 = __riscv_vle32_v_f32m1(&C.data[(15) * (C.strides[0])],(4));
C_reg_16 = __riscv_vle32_v_f32m1(&C.data[(16) * (C.strides[0])],(4));
C_reg_17 = __riscv_vle32_v_f32m1(&C.data[(17) * (C.strides[0])],(4));
C_reg_18 = __riscv_vle32_v_f32m1(&C.data[(18) * (C.strides[0])],(4));
C_reg_19 = __riscv_vle32_v_f32m1(&C.data[(19) * (C.strides[0])],(4));
C_reg_20 = __riscv_vle32_v_f32m1(&C.data[(20) * (C.strides[0])],(4));
C_reg_21 = __riscv_vle32_v_f32m1(&C.data[(21) * (C.strides[0])],(4));
C_reg_22 = __riscv_vle32_v_f32m1(&C.data[(22) * (C.strides[0])],(4));
C_reg_23 = __riscv_vle32_v_f32m1(&C.data[(23) * (C.strides[0])],(4));
C_reg_24 = __riscv_vle32_v_f32m1(&C.data[(24) * (C.strides[0])],(4));
C_reg_25 = __riscv_vle32_v_f32m1(&C.data[(25) * (C.strides[0])],(4));
C_reg_26 = __riscv_vle32_v_f32m1(&C.data[(26) * (C.strides[0])],(4));
C_reg_27 = __riscv_vle32_v_f32m1(&C.data[(27) * (C.strides[0])],(4));
C_reg_28 = __riscv_vle32_v_f32m1(&C.data[(28) * (C.strides[0])],(4));
C_reg_29 = __riscv_vle32_v_f32m1(&C.data[(29) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
  C_reg_9 = __riscv_vfmacc_vf_f32m1(C_reg_9, B.data[(k) * (B.strides[0]) + 9], A_reg,(4));
  C_reg_10 = __riscv_vfmacc_vf_f32m1(C_reg_10, B.data[(k) * (B.strides[0]) + 10], A_reg,(4));
  C_reg_11 = __riscv_vfmacc_vf_f32m1(C_reg_11, B.data[(k) * (B.strides[0]) + 11], A_reg,(4));
  C_reg_12 = __riscv_vfmacc_vf_f32m1(C_reg_12, B.data[(k) * (B.strides[0]) + 12], A_reg,(4));
  C_reg_13 = __riscv_vfmacc_vf_f32m1(C_reg_13, B.data[(k) * (B.strides[0]) + 13], A_reg,(4));
  C_reg_14 = __riscv_vfmacc_vf_f32m1(C_reg_14, B.data[(k) * (B.strides[0]) + 14], A_reg,(4));
  C_reg_15 = __riscv_vfmacc_vf_f32m1(C_reg_15, B.data[(k) * (B.strides[0]) + 15], A_reg,(4));
  C_reg_16 = __riscv_vfmacc_vf_f32m1(C_reg_16, B.data[(k) * (B.strides[0]) + 16], A_reg,(4));
  C_reg_17 = __riscv_vfmacc_vf_f32m1(C_reg_17, B.data[(k) * (B.strides[0]) + 17], A_reg,(4));
  C_reg_18 = __riscv_vfmacc_vf_f32m1(C_reg_18, B.data[(k) * (B.strides[0]) + 18], A_reg,(4));
  C_reg_19 = __riscv_vfmacc_vf_f32m1(C_reg_19, B.data[(k) * (B.strides[0]) + 19], A_reg,(4));
  C_reg_20 = __riscv_vfmacc_vf_f32m1(C_reg_20, B.data[(k) * (B.strides[0]) + 20], A_reg,(4));
  C_reg_21 = __riscv_vfmacc_vf_f32m1(C_reg_21, B.data[(k) * (B.strides[0]) + 21], A_reg,(4));
  C_reg_22 = __riscv_vfmacc_vf_f32m1(C_reg_22, B.data[(k) * (B.strides[0]) + 22], A_reg,(4));
  C_reg_23 = __riscv_vfmacc_vf_f32m1(C_reg_23, B.data[(k) * (B.strides[0]) + 23], A_reg,(4));
  C_reg_24 = __riscv_vfmacc_vf_f32m1(C_reg_24, B.data[(k) * (B.strides[0]) + 24], A_reg,(4));
  C_reg_25 = __riscv_vfmacc_vf_f32m1(C_reg_25, B.data[(k) * (B.strides[0]) + 25], A_reg,(4));
  C_reg_26 = __riscv_vfmacc_vf_f32m1(C_reg_26, B.data[(k) * (B.strides[0]) + 26], A_reg,(4));
  C_reg_27 = __riscv_vfmacc_vf_f32m1(C_reg_27, B.data[(k) * (B.strides[0]) + 27], A_reg,(4));
  C_reg_28 = __riscv_vfmacc_vf_f32m1(C_reg_28, B.data[(k) * (B.strides[0]) + 28], A_reg,(4));
  C_reg_29 = __riscv_vfmacc_vf_f32m1(C_reg_29, B.data[(k) * (B.strides[0]) + 29], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
__riscv_vse32_v_f32m1(&C.data[(9) * (C.strides[0])], C_reg_9,(4));
__riscv_vse32_v_f32m1(&C.data[(10) * (C.strides[0])], C_reg_10,(4));
__riscv_vse32_v_f32m1(&C.data[(11) * (C.strides[0])], C_reg_11,(4));
__riscv_vse32_v_f32m1(&C.data[(12) * (C.strides[0])], C_reg_12,(4));
__riscv_vse32_v_f32m1(&C.data[(13) * (C.strides[0])], C_reg_13,(4));
__riscv_vse32_v_f32m1(&C.data[(14) * (C.strides[0])], C_reg_14,(4));
__riscv_vse32_v_f32m1(&C.data[(15) * (C.strides[0])], C_reg_15,(4));
__riscv_vse32_v_f32m1(&C.data[(16) * (C.strides[0])], C_reg_16,(4));
__riscv_vse32_v_f32m1(&C.data[(17) * (C.strides[0])], C_reg_17,(4));
__riscv_vse32_v_f32m1(&C.data[(18) * (C.strides[0])], C_reg_18,(4));
__riscv_vse32_v_f32m1(&C.data[(19) * (C.strides[0])], C_reg_19,(4));
__riscv_vse32_v_f32m1(&C.data[(20) * (C.strides[0])], C_reg_20,(4));
__riscv_vse32_v_f32m1(&C.data[(21) * (C.strides[0])], C_reg_21,(4));
__riscv_vse32_v_f32m1(&C.data[(22) * (C.strides[0])], C_reg_22,(4));
__riscv_vse32_v_f32m1(&C.data[(23) * (C.strides[0])], C_reg_23,(4));
__riscv_vse32_v_f32m1(&C.data[(24) * (C.strides[0])], C_reg_24,(4));
__riscv_vse32_v_f32m1(&C.data[(25) * (C.strides[0])], C_reg_25,(4));
__riscv_vse32_v_f32m1(&C.data[(26) * (C.strides[0])], C_reg_26,(4));
__riscv_vse32_v_f32m1(&C.data[(27) * (C.strides[0])], C_reg_27,(4));
__riscv_vse32_v_f32m1(&C.data[(28) * (C.strides[0])], C_reg_28,(4));
__riscv_vse32_v_f32m1(&C.data[(29) * (C.strides[0])], C_reg_29,(4));
}

// gemm_RVV_4x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 4] @DRAM
// )
void gemm_RVV_4x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
}

// gemm_RVV_4x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 4] @DRAM
// )
void gemm_RVV_4x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
}

// gemm_RVV_4x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 4] @DRAM
// )
void gemm_RVV_4x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
}

// gemm_RVV_4x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 4] @DRAM
// )
void gemm_RVV_4x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
}

// gemm_RVV_4x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 4] @DRAM
// )
void gemm_RVV_4x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
}

// gemm_RVV_4x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 4] @DRAM
// )
void gemm_RVV_4x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
}

// gemm_RVV_4x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 4] @DRAM
// )
void gemm_RVV_4x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
}

// gemm_RVV_4x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 4] @DRAM
// )
void gemm_RVV_4x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
}

// gemm_RVV_4x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 4] @DRAM
// )
void gemm_RVV_4x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
}

// gemm_RVV_4x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 4] @DRAM
// )
void gemm_RVV_4x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
}

// gemm_RVV_4x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 4] @DRAM
// )
void gemm_RVV_4x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
}

// gemm_RVV_4x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 4] @DRAM
// )
void gemm_RVV_4x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
}

// gemm_RVV_4x9_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][9, 4] @DRAM
// )
void gemm_RVV_4x9_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_8 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
}

// gemm_RVV_4x9_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][9, 4] @DRAM
// )
void gemm_RVV_4x9_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
vfloat32m1_t C_reg_8;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
C_reg_8 = __riscv_vle32_v_f32m1(&C.data[(8) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
  C_reg_8 = __riscv_vfmacc_vf_f32m1(C_reg_8, B.data[(k) * (B.strides[0]) + 8], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
__riscv_vse32_v_f32m1(&C.data[(8) * (C.strides[0])], C_reg_8,(4));
}


/* relying on the following instruction..."
rvv_broadcast_4xf32_0(dst,vl)
{dst_data} = __riscv_vfmv_v_f_f32m1(0.0f,{vl});
*/

/* relying on the following instruction..."
rvv_vfmacc_4xf32_1xf32(dst,lhs,rhs,vl)
{dst_data} = __riscv_vfmacc_vf_f32m1({dst_data}, {rhs_data}, {lhs_data},{vl});
*/

/* relying on the following instruction..."
rvv_vld_4xf32(dst,src,vl)
{dst_data} = __riscv_vle32_v_f32m1(&{src_data},{vl});
*/

/* relying on the following instruction..."
rvv_vst_4xf32(dst,src,vl)
__riscv_vse32_v_f32m1(&{dst_data}, {src_data},{vl});
*/
