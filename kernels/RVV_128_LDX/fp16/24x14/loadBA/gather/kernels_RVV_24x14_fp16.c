#include "kernels_RVV_24x14_fp16.h"



#include <stdio.h>
#include <stdlib.h>

#include <riscv_vector.h>


// gemm_RVV_10x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 10] @DRAM
// )
void gemm_RVV_10x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
}

// gemm_RVV_10x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 10] @DRAM
// )
void gemm_RVV_10x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
}

// gemm_RVV_10x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 10] @DRAM
// )
void gemm_RVV_10x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(2));
}

// gemm_RVV_10x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 10] @DRAM
// )
void gemm_RVV_10x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(2));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(2));
}

// gemm_RVV_10x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 10] @DRAM
// )
void gemm_RVV_10x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(2));
}

// gemm_RVV_10x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 10] @DRAM
// )
void gemm_RVV_10x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(2));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(2));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(2));
}

// gemm_RVV_10x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 10] @DRAM
// )
void gemm_RVV_10x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(2));
}

// gemm_RVV_10x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 10] @DRAM
// )
void gemm_RVV_10x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(2));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(2));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(2));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(2));
}

// gemm_RVV_10x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 10] @DRAM
// )
void gemm_RVV_10x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(2));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(2));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(2));
}

// gemm_RVV_10x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 10] @DRAM
// )
void gemm_RVV_10x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(2));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(2));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(2));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(2));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(2));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(2));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(2));
}

// gemm_RVV_10x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 10] @DRAM
// )
void gemm_RVV_10x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
}

// gemm_RVV_10x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 10] @DRAM
// )
void gemm_RVV_10x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
}

// gemm_RVV_10x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 10] @DRAM
// )
void gemm_RVV_10x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
}

// gemm_RVV_10x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 10] @DRAM
// )
void gemm_RVV_10x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
}

// gemm_RVV_10x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 10] @DRAM
// )
void gemm_RVV_10x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
}

// gemm_RVV_10x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 10] @DRAM
// )
void gemm_RVV_10x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
}

// gemm_RVV_10x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 10] @DRAM
// )
void gemm_RVV_10x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
}

// gemm_RVV_10x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 10] @DRAM
// )
void gemm_RVV_10x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
}

// gemm_RVV_10x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 10] @DRAM
// )
void gemm_RVV_10x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(2));
}

// gemm_RVV_10x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 10] @DRAM
// )
void gemm_RVV_10x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(2));
}

// gemm_RVV_10x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 10] @DRAM
// )
void gemm_RVV_10x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(2));
}

// gemm_RVV_10x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 10] @DRAM
// )
void gemm_RVV_10x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(2));
}

// gemm_RVV_10x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 10] @DRAM
// )
void gemm_RVV_10x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(2));
}

// gemm_RVV_10x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 10] @DRAM
// )
void gemm_RVV_10x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(2));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(2));
}

// gemm_RVV_10x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 10] @DRAM
// )
void gemm_RVV_10x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(2));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(2));
}

// gemm_RVV_10x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 10] @DRAM
// )
void gemm_RVV_10x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(2));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(2));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(2));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(2));
}

// gemm_RVV_10x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 10] @DRAM
// )
void gemm_RVV_10x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
}

// gemm_RVV_10x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 10] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 10] @DRAM
// )
void gemm_RVV_10x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(2));
}

// gemm_RVV_11x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 11] @DRAM
// )
void gemm_RVV_11x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
}

// gemm_RVV_11x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 11] @DRAM
// )
void gemm_RVV_11x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
}

// gemm_RVV_11x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 11] @DRAM
// )
void gemm_RVV_11x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(3));
}

// gemm_RVV_11x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 11] @DRAM
// )
void gemm_RVV_11x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(3));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(3));
}

// gemm_RVV_11x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 11] @DRAM
// )
void gemm_RVV_11x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(3));
}

// gemm_RVV_11x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 11] @DRAM
// )
void gemm_RVV_11x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(3));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(3));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(3));
}

// gemm_RVV_11x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 11] @DRAM
// )
void gemm_RVV_11x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(3));
}

// gemm_RVV_11x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 11] @DRAM
// )
void gemm_RVV_11x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(3));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(3));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(3));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(3));
}

// gemm_RVV_11x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 11] @DRAM
// )
void gemm_RVV_11x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(3));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(3));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(3));
}

// gemm_RVV_11x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 11] @DRAM
// )
void gemm_RVV_11x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(3));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(3));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(3));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(3));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(3));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(3));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(3));
}

// gemm_RVV_11x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 11] @DRAM
// )
void gemm_RVV_11x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
}

// gemm_RVV_11x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 11] @DRAM
// )
void gemm_RVV_11x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
}

// gemm_RVV_11x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 11] @DRAM
// )
void gemm_RVV_11x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
}

// gemm_RVV_11x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 11] @DRAM
// )
void gemm_RVV_11x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
}

// gemm_RVV_11x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 11] @DRAM
// )
void gemm_RVV_11x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
}

// gemm_RVV_11x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 11] @DRAM
// )
void gemm_RVV_11x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
}

// gemm_RVV_11x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 11] @DRAM
// )
void gemm_RVV_11x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
}

// gemm_RVV_11x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 11] @DRAM
// )
void gemm_RVV_11x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
}

// gemm_RVV_11x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 11] @DRAM
// )
void gemm_RVV_11x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(3));
}

// gemm_RVV_11x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 11] @DRAM
// )
void gemm_RVV_11x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(3));
}

// gemm_RVV_11x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 11] @DRAM
// )
void gemm_RVV_11x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(3));
}

// gemm_RVV_11x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 11] @DRAM
// )
void gemm_RVV_11x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(3));
}

// gemm_RVV_11x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 11] @DRAM
// )
void gemm_RVV_11x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(3));
}

// gemm_RVV_11x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 11] @DRAM
// )
void gemm_RVV_11x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(3));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(3));
}

// gemm_RVV_11x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 11] @DRAM
// )
void gemm_RVV_11x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(3));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(3));
}

// gemm_RVV_11x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 11] @DRAM
// )
void gemm_RVV_11x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(3));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(3));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(3));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(3));
}

// gemm_RVV_11x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 11] @DRAM
// )
void gemm_RVV_11x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
}

// gemm_RVV_11x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 11] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 11] @DRAM
// )
void gemm_RVV_11x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(3));
}

// gemm_RVV_12x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 12] @DRAM
// )
void gemm_RVV_12x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
}

// gemm_RVV_12x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 12] @DRAM
// )
void gemm_RVV_12x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
}

// gemm_RVV_12x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 12] @DRAM
// )
void gemm_RVV_12x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(4));
}

// gemm_RVV_12x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 12] @DRAM
// )
void gemm_RVV_12x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(4));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(4));
}

// gemm_RVV_12x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 12] @DRAM
// )
void gemm_RVV_12x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(4));
}

// gemm_RVV_12x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 12] @DRAM
// )
void gemm_RVV_12x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(4));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(4));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(4));
}

// gemm_RVV_12x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 12] @DRAM
// )
void gemm_RVV_12x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(4));
}

// gemm_RVV_12x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 12] @DRAM
// )
void gemm_RVV_12x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(4));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(4));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(4));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(4));
}

// gemm_RVV_12x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 12] @DRAM
// )
void gemm_RVV_12x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(4));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(4));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(4));
}

// gemm_RVV_12x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 12] @DRAM
// )
void gemm_RVV_12x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(4));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(4));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(4));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(4));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(4));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(4));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(4));
}

// gemm_RVV_12x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 12] @DRAM
// )
void gemm_RVV_12x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
}

// gemm_RVV_12x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 12] @DRAM
// )
void gemm_RVV_12x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
}

// gemm_RVV_12x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 12] @DRAM
// )
void gemm_RVV_12x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
}

// gemm_RVV_12x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 12] @DRAM
// )
void gemm_RVV_12x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
}

// gemm_RVV_12x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 12] @DRAM
// )
void gemm_RVV_12x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
}

// gemm_RVV_12x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 12] @DRAM
// )
void gemm_RVV_12x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
}

// gemm_RVV_12x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 12] @DRAM
// )
void gemm_RVV_12x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
}

// gemm_RVV_12x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 12] @DRAM
// )
void gemm_RVV_12x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
}

// gemm_RVV_12x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 12] @DRAM
// )
void gemm_RVV_12x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(4));
}

// gemm_RVV_12x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 12] @DRAM
// )
void gemm_RVV_12x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(4));
}

// gemm_RVV_12x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 12] @DRAM
// )
void gemm_RVV_12x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(4));
}

// gemm_RVV_12x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 12] @DRAM
// )
void gemm_RVV_12x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(4));
}

// gemm_RVV_12x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 12] @DRAM
// )
void gemm_RVV_12x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(4));
}

// gemm_RVV_12x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 12] @DRAM
// )
void gemm_RVV_12x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(4));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(4));
}

// gemm_RVV_12x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 12] @DRAM
// )
void gemm_RVV_12x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(4));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(4));
}

// gemm_RVV_12x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 12] @DRAM
// )
void gemm_RVV_12x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(4));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(4));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(4));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(4));
}

// gemm_RVV_12x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 12] @DRAM
// )
void gemm_RVV_12x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
}

// gemm_RVV_12x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 12] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 12] @DRAM
// )
void gemm_RVV_12x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(4));
}

// gemm_RVV_13x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 13] @DRAM
// )
void gemm_RVV_13x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
}

// gemm_RVV_13x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 13] @DRAM
// )
void gemm_RVV_13x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
}

// gemm_RVV_13x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 13] @DRAM
// )
void gemm_RVV_13x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(5));
}

// gemm_RVV_13x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 13] @DRAM
// )
void gemm_RVV_13x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(5));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(5));
}

// gemm_RVV_13x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 13] @DRAM
// )
void gemm_RVV_13x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(5));
}

// gemm_RVV_13x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 13] @DRAM
// )
void gemm_RVV_13x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(5));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(5));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(5));
}

// gemm_RVV_13x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 13] @DRAM
// )
void gemm_RVV_13x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(5));
}

// gemm_RVV_13x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 13] @DRAM
// )
void gemm_RVV_13x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(5));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(5));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(5));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(5));
}

// gemm_RVV_13x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 13] @DRAM
// )
void gemm_RVV_13x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(5));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(5));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(5));
}

// gemm_RVV_13x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 13] @DRAM
// )
void gemm_RVV_13x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(5));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(5));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(5));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(5));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(5));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(5));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(5));
}

// gemm_RVV_13x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 13] @DRAM
// )
void gemm_RVV_13x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
}

// gemm_RVV_13x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 13] @DRAM
// )
void gemm_RVV_13x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
}

// gemm_RVV_13x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 13] @DRAM
// )
void gemm_RVV_13x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
}

// gemm_RVV_13x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 13] @DRAM
// )
void gemm_RVV_13x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
}

// gemm_RVV_13x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 13] @DRAM
// )
void gemm_RVV_13x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
}

// gemm_RVV_13x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 13] @DRAM
// )
void gemm_RVV_13x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
}

// gemm_RVV_13x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 13] @DRAM
// )
void gemm_RVV_13x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
}

// gemm_RVV_13x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 13] @DRAM
// )
void gemm_RVV_13x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
}

// gemm_RVV_13x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 13] @DRAM
// )
void gemm_RVV_13x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(5));
}

// gemm_RVV_13x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 13] @DRAM
// )
void gemm_RVV_13x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(5));
}

// gemm_RVV_13x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 13] @DRAM
// )
void gemm_RVV_13x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(5));
}

// gemm_RVV_13x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 13] @DRAM
// )
void gemm_RVV_13x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(5));
}

// gemm_RVV_13x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 13] @DRAM
// )
void gemm_RVV_13x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(5));
}

// gemm_RVV_13x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 13] @DRAM
// )
void gemm_RVV_13x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(5));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(5));
}

// gemm_RVV_13x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 13] @DRAM
// )
void gemm_RVV_13x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(5));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(5));
}

// gemm_RVV_13x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 13] @DRAM
// )
void gemm_RVV_13x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(5));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(5));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(5));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(5));
}

// gemm_RVV_13x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 13] @DRAM
// )
void gemm_RVV_13x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
}

// gemm_RVV_13x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 13] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 13] @DRAM
// )
void gemm_RVV_13x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(5));
}

// gemm_RVV_14x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 14] @DRAM
// )
void gemm_RVV_14x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
}

// gemm_RVV_14x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 14] @DRAM
// )
void gemm_RVV_14x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
}

// gemm_RVV_14x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 14] @DRAM
// )
void gemm_RVV_14x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(6));
}

// gemm_RVV_14x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 14] @DRAM
// )
void gemm_RVV_14x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(6));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(6));
}

// gemm_RVV_14x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 14] @DRAM
// )
void gemm_RVV_14x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(6));
}

// gemm_RVV_14x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 14] @DRAM
// )
void gemm_RVV_14x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(6));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(6));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(6));
}

// gemm_RVV_14x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 14] @DRAM
// )
void gemm_RVV_14x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(6));
}

// gemm_RVV_14x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 14] @DRAM
// )
void gemm_RVV_14x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(6));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(6));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(6));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(6));
}

// gemm_RVV_14x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 14] @DRAM
// )
void gemm_RVV_14x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(6));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(6));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(6));
}

// gemm_RVV_14x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 14] @DRAM
// )
void gemm_RVV_14x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(6));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(6));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(6));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(6));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(6));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(6));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(6));
}

// gemm_RVV_14x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 14] @DRAM
// )
void gemm_RVV_14x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
}

// gemm_RVV_14x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 14] @DRAM
// )
void gemm_RVV_14x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
}

// gemm_RVV_14x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 14] @DRAM
// )
void gemm_RVV_14x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
}

// gemm_RVV_14x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 14] @DRAM
// )
void gemm_RVV_14x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
}

// gemm_RVV_14x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 14] @DRAM
// )
void gemm_RVV_14x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
}

// gemm_RVV_14x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 14] @DRAM
// )
void gemm_RVV_14x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
}

// gemm_RVV_14x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 14] @DRAM
// )
void gemm_RVV_14x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
}

// gemm_RVV_14x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 14] @DRAM
// )
void gemm_RVV_14x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
}

// gemm_RVV_14x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 14] @DRAM
// )
void gemm_RVV_14x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(6));
}

// gemm_RVV_14x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 14] @DRAM
// )
void gemm_RVV_14x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(6));
}

// gemm_RVV_14x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 14] @DRAM
// )
void gemm_RVV_14x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(6));
}

// gemm_RVV_14x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 14] @DRAM
// )
void gemm_RVV_14x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(6));
}

// gemm_RVV_14x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 14] @DRAM
// )
void gemm_RVV_14x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(6));
}

// gemm_RVV_14x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 14] @DRAM
// )
void gemm_RVV_14x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(6));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(6));
}

// gemm_RVV_14x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 14] @DRAM
// )
void gemm_RVV_14x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(6));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(6));
}

// gemm_RVV_14x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 14] @DRAM
// )
void gemm_RVV_14x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(6));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(6));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(6));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(6));
}

// gemm_RVV_14x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 14] @DRAM
// )
void gemm_RVV_14x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
}

// gemm_RVV_14x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 14] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 14] @DRAM
// )
void gemm_RVV_14x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(6));
}

// gemm_RVV_15x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 15] @DRAM
// )
void gemm_RVV_15x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
}

// gemm_RVV_15x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 15] @DRAM
// )
void gemm_RVV_15x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
}

// gemm_RVV_15x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 15] @DRAM
// )
void gemm_RVV_15x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(7));
}

// gemm_RVV_15x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 15] @DRAM
// )
void gemm_RVV_15x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(7));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(7));
}

// gemm_RVV_15x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 15] @DRAM
// )
void gemm_RVV_15x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(7));
}

// gemm_RVV_15x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 15] @DRAM
// )
void gemm_RVV_15x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(7));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(7));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(7));
}

// gemm_RVV_15x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 15] @DRAM
// )
void gemm_RVV_15x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(7));
}

// gemm_RVV_15x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 15] @DRAM
// )
void gemm_RVV_15x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(7));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(7));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(7));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(7));
}

// gemm_RVV_15x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 15] @DRAM
// )
void gemm_RVV_15x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(7));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(7));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(7));
}

// gemm_RVV_15x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 15] @DRAM
// )
void gemm_RVV_15x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(7));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(7));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(7));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(7));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(7));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(7));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(7));
}

// gemm_RVV_15x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 15] @DRAM
// )
void gemm_RVV_15x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
}

// gemm_RVV_15x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 15] @DRAM
// )
void gemm_RVV_15x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
}

// gemm_RVV_15x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 15] @DRAM
// )
void gemm_RVV_15x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
}

// gemm_RVV_15x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 15] @DRAM
// )
void gemm_RVV_15x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
}

// gemm_RVV_15x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 15] @DRAM
// )
void gemm_RVV_15x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
}

// gemm_RVV_15x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 15] @DRAM
// )
void gemm_RVV_15x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
}

// gemm_RVV_15x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 15] @DRAM
// )
void gemm_RVV_15x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
}

// gemm_RVV_15x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 15] @DRAM
// )
void gemm_RVV_15x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
}

// gemm_RVV_15x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 15] @DRAM
// )
void gemm_RVV_15x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(7));
}

// gemm_RVV_15x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 15] @DRAM
// )
void gemm_RVV_15x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(7));
}

// gemm_RVV_15x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 15] @DRAM
// )
void gemm_RVV_15x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(7));
}

// gemm_RVV_15x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 15] @DRAM
// )
void gemm_RVV_15x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(7));
}

// gemm_RVV_15x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 15] @DRAM
// )
void gemm_RVV_15x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(7));
}

// gemm_RVV_15x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 15] @DRAM
// )
void gemm_RVV_15x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(7));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(7));
}

// gemm_RVV_15x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 15] @DRAM
// )
void gemm_RVV_15x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(7));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(7));
}

// gemm_RVV_15x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 15] @DRAM
// )
void gemm_RVV_15x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(7));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(7));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(7));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(7));
}

// gemm_RVV_15x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 15] @DRAM
// )
void gemm_RVV_15x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
}

// gemm_RVV_15x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 15] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 15] @DRAM
// )
void gemm_RVV_15x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(7));
}

// gemm_RVV_16x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 16] @DRAM
// )
void gemm_RVV_16x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
}

// gemm_RVV_16x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 16] @DRAM
// )
void gemm_RVV_16x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
}

// gemm_RVV_16x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 16] @DRAM
// )
void gemm_RVV_16x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
}

// gemm_RVV_16x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 16] @DRAM
// )
void gemm_RVV_16x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
}

// gemm_RVV_16x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 16] @DRAM
// )
void gemm_RVV_16x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
}

// gemm_RVV_16x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 16] @DRAM
// )
void gemm_RVV_16x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
}

// gemm_RVV_16x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 16] @DRAM
// )
void gemm_RVV_16x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
}

// gemm_RVV_16x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 16] @DRAM
// )
void gemm_RVV_16x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
}

// gemm_RVV_16x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 16] @DRAM
// )
void gemm_RVV_16x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
}

// gemm_RVV_16x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 16] @DRAM
// )
void gemm_RVV_16x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg2_5_1 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
}

// gemm_RVV_16x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 16] @DRAM
// )
void gemm_RVV_16x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
}

// gemm_RVV_16x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 16] @DRAM
// )
void gemm_RVV_16x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
}

// gemm_RVV_16x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 16] @DRAM
// )
void gemm_RVV_16x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
}

// gemm_RVV_16x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 16] @DRAM
// )
void gemm_RVV_16x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
}

// gemm_RVV_16x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 16] @DRAM
// )
void gemm_RVV_16x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
}

// gemm_RVV_16x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 16] @DRAM
// )
void gemm_RVV_16x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
}

// gemm_RVV_16x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 16] @DRAM
// )
void gemm_RVV_16x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
}

// gemm_RVV_16x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 16] @DRAM
// )
void gemm_RVV_16x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
}

// gemm_RVV_16x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 16] @DRAM
// )
void gemm_RVV_16x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
}

// gemm_RVV_16x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 16] @DRAM
// )
void gemm_RVV_16x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
}

// gemm_RVV_16x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 16] @DRAM
// )
void gemm_RVV_16x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
}

// gemm_RVV_16x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 16] @DRAM
// )
void gemm_RVV_16x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
}

// gemm_RVV_16x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 16] @DRAM
// )
void gemm_RVV_16x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
}

// gemm_RVV_16x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 16] @DRAM
// )
void gemm_RVV_16x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
}

// gemm_RVV_16x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 16] @DRAM
// )
void gemm_RVV_16x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
}

// gemm_RVV_16x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 16] @DRAM
// )
void gemm_RVV_16x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
}

// gemm_RVV_16x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 16] @DRAM
// )
void gemm_RVV_16x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
}

// gemm_RVV_16x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 16] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 16] @DRAM
// )
void gemm_RVV_16x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
}

// gemm_RVV_17x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 17] @DRAM
// )
void gemm_RVV_17x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
}

// gemm_RVV_17x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 17] @DRAM
// )
void gemm_RVV_17x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
}

// gemm_RVV_17x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 17] @DRAM
// )
void gemm_RVV_17x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(1));
}

// gemm_RVV_17x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 17] @DRAM
// )
void gemm_RVV_17x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(1));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(1));
}

// gemm_RVV_17x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 17] @DRAM
// )
void gemm_RVV_17x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(1));
}

// gemm_RVV_17x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 17] @DRAM
// )
void gemm_RVV_17x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(1));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(1));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(1));
}

// gemm_RVV_17x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 17] @DRAM
// )
void gemm_RVV_17x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(1));
}

// gemm_RVV_17x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 17] @DRAM
// )
void gemm_RVV_17x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(1));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(1));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(1));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(1));
}

// gemm_RVV_17x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 17] @DRAM
// )
void gemm_RVV_17x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(1));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(1));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(1));
}

// gemm_RVV_17x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 17] @DRAM
// )
void gemm_RVV_17x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg2_5_1 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(1));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(1));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(1));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(1));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(1));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(1));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(1));
}

// gemm_RVV_17x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 17] @DRAM
// )
void gemm_RVV_17x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
}

// gemm_RVV_17x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 17] @DRAM
// )
void gemm_RVV_17x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
}

// gemm_RVV_17x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 17] @DRAM
// )
void gemm_RVV_17x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
}

// gemm_RVV_17x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 17] @DRAM
// )
void gemm_RVV_17x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
}

// gemm_RVV_17x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 17] @DRAM
// )
void gemm_RVV_17x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
}

// gemm_RVV_17x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 17] @DRAM
// )
void gemm_RVV_17x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
}

// gemm_RVV_17x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 17] @DRAM
// )
void gemm_RVV_17x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
}

// gemm_RVV_17x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 17] @DRAM
// )
void gemm_RVV_17x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
}

// gemm_RVV_17x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 17] @DRAM
// )
void gemm_RVV_17x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(1));
}

// gemm_RVV_17x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 17] @DRAM
// )
void gemm_RVV_17x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(1));
}

// gemm_RVV_17x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 17] @DRAM
// )
void gemm_RVV_17x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(1));
}

// gemm_RVV_17x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 17] @DRAM
// )
void gemm_RVV_17x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(1));
}

// gemm_RVV_17x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 17] @DRAM
// )
void gemm_RVV_17x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(1));
}

// gemm_RVV_17x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 17] @DRAM
// )
void gemm_RVV_17x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(1));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(1));
}

// gemm_RVV_17x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 17] @DRAM
// )
void gemm_RVV_17x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(1));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(1));
}

// gemm_RVV_17x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 17] @DRAM
// )
void gemm_RVV_17x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(1));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(1));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(1));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(1));
}

// gemm_RVV_17x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 17] @DRAM
// )
void gemm_RVV_17x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
}

// gemm_RVV_17x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 17] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 17] @DRAM
// )
void gemm_RVV_17x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(1));
}

// gemm_RVV_18x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 18] @DRAM
// )
void gemm_RVV_18x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
}

// gemm_RVV_18x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 18] @DRAM
// )
void gemm_RVV_18x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
}

// gemm_RVV_18x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 18] @DRAM
// )
void gemm_RVV_18x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(2));
}

// gemm_RVV_18x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 18] @DRAM
// )
void gemm_RVV_18x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(2));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(2));
}

// gemm_RVV_18x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 18] @DRAM
// )
void gemm_RVV_18x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(2));
}

// gemm_RVV_18x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 18] @DRAM
// )
void gemm_RVV_18x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(2));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(2));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(2));
}

// gemm_RVV_18x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 18] @DRAM
// )
void gemm_RVV_18x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(2));
}

// gemm_RVV_18x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 18] @DRAM
// )
void gemm_RVV_18x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(2));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(2));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(2));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(2));
}

// gemm_RVV_18x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 18] @DRAM
// )
void gemm_RVV_18x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(2));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(2));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(2));
}

// gemm_RVV_18x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 18] @DRAM
// )
void gemm_RVV_18x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg2_5_1 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(2));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(2));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(2));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(2));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(2));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(2));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(2));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(2));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(2));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(2));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(2));
}

// gemm_RVV_18x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 18] @DRAM
// )
void gemm_RVV_18x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
}

// gemm_RVV_18x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 18] @DRAM
// )
void gemm_RVV_18x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
}

// gemm_RVV_18x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 18] @DRAM
// )
void gemm_RVV_18x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
}

// gemm_RVV_18x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 18] @DRAM
// )
void gemm_RVV_18x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
}

// gemm_RVV_18x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 18] @DRAM
// )
void gemm_RVV_18x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
}

// gemm_RVV_18x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 18] @DRAM
// )
void gemm_RVV_18x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
}

// gemm_RVV_18x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 18] @DRAM
// )
void gemm_RVV_18x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
}

// gemm_RVV_18x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 18] @DRAM
// )
void gemm_RVV_18x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
}

// gemm_RVV_18x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 18] @DRAM
// )
void gemm_RVV_18x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(2));
}

// gemm_RVV_18x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 18] @DRAM
// )
void gemm_RVV_18x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(2));
}

// gemm_RVV_18x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 18] @DRAM
// )
void gemm_RVV_18x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(2));
}

// gemm_RVV_18x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 18] @DRAM
// )
void gemm_RVV_18x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(2));
}

// gemm_RVV_18x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 18] @DRAM
// )
void gemm_RVV_18x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(2));
}

// gemm_RVV_18x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 18] @DRAM
// )
void gemm_RVV_18x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(2));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(2));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(2));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(2));
}

// gemm_RVV_18x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 18] @DRAM
// )
void gemm_RVV_18x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(2));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(2));
}

// gemm_RVV_18x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 18] @DRAM
// )
void gemm_RVV_18x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(2));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(2));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(2));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(2));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(2));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(2));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(2));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(2));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(2));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(2));
}

// gemm_RVV_18x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 18] @DRAM
// )
void gemm_RVV_18x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
}

// gemm_RVV_18x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 18] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 18] @DRAM
// )
void gemm_RVV_18x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(2));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(2));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(2));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(2));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(2));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(2));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(2));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(2));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(2));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(2));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(2));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(2));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(2));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(2));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(2));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(2));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(2));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(2));
}

// gemm_RVV_19x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 19] @DRAM
// )
void gemm_RVV_19x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
}

// gemm_RVV_19x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 19] @DRAM
// )
void gemm_RVV_19x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
}

// gemm_RVV_19x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 19] @DRAM
// )
void gemm_RVV_19x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(3));
}

// gemm_RVV_19x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 19] @DRAM
// )
void gemm_RVV_19x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(3));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(3));
}

// gemm_RVV_19x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 19] @DRAM
// )
void gemm_RVV_19x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(3));
}

// gemm_RVV_19x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 19] @DRAM
// )
void gemm_RVV_19x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(3));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(3));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(3));
}

// gemm_RVV_19x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 19] @DRAM
// )
void gemm_RVV_19x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(3));
}

// gemm_RVV_19x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 19] @DRAM
// )
void gemm_RVV_19x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(3));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(3));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(3));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(3));
}

// gemm_RVV_19x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 19] @DRAM
// )
void gemm_RVV_19x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(3));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(3));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(3));
}

// gemm_RVV_19x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 19] @DRAM
// )
void gemm_RVV_19x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg2_5_1 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(3));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(3));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(3));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(3));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(3));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(3));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(3));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(3));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(3));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(3));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(3));
}

// gemm_RVV_19x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 19] @DRAM
// )
void gemm_RVV_19x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
}

// gemm_RVV_19x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 19] @DRAM
// )
void gemm_RVV_19x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
}

// gemm_RVV_19x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 19] @DRAM
// )
void gemm_RVV_19x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
}

// gemm_RVV_19x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 19] @DRAM
// )
void gemm_RVV_19x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
}

// gemm_RVV_19x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 19] @DRAM
// )
void gemm_RVV_19x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
}

// gemm_RVV_19x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 19] @DRAM
// )
void gemm_RVV_19x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
}

// gemm_RVV_19x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 19] @DRAM
// )
void gemm_RVV_19x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
}

// gemm_RVV_19x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 19] @DRAM
// )
void gemm_RVV_19x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
}

// gemm_RVV_19x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 19] @DRAM
// )
void gemm_RVV_19x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(3));
}

// gemm_RVV_19x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 19] @DRAM
// )
void gemm_RVV_19x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(3));
}

// gemm_RVV_19x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 19] @DRAM
// )
void gemm_RVV_19x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(3));
}

// gemm_RVV_19x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 19] @DRAM
// )
void gemm_RVV_19x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(3));
}

// gemm_RVV_19x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 19] @DRAM
// )
void gemm_RVV_19x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(3));
}

// gemm_RVV_19x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 19] @DRAM
// )
void gemm_RVV_19x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(3));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(3));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(3));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(3));
}

// gemm_RVV_19x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 19] @DRAM
// )
void gemm_RVV_19x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(3));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(3));
}

// gemm_RVV_19x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 19] @DRAM
// )
void gemm_RVV_19x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(3));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(3));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(3));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(3));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(3));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(3));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(3));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(3));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(3));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(3));
}

// gemm_RVV_19x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 19] @DRAM
// )
void gemm_RVV_19x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
}

// gemm_RVV_19x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 19] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 19] @DRAM
// )
void gemm_RVV_19x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(3));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(3));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(3));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(3));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(3));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(3));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(3));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(3));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(3));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(3));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(3));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(3));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(3));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(3));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(3));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(3));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(3));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(3));
}

// gemm_RVV_1x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 1] @DRAM
// )
void gemm_RVV_1x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
}

// gemm_RVV_1x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 1] @DRAM
// )
void gemm_RVV_1x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(1));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(1));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(1));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(1));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
}

// gemm_RVV_1x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 1] @DRAM
// )
void gemm_RVV_1x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(1));
}

// gemm_RVV_1x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 1] @DRAM
// )
void gemm_RVV_1x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(1));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(1));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(1));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(1));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(1));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(1));
}

// gemm_RVV_1x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 1] @DRAM
// )
void gemm_RVV_1x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(1));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(1));
}

// gemm_RVV_1x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 1] @DRAM
// )
void gemm_RVV_1x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(1));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(1));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(1));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(1));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(1));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(1));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(1));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(1));
}

// gemm_RVV_1x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 1] @DRAM
// )
void gemm_RVV_1x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(1));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(1));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(1));
}

// gemm_RVV_1x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 1] @DRAM
// )
void gemm_RVV_1x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(1));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(1));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(1));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(1));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(1));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(1));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(1));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(1));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(1));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(1));
}

// gemm_RVV_1x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 1] @DRAM
// )
void gemm_RVV_1x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(1));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(1));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(1));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(1));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(1));
}

// gemm_RVV_1x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 1] @DRAM
// )
void gemm_RVV_1x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(1));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(1));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(1));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(1));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(1));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(1));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(1));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(1));
C_reg2_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(1));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(1));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(1));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(1));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(1));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(1));
}

// gemm_RVV_1x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 1] @DRAM
// )
void gemm_RVV_1x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
}

// gemm_RVV_1x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 1] @DRAM
// )
void gemm_RVV_1x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
}

// gemm_RVV_1x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 1] @DRAM
// )
void gemm_RVV_1x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
}

// gemm_RVV_1x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 1] @DRAM
// )
void gemm_RVV_1x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
}

// gemm_RVV_1x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 1] @DRAM
// )
void gemm_RVV_1x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
}

// gemm_RVV_1x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 1] @DRAM
// )
void gemm_RVV_1x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
}

// gemm_RVV_1x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 1] @DRAM
// )
void gemm_RVV_1x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
}

// gemm_RVV_1x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 1] @DRAM
// )
void gemm_RVV_1x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
}

// gemm_RVV_1x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 1] @DRAM
// )
void gemm_RVV_1x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
}

// gemm_RVV_1x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 1] @DRAM
// )
void gemm_RVV_1x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
}

// gemm_RVV_1x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 1] @DRAM
// )
void gemm_RVV_1x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
}

// gemm_RVV_1x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 1] @DRAM
// )
void gemm_RVV_1x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
}

// gemm_RVV_1x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 1] @DRAM
// )
void gemm_RVV_1x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
}

// gemm_RVV_1x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 1] @DRAM
// )
void gemm_RVV_1x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(1));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
}

// gemm_RVV_1x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 1] @DRAM
// )
void gemm_RVV_1x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
}

// gemm_RVV_1x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 1] @DRAM
// )
void gemm_RVV_1x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(1));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(1));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
}

// gemm_RVV_1x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 1] @DRAM
// )
void gemm_RVV_1x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
}

// gemm_RVV_1x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 1] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 1] @DRAM
// )
void gemm_RVV_1x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(1));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(1));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(1));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(1));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(1));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(1));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(1));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(1));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(1));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(1));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(1));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(1));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(1));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(1));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(1));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(1));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(1));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(1));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(1));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(1));
}

// gemm_RVV_20x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 20] @DRAM
// )
void gemm_RVV_20x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
}

// gemm_RVV_20x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 20] @DRAM
// )
void gemm_RVV_20x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
}

// gemm_RVV_20x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 20] @DRAM
// )
void gemm_RVV_20x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(4));
}

// gemm_RVV_20x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 20] @DRAM
// )
void gemm_RVV_20x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(4));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(4));
}

// gemm_RVV_20x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 20] @DRAM
// )
void gemm_RVV_20x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(4));
}

// gemm_RVV_20x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 20] @DRAM
// )
void gemm_RVV_20x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(4));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(4));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(4));
}

// gemm_RVV_20x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 20] @DRAM
// )
void gemm_RVV_20x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(4));
}

// gemm_RVV_20x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 20] @DRAM
// )
void gemm_RVV_20x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(4));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(4));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(4));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(4));
}

// gemm_RVV_20x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 20] @DRAM
// )
void gemm_RVV_20x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(4));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(4));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(4));
}

// gemm_RVV_20x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 20] @DRAM
// )
void gemm_RVV_20x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg2_5_1 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(4));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(4));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(4));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(4));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(4));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(4));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(4));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(4));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(4));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(4));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(4));
}

// gemm_RVV_20x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 20] @DRAM
// )
void gemm_RVV_20x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
}

// gemm_RVV_20x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 20] @DRAM
// )
void gemm_RVV_20x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
}

// gemm_RVV_20x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 20] @DRAM
// )
void gemm_RVV_20x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
}

// gemm_RVV_20x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 20] @DRAM
// )
void gemm_RVV_20x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
}

// gemm_RVV_20x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 20] @DRAM
// )
void gemm_RVV_20x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
}

// gemm_RVV_20x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 20] @DRAM
// )
void gemm_RVV_20x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
}

// gemm_RVV_20x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 20] @DRAM
// )
void gemm_RVV_20x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
}

// gemm_RVV_20x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 20] @DRAM
// )
void gemm_RVV_20x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
}

// gemm_RVV_20x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 20] @DRAM
// )
void gemm_RVV_20x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(4));
}

// gemm_RVV_20x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 20] @DRAM
// )
void gemm_RVV_20x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(4));
}

// gemm_RVV_20x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 20] @DRAM
// )
void gemm_RVV_20x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(4));
}

// gemm_RVV_20x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 20] @DRAM
// )
void gemm_RVV_20x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(4));
}

// gemm_RVV_20x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 20] @DRAM
// )
void gemm_RVV_20x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(4));
}

// gemm_RVV_20x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 20] @DRAM
// )
void gemm_RVV_20x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(4));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(4));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(4));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(4));
}

// gemm_RVV_20x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 20] @DRAM
// )
void gemm_RVV_20x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(4));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(4));
}

// gemm_RVV_20x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 20] @DRAM
// )
void gemm_RVV_20x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(4));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(4));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(4));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(4));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(4));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(4));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(4));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(4));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(4));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(4));
}

// gemm_RVV_20x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 20] @DRAM
// )
void gemm_RVV_20x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
}

// gemm_RVV_20x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 20] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 20] @DRAM
// )
void gemm_RVV_20x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(4));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(4));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(4));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(4));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(4));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(4));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(4));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(4));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(4));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(4));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(4));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(4));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(4));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(4));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(4));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(4));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(4));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(4));
}

// gemm_RVV_21x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 21] @DRAM
// )
void gemm_RVV_21x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
}

// gemm_RVV_21x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 21] @DRAM
// )
void gemm_RVV_21x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
}

// gemm_RVV_21x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 21] @DRAM
// )
void gemm_RVV_21x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(5));
}

// gemm_RVV_21x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 21] @DRAM
// )
void gemm_RVV_21x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(5));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(5));
}

// gemm_RVV_21x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 21] @DRAM
// )
void gemm_RVV_21x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(5));
}

// gemm_RVV_21x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 21] @DRAM
// )
void gemm_RVV_21x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(5));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(5));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(5));
}

// gemm_RVV_21x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 21] @DRAM
// )
void gemm_RVV_21x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(5));
}

// gemm_RVV_21x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 21] @DRAM
// )
void gemm_RVV_21x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(5));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(5));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(5));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(5));
}

// gemm_RVV_21x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 21] @DRAM
// )
void gemm_RVV_21x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(5));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(5));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(5));
}

// gemm_RVV_21x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 21] @DRAM
// )
void gemm_RVV_21x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg2_5_1 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(5));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(5));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(5));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(5));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(5));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(5));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(5));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(5));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(5));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(5));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(5));
}

// gemm_RVV_21x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 21] @DRAM
// )
void gemm_RVV_21x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
}

// gemm_RVV_21x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 21] @DRAM
// )
void gemm_RVV_21x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
}

// gemm_RVV_21x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 21] @DRAM
// )
void gemm_RVV_21x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
}

// gemm_RVV_21x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 21] @DRAM
// )
void gemm_RVV_21x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
}

// gemm_RVV_21x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 21] @DRAM
// )
void gemm_RVV_21x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
}

// gemm_RVV_21x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 21] @DRAM
// )
void gemm_RVV_21x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
}

// gemm_RVV_21x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 21] @DRAM
// )
void gemm_RVV_21x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
}

// gemm_RVV_21x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 21] @DRAM
// )
void gemm_RVV_21x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
}

// gemm_RVV_21x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 21] @DRAM
// )
void gemm_RVV_21x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(5));
}

// gemm_RVV_21x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 21] @DRAM
// )
void gemm_RVV_21x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(5));
}

// gemm_RVV_21x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 21] @DRAM
// )
void gemm_RVV_21x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(5));
}

// gemm_RVV_21x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 21] @DRAM
// )
void gemm_RVV_21x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(5));
}

// gemm_RVV_21x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 21] @DRAM
// )
void gemm_RVV_21x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(5));
}

// gemm_RVV_21x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 21] @DRAM
// )
void gemm_RVV_21x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(5));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(5));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(5));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(5));
}

// gemm_RVV_21x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 21] @DRAM
// )
void gemm_RVV_21x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(5));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(5));
}

// gemm_RVV_21x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 21] @DRAM
// )
void gemm_RVV_21x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(5));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(5));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(5));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(5));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(5));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(5));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(5));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(5));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(5));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(5));
}

// gemm_RVV_21x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 21] @DRAM
// )
void gemm_RVV_21x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
}

// gemm_RVV_21x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 21] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 21] @DRAM
// )
void gemm_RVV_21x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(5));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(5));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(5));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(5));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(5));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(5));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(5));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(5));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(5));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(5));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(5));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(5));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(5));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(5));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(5));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(5));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(5));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(5));
}

// gemm_RVV_22x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 22] @DRAM
// )
void gemm_RVV_22x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
}

// gemm_RVV_22x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 22] @DRAM
// )
void gemm_RVV_22x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
}

// gemm_RVV_22x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 22] @DRAM
// )
void gemm_RVV_22x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(6));
}

// gemm_RVV_22x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 22] @DRAM
// )
void gemm_RVV_22x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(6));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(6));
}

// gemm_RVV_22x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 22] @DRAM
// )
void gemm_RVV_22x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(6));
}

// gemm_RVV_22x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 22] @DRAM
// )
void gemm_RVV_22x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(6));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(6));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(6));
}

// gemm_RVV_22x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 22] @DRAM
// )
void gemm_RVV_22x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(6));
}

// gemm_RVV_22x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 22] @DRAM
// )
void gemm_RVV_22x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(6));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(6));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(6));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(6));
}

// gemm_RVV_22x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 22] @DRAM
// )
void gemm_RVV_22x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(6));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(6));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(6));
}

// gemm_RVV_22x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 22] @DRAM
// )
void gemm_RVV_22x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg2_5_1 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(6));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(6));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(6));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(6));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(6));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(6));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(6));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(6));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(6));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(6));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(6));
}

// gemm_RVV_22x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 22] @DRAM
// )
void gemm_RVV_22x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
}

// gemm_RVV_22x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 22] @DRAM
// )
void gemm_RVV_22x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
}

// gemm_RVV_22x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 22] @DRAM
// )
void gemm_RVV_22x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
}

// gemm_RVV_22x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 22] @DRAM
// )
void gemm_RVV_22x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
}

// gemm_RVV_22x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 22] @DRAM
// )
void gemm_RVV_22x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
}

// gemm_RVV_22x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 22] @DRAM
// )
void gemm_RVV_22x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
}

// gemm_RVV_22x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 22] @DRAM
// )
void gemm_RVV_22x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
}

// gemm_RVV_22x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 22] @DRAM
// )
void gemm_RVV_22x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
}

// gemm_RVV_22x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 22] @DRAM
// )
void gemm_RVV_22x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(6));
}

// gemm_RVV_22x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 22] @DRAM
// )
void gemm_RVV_22x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(6));
}

// gemm_RVV_22x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 22] @DRAM
// )
void gemm_RVV_22x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(6));
}

// gemm_RVV_22x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 22] @DRAM
// )
void gemm_RVV_22x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(6));
}

// gemm_RVV_22x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 22] @DRAM
// )
void gemm_RVV_22x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(6));
}

// gemm_RVV_22x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 22] @DRAM
// )
void gemm_RVV_22x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(6));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(6));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(6));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(6));
}

// gemm_RVV_22x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 22] @DRAM
// )
void gemm_RVV_22x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(6));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(6));
}

// gemm_RVV_22x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 22] @DRAM
// )
void gemm_RVV_22x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(6));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(6));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(6));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(6));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(6));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(6));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(6));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(6));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(6));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(6));
}

// gemm_RVV_22x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 22] @DRAM
// )
void gemm_RVV_22x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
}

// gemm_RVV_22x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 22] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 22] @DRAM
// )
void gemm_RVV_22x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(6));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(6));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(6));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(6));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(6));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(6));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(6));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(6));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(6));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(6));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(6));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(6));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(6));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(6));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(6));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(6));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(6));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(6));
}

// gemm_RVV_23x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 23] @DRAM
// )
void gemm_RVV_23x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
}

// gemm_RVV_23x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 23] @DRAM
// )
void gemm_RVV_23x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
}

// gemm_RVV_23x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 23] @DRAM
// )
void gemm_RVV_23x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(7));
}

// gemm_RVV_23x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 23] @DRAM
// )
void gemm_RVV_23x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(7));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(7));
}

// gemm_RVV_23x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 23] @DRAM
// )
void gemm_RVV_23x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(7));
}

// gemm_RVV_23x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 23] @DRAM
// )
void gemm_RVV_23x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(7));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(7));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(7));
}

// gemm_RVV_23x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 23] @DRAM
// )
void gemm_RVV_23x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(7));
}

// gemm_RVV_23x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 23] @DRAM
// )
void gemm_RVV_23x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(7));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(7));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(7));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(7));
}

// gemm_RVV_23x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 23] @DRAM
// )
void gemm_RVV_23x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(7));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(7));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(7));
}

// gemm_RVV_23x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 23] @DRAM
// )
void gemm_RVV_23x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg2_5_1 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(7));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(7));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(7));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(7));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(7));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(7));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(7));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(7));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(7));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg3_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg3_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg3_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg3_4,(7));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg3_5,(7));
}

// gemm_RVV_23x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 23] @DRAM
// )
void gemm_RVV_23x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
}

// gemm_RVV_23x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 23] @DRAM
// )
void gemm_RVV_23x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
}

// gemm_RVV_23x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 23] @DRAM
// )
void gemm_RVV_23x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
}

// gemm_RVV_23x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 23] @DRAM
// )
void gemm_RVV_23x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
}

// gemm_RVV_23x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 23] @DRAM
// )
void gemm_RVV_23x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
}

// gemm_RVV_23x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 23] @DRAM
// )
void gemm_RVV_23x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
}

// gemm_RVV_23x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 23] @DRAM
// )
void gemm_RVV_23x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
}

// gemm_RVV_23x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 23] @DRAM
// )
void gemm_RVV_23x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
}

// gemm_RVV_23x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 23] @DRAM
// )
void gemm_RVV_23x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(7));
}

// gemm_RVV_23x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 23] @DRAM
// )
void gemm_RVV_23x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(7));
}

// gemm_RVV_23x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 23] @DRAM
// )
void gemm_RVV_23x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(7));
}

// gemm_RVV_23x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 23] @DRAM
// )
void gemm_RVV_23x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(7));
}

// gemm_RVV_23x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 23] @DRAM
// )
void gemm_RVV_23x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(7));
}

// gemm_RVV_23x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 23] @DRAM
// )
void gemm_RVV_23x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(7));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(7));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(7));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(7));
}

// gemm_RVV_23x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 23] @DRAM
// )
void gemm_RVV_23x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(7));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(7));
}

// gemm_RVV_23x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 23] @DRAM
// )
void gemm_RVV_23x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(7));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(7));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(7));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(7));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(7));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(7));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(7));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(7));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(7));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_regt_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_regt_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_regt_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_regt_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_regt_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_regt_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_regt_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_regt_7,(7));
}

// gemm_RVV_23x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 23] @DRAM
// )
void gemm_RVV_23x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
}

// gemm_RVV_23x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 23] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 23] @DRAM
// )
void gemm_RVV_23x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[16],(7));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 16],(7));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(7));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(7));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(7));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(7));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(7));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(7));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(7));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(7));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(7));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(7));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(7));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(7));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(7));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(7));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg1_0,(7));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg1_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg1_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg1_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg1_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg1_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg1_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg1_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg3_0,(7));
}

// gemm_RVV_24x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 24] @DRAM
// )
void gemm_RVV_24x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
}

// gemm_RVV_24x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 24] @DRAM
// )
void gemm_RVV_24x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_5_2 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_6_2 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg_7_2 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_0_2 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_1_2 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
}

// gemm_RVV_24x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 24] @DRAM
// )
void gemm_RVV_24x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2_2, A_reg_2, B_reg2_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg2_2_2,(8));
}

// gemm_RVV_24x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 24] @DRAM
// )
void gemm_RVV_24x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_2_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_5_2 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_6_2 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg_7_2 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_0_2 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_1_2 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2_2, A_reg_2, B_reg2_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg2_2_2,(8));
}

// gemm_RVV_24x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 24] @DRAM
// )
void gemm_RVV_24x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_2_2;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2_2, A_reg_2, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_3_2 = __riscv_vfmacc_vv_f16m1(C_reg2_3_2, A_reg_2, B_reg2_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg2_2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg2_3_2,(8));
}

// gemm_RVV_24x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 24] @DRAM
// )
void gemm_RVV_24x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_2_2;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_5_2 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_6_2 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg_7_2 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_0_2 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_1_2 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_3_2 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2_2, A_reg_2, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_3_2 = __riscv_vfmacc_vv_f16m1(C_reg2_3_2, A_reg_2, B_reg2_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg2_2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg2_3_2,(8));
}

// gemm_RVV_24x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 24] @DRAM
// )
void gemm_RVV_24x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_2_2;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_3_2;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2_2, A_reg_2, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_3_2 = __riscv_vfmacc_vv_f16m1(C_reg2_3_2, A_reg_2, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_4_2 = __riscv_vfmacc_vv_f16m1(C_reg2_4_2, A_reg_2, B_reg2_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg2_2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg2_3_2,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg2_4_2,(8));
}

// gemm_RVV_24x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 24] @DRAM
// )
void gemm_RVV_24x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_2_2;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_3_2;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_4_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_5_2 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_6_2 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg_7_2 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_0_2 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_1_2 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_3_2 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_4_2 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2_2, A_reg_2, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_3_2 = __riscv_vfmacc_vv_f16m1(C_reg2_3_2, A_reg_2, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_4_2 = __riscv_vfmacc_vv_f16m1(C_reg2_4_2, A_reg_2, B_reg2_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg2_2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg2_3_2,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg2_4_2,(8));
}

// gemm_RVV_24x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 24] @DRAM
// )
void gemm_RVV_24x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_2_2;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_3_2;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_4_2;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg2_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2_2, A_reg_2, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_3_2 = __riscv_vfmacc_vv_f16m1(C_reg2_3_2, A_reg_2, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_4_2 = __riscv_vfmacc_vv_f16m1(C_reg2_4_2, A_reg_2, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg2_5_2 = __riscv_vfmacc_vv_f16m1(C_reg2_5_2, A_reg_2, B_reg2_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg2_2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg2_3_2,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg2_4_2,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg2_5_2,(8));
}

// gemm_RVV_24x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 24] @DRAM
// )
void gemm_RVV_24x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_1_1;
vfloat16m1_t C_reg2_1_2;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_2_1;
vfloat16m1_t C_reg2_2_2;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_3_1;
vfloat16m1_t C_reg2_3_2;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_4_1;
vfloat16m1_t C_reg2_4_2;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg2_5_1;
vfloat16m1_t C_reg2_5_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_5_2 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_6_2 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg_7_2 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_0_2 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_1_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(8));
C_reg2_1_2 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 16],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_2_1 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(8));
C_reg2_2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 16],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_3_1 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(8));
C_reg2_3_2 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 16],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_4_1 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(8));
C_reg2_4_2 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 16],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg2_5_1 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(8));
C_reg2_5_2 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_1_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1_1, A_reg_1, B_reg2_1,(8));
  C_reg2_1_2 = __riscv_vfmacc_vv_f16m1(C_reg2_1_2, A_reg_2, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_2_1, A_reg_1, B_reg2_2,(8));
  C_reg2_2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2_2, A_reg_2, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_3_1 = __riscv_vfmacc_vv_f16m1(C_reg2_3_1, A_reg_1, B_reg2_3,(8));
  C_reg2_3_2 = __riscv_vfmacc_vv_f16m1(C_reg2_3_2, A_reg_2, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_4_1 = __riscv_vfmacc_vv_f16m1(C_reg2_4_1, A_reg_1, B_reg2_4,(8));
  C_reg2_4_2 = __riscv_vfmacc_vv_f16m1(C_reg2_4_2, A_reg_2, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg2_5_1 = __riscv_vfmacc_vv_f16m1(C_reg2_5_1, A_reg_1, B_reg2_5,(8));
  C_reg2_5_2 = __riscv_vfmacc_vv_f16m1(C_reg2_5_2, A_reg_2, B_reg2_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg2_1_1,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 16], C_reg2_1_2,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg2_2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 16], C_reg2_2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg2_3_1,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 16], C_reg2_3_2,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg2_4_1,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 16], C_reg2_4_2,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg2_5_1,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 16], C_reg2_5_2,(8));
}

// gemm_RVV_24x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 24] @DRAM
// )
void gemm_RVV_24x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
}

// gemm_RVV_24x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 24] @DRAM
// )
void gemm_RVV_24x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
}

// gemm_RVV_24x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 24] @DRAM
// )
void gemm_RVV_24x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
}

// gemm_RVV_24x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 24] @DRAM
// )
void gemm_RVV_24x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
}

// gemm_RVV_24x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 24] @DRAM
// )
void gemm_RVV_24x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
}

// gemm_RVV_24x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 24] @DRAM
// )
void gemm_RVV_24x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
}

// gemm_RVV_24x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 24] @DRAM
// )
void gemm_RVV_24x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
}

// gemm_RVV_24x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 24] @DRAM
// )
void gemm_RVV_24x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
}

// gemm_RVV_24x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 24] @DRAM
// )
void gemm_RVV_24x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
}

// gemm_RVV_24x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 24] @DRAM
// )
void gemm_RVV_24x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
}

// gemm_RVV_24x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 24] @DRAM
// )
void gemm_RVV_24x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
}

// gemm_RVV_24x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 24] @DRAM
// )
void gemm_RVV_24x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_5_2 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
}

// gemm_RVV_24x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 24] @DRAM
// )
void gemm_RVV_24x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
}

// gemm_RVV_24x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 24] @DRAM
// )
void gemm_RVV_24x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_5_2 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_6_2 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
}

// gemm_RVV_24x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 24] @DRAM
// )
void gemm_RVV_24x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
}

// gemm_RVV_24x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 24] @DRAM
// )
void gemm_RVV_24x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_5_2 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_6_2 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg_7_2 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
}

// gemm_RVV_24x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 24] @DRAM
// )
void gemm_RVV_24x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
}

// gemm_RVV_24x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 24] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 24] @DRAM
// )
void gemm_RVV_24x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_reg_1;
vfloat16m1_t A_reg_2;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_0_1;
vfloat16m1_t C_reg_0_2;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_1_1;
vfloat16m1_t C_reg_1_2;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_2_1;
vfloat16m1_t C_reg_2_2;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_3_1;
vfloat16m1_t C_reg_3_2;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_4_1;
vfloat16m1_t C_reg_4_2;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_5_1;
vfloat16m1_t C_reg_5_2;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_6_1;
vfloat16m1_t C_reg_6_2;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg_7_1;
vfloat16m1_t C_reg_7_2;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_0_1;
vfloat16m1_t C_reg2_0_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_0_1 = __riscv_vle16_v_f16m1(&C[8],(8));
C_reg_0_2 = __riscv_vle16_v_f16m1(&C[16],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(8));
C_reg_1_2 = __riscv_vle16_v_f16m1(&C[ldc + 16],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_2_1 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(8));
C_reg_2_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 16],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_3_1 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(8));
C_reg_3_2 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 16],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_4_1 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(8));
C_reg_4_2 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 16],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_5_1 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(8));
C_reg_5_2 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 16],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_6_1 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(8));
C_reg_6_2 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 16],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg_7_1 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(8));
C_reg_7_2 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 16],(8));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_0_1 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(8));
C_reg2_0_2 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg_1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(8));
  A_reg_2 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vv_f16m1(C_reg_0_1, A_reg_1, B_reg_0,(8));
  C_reg_0_2 = __riscv_vfmacc_vv_f16m1(C_reg_0_2, A_reg_2, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_1_1 = __riscv_vfmacc_vv_f16m1(C_reg_1_1, A_reg_1, B_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vv_f16m1(C_reg_1_2, A_reg_2, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_2_1 = __riscv_vfmacc_vv_f16m1(C_reg_2_1, A_reg_1, B_reg_2,(8));
  C_reg_2_2 = __riscv_vfmacc_vv_f16m1(C_reg_2_2, A_reg_2, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_3_1 = __riscv_vfmacc_vv_f16m1(C_reg_3_1, A_reg_1, B_reg_3,(8));
  C_reg_3_2 = __riscv_vfmacc_vv_f16m1(C_reg_3_2, A_reg_2, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_4_1 = __riscv_vfmacc_vv_f16m1(C_reg_4_1, A_reg_1, B_reg_4,(8));
  C_reg_4_2 = __riscv_vfmacc_vv_f16m1(C_reg_4_2, A_reg_2, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_5_1 = __riscv_vfmacc_vv_f16m1(C_reg_5_1, A_reg_1, B_reg_5,(8));
  C_reg_5_2 = __riscv_vfmacc_vv_f16m1(C_reg_5_2, A_reg_2, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_6_1 = __riscv_vfmacc_vv_f16m1(C_reg_6_1, A_reg_1, B_reg_6,(8));
  C_reg_6_2 = __riscv_vfmacc_vv_f16m1(C_reg_6_2, A_reg_2, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg_7_1 = __riscv_vfmacc_vv_f16m1(C_reg_7_1, A_reg_1, B_reg_7,(8));
  C_reg_7_2 = __riscv_vfmacc_vv_f16m1(C_reg_7_2, A_reg_2, B_reg_7,(8));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_0_1 = __riscv_vfmacc_vv_f16m1(C_reg2_0_1, A_reg_1, B_reg2_0,(8));
  C_reg2_0_2 = __riscv_vfmacc_vv_f16m1(C_reg2_0_2, A_reg_2, B_reg2_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg_0_1,(8));
__riscv_vse16_v_f16m1(&C[16], C_reg_0_2,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg_1_1,(8));
__riscv_vse16_v_f16m1(&C[ldc + 16], C_reg_1_2,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg_2_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 16], C_reg_2_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg_3_1,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 16], C_reg_3_2,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg_4_1,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 16], C_reg_4_2,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg_5_1,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 16], C_reg_5_2,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg_6_1,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 16], C_reg_6_2,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg_7_1,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 16], C_reg_7_2,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg2_0_1,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 16], C_reg2_0_2,(8));
}

// gemm_RVV_2x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 2] @DRAM
// )
void gemm_RVV_2x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
}

// gemm_RVV_2x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 2] @DRAM
// )
void gemm_RVV_2x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(2));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(2));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(2));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(2));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
}

// gemm_RVV_2x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 2] @DRAM
// )
void gemm_RVV_2x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(2));
}

// gemm_RVV_2x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 2] @DRAM
// )
void gemm_RVV_2x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(2));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(2));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(2));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(2));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(2));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(2));
}

// gemm_RVV_2x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 2] @DRAM
// )
void gemm_RVV_2x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(2));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(2));
}

// gemm_RVV_2x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 2] @DRAM
// )
void gemm_RVV_2x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(2));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(2));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(2));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(2));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(2));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(2));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(2));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(2));
}

// gemm_RVV_2x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 2] @DRAM
// )
void gemm_RVV_2x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(2));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(2));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(2));
}

// gemm_RVV_2x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 2] @DRAM
// )
void gemm_RVV_2x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(2));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(2));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(2));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(2));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(2));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(2));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(2));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(2));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(2));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(2));
}

// gemm_RVV_2x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 2] @DRAM
// )
void gemm_RVV_2x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(2));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(2));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(2));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(2));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(2));
}

// gemm_RVV_2x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 2] @DRAM
// )
void gemm_RVV_2x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(2));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(2));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(2));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(2));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(2));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(2));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(2));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(2));
C_reg2_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(2));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(2));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(2));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(2));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(2));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(2));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(2));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(2));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(2));
}

// gemm_RVV_2x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 2] @DRAM
// )
void gemm_RVV_2x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
}

// gemm_RVV_2x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 2] @DRAM
// )
void gemm_RVV_2x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
}

// gemm_RVV_2x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 2] @DRAM
// )
void gemm_RVV_2x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
}

// gemm_RVV_2x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 2] @DRAM
// )
void gemm_RVV_2x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
}

// gemm_RVV_2x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 2] @DRAM
// )
void gemm_RVV_2x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
}

// gemm_RVV_2x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 2] @DRAM
// )
void gemm_RVV_2x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
}

// gemm_RVV_2x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 2] @DRAM
// )
void gemm_RVV_2x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
}

// gemm_RVV_2x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 2] @DRAM
// )
void gemm_RVV_2x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
}

// gemm_RVV_2x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 2] @DRAM
// )
void gemm_RVV_2x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
}

// gemm_RVV_2x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 2] @DRAM
// )
void gemm_RVV_2x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
}

// gemm_RVV_2x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 2] @DRAM
// )
void gemm_RVV_2x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
}

// gemm_RVV_2x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 2] @DRAM
// )
void gemm_RVV_2x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
}

// gemm_RVV_2x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 2] @DRAM
// )
void gemm_RVV_2x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
}

// gemm_RVV_2x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 2] @DRAM
// )
void gemm_RVV_2x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(2));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(2));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(2));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(2));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(2));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(2));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(2));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
}

// gemm_RVV_2x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 2] @DRAM
// )
void gemm_RVV_2x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
}

// gemm_RVV_2x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 2] @DRAM
// )
void gemm_RVV_2x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(2));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(2));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
}

// gemm_RVV_2x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 2] @DRAM
// )
void gemm_RVV_2x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
}

// gemm_RVV_2x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 2] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 2] @DRAM
// )
void gemm_RVV_2x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(2));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(2));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(2));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(2));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(2));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(2));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(2));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(2));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(2));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (2));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (2));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (2));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (2));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (2));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (2));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (2));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(2));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(2));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(2));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(2));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(2));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(2));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(2));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(2));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(2));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(2));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(2));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(2));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(2));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(2));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(2));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(2));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(2));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(2));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(2));
}

// gemm_RVV_3x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 3] @DRAM
// )
void gemm_RVV_3x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
}

// gemm_RVV_3x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 3] @DRAM
// )
void gemm_RVV_3x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(3));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(3));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(3));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(3));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
}

// gemm_RVV_3x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 3] @DRAM
// )
void gemm_RVV_3x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(3));
}

// gemm_RVV_3x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 3] @DRAM
// )
void gemm_RVV_3x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(3));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(3));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(3));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(3));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(3));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(3));
}

// gemm_RVV_3x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 3] @DRAM
// )
void gemm_RVV_3x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(3));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(3));
}

// gemm_RVV_3x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 3] @DRAM
// )
void gemm_RVV_3x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(3));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(3));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(3));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(3));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(3));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(3));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(3));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(3));
}

// gemm_RVV_3x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 3] @DRAM
// )
void gemm_RVV_3x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(3));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(3));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(3));
}

// gemm_RVV_3x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 3] @DRAM
// )
void gemm_RVV_3x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(3));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(3));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(3));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(3));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(3));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(3));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(3));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(3));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(3));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(3));
}

// gemm_RVV_3x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 3] @DRAM
// )
void gemm_RVV_3x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(3));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(3));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(3));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(3));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(3));
}

// gemm_RVV_3x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 3] @DRAM
// )
void gemm_RVV_3x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(3));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(3));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(3));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(3));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(3));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(3));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(3));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(3));
C_reg2_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(3));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(3));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(3));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(3));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(3));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(3));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(3));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(3));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(3));
}

// gemm_RVV_3x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 3] @DRAM
// )
void gemm_RVV_3x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
}

// gemm_RVV_3x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 3] @DRAM
// )
void gemm_RVV_3x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
}

// gemm_RVV_3x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 3] @DRAM
// )
void gemm_RVV_3x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
}

// gemm_RVV_3x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 3] @DRAM
// )
void gemm_RVV_3x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
}

// gemm_RVV_3x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 3] @DRAM
// )
void gemm_RVV_3x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
}

// gemm_RVV_3x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 3] @DRAM
// )
void gemm_RVV_3x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
}

// gemm_RVV_3x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 3] @DRAM
// )
void gemm_RVV_3x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
}

// gemm_RVV_3x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 3] @DRAM
// )
void gemm_RVV_3x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
}

// gemm_RVV_3x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 3] @DRAM
// )
void gemm_RVV_3x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
}

// gemm_RVV_3x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 3] @DRAM
// )
void gemm_RVV_3x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
}

// gemm_RVV_3x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 3] @DRAM
// )
void gemm_RVV_3x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
}

// gemm_RVV_3x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 3] @DRAM
// )
void gemm_RVV_3x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
}

// gemm_RVV_3x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 3] @DRAM
// )
void gemm_RVV_3x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
}

// gemm_RVV_3x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 3] @DRAM
// )
void gemm_RVV_3x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(3));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(3));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(3));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(3));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(3));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(3));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(3));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
}

// gemm_RVV_3x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 3] @DRAM
// )
void gemm_RVV_3x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
}

// gemm_RVV_3x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 3] @DRAM
// )
void gemm_RVV_3x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(3));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(3));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
}

// gemm_RVV_3x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 3] @DRAM
// )
void gemm_RVV_3x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
}

// gemm_RVV_3x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 3] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 3] @DRAM
// )
void gemm_RVV_3x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(3));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(3));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(3));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(3));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(3));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(3));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(3));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(3));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(3));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (3));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (3));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (3));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (3));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (3));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (3));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (3));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(3));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(3));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(3));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(3));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(3));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(3));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(3));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(3));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(3));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(3));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(3));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(3));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(3));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(3));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(3));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(3));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(3));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(3));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(3));
}

// gemm_RVV_4x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 4] @DRAM
// )
void gemm_RVV_4x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
}

// gemm_RVV_4x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 4] @DRAM
// )
void gemm_RVV_4x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(4));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(4));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(4));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(4));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
}

// gemm_RVV_4x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 4] @DRAM
// )
void gemm_RVV_4x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(4));
}

// gemm_RVV_4x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 4] @DRAM
// )
void gemm_RVV_4x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(4));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(4));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(4));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(4));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(4));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(4));
}

// gemm_RVV_4x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 4] @DRAM
// )
void gemm_RVV_4x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(4));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(4));
}

// gemm_RVV_4x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 4] @DRAM
// )
void gemm_RVV_4x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(4));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(4));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(4));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(4));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(4));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(4));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(4));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(4));
}

// gemm_RVV_4x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 4] @DRAM
// )
void gemm_RVV_4x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(4));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(4));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(4));
}

// gemm_RVV_4x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 4] @DRAM
// )
void gemm_RVV_4x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(4));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(4));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(4));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(4));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(4));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(4));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(4));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(4));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(4));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(4));
}

// gemm_RVV_4x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 4] @DRAM
// )
void gemm_RVV_4x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(4));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(4));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(4));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(4));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(4));
}

// gemm_RVV_4x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 4] @DRAM
// )
void gemm_RVV_4x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(4));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(4));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(4));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(4));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(4));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(4));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(4));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(4));
C_reg2_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(4));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(4));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(4));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(4));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(4));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(4));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(4));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(4));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(4));
}

// gemm_RVV_4x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 4] @DRAM
// )
void gemm_RVV_4x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
}

// gemm_RVV_4x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 4] @DRAM
// )
void gemm_RVV_4x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
}

// gemm_RVV_4x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 4] @DRAM
// )
void gemm_RVV_4x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
}

// gemm_RVV_4x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 4] @DRAM
// )
void gemm_RVV_4x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
}

// gemm_RVV_4x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 4] @DRAM
// )
void gemm_RVV_4x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
}

// gemm_RVV_4x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 4] @DRAM
// )
void gemm_RVV_4x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
}

// gemm_RVV_4x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 4] @DRAM
// )
void gemm_RVV_4x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
}

// gemm_RVV_4x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 4] @DRAM
// )
void gemm_RVV_4x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
}

// gemm_RVV_4x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 4] @DRAM
// )
void gemm_RVV_4x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
}

// gemm_RVV_4x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 4] @DRAM
// )
void gemm_RVV_4x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
}

// gemm_RVV_4x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 4] @DRAM
// )
void gemm_RVV_4x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
}

// gemm_RVV_4x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 4] @DRAM
// )
void gemm_RVV_4x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
}

// gemm_RVV_4x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 4] @DRAM
// )
void gemm_RVV_4x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
}

// gemm_RVV_4x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 4] @DRAM
// )
void gemm_RVV_4x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(4));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(4));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(4));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(4));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(4));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(4));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(4));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
}

// gemm_RVV_4x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 4] @DRAM
// )
void gemm_RVV_4x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
}

// gemm_RVV_4x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 4] @DRAM
// )
void gemm_RVV_4x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(4));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(4));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
}

// gemm_RVV_4x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 4] @DRAM
// )
void gemm_RVV_4x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
}

// gemm_RVV_4x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 4] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 4] @DRAM
// )
void gemm_RVV_4x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(4));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(4));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(4));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(4));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(4));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(4));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(4));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(4));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(4));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (4));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (4));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (4));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (4));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (4));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (4));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (4));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(4));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(4));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(4));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(4));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(4));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(4));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(4));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(4));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(4));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(4));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(4));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(4));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(4));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(4));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(4));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(4));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(4));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(4));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(4));
}

// gemm_RVV_5x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 5] @DRAM
// )
void gemm_RVV_5x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
}

// gemm_RVV_5x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 5] @DRAM
// )
void gemm_RVV_5x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(5));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(5));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(5));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(5));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
}

// gemm_RVV_5x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 5] @DRAM
// )
void gemm_RVV_5x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(5));
}

// gemm_RVV_5x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 5] @DRAM
// )
void gemm_RVV_5x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(5));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(5));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(5));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(5));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(5));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(5));
}

// gemm_RVV_5x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 5] @DRAM
// )
void gemm_RVV_5x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(5));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(5));
}

// gemm_RVV_5x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 5] @DRAM
// )
void gemm_RVV_5x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(5));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(5));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(5));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(5));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(5));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(5));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(5));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(5));
}

// gemm_RVV_5x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 5] @DRAM
// )
void gemm_RVV_5x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(5));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(5));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(5));
}

// gemm_RVV_5x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 5] @DRAM
// )
void gemm_RVV_5x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(5));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(5));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(5));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(5));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(5));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(5));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(5));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(5));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(5));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(5));
}

// gemm_RVV_5x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 5] @DRAM
// )
void gemm_RVV_5x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(5));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(5));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(5));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(5));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(5));
}

// gemm_RVV_5x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 5] @DRAM
// )
void gemm_RVV_5x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(5));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(5));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(5));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(5));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(5));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(5));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(5));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(5));
C_reg2_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(5));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(5));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(5));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(5));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(5));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(5));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(5));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(5));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(5));
}

// gemm_RVV_5x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 5] @DRAM
// )
void gemm_RVV_5x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
}

// gemm_RVV_5x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 5] @DRAM
// )
void gemm_RVV_5x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
}

// gemm_RVV_5x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 5] @DRAM
// )
void gemm_RVV_5x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
}

// gemm_RVV_5x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 5] @DRAM
// )
void gemm_RVV_5x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
}

// gemm_RVV_5x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 5] @DRAM
// )
void gemm_RVV_5x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
}

// gemm_RVV_5x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 5] @DRAM
// )
void gemm_RVV_5x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
}

// gemm_RVV_5x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 5] @DRAM
// )
void gemm_RVV_5x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
}

// gemm_RVV_5x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 5] @DRAM
// )
void gemm_RVV_5x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
}

// gemm_RVV_5x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 5] @DRAM
// )
void gemm_RVV_5x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
}

// gemm_RVV_5x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 5] @DRAM
// )
void gemm_RVV_5x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
}

// gemm_RVV_5x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 5] @DRAM
// )
void gemm_RVV_5x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
}

// gemm_RVV_5x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 5] @DRAM
// )
void gemm_RVV_5x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
}

// gemm_RVV_5x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 5] @DRAM
// )
void gemm_RVV_5x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
}

// gemm_RVV_5x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 5] @DRAM
// )
void gemm_RVV_5x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(5));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(5));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(5));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(5));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(5));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(5));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(5));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
}

// gemm_RVV_5x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 5] @DRAM
// )
void gemm_RVV_5x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
}

// gemm_RVV_5x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 5] @DRAM
// )
void gemm_RVV_5x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(5));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(5));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
}

// gemm_RVV_5x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 5] @DRAM
// )
void gemm_RVV_5x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
}

// gemm_RVV_5x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 5] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 5] @DRAM
// )
void gemm_RVV_5x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(5));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(5));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(5));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(5));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(5));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(5));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(5));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(5));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(5));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (5));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (5));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (5));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (5));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (5));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (5));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (5));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(5));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(5));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(5));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(5));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(5));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(5));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(5));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(5));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(5));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(5));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(5));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(5));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(5));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(5));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(5));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(5));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(5));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(5));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(5));
}

// gemm_RVV_6x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 6] @DRAM
// )
void gemm_RVV_6x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
}

// gemm_RVV_6x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 6] @DRAM
// )
void gemm_RVV_6x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(6));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(6));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(6));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(6));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
}

// gemm_RVV_6x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 6] @DRAM
// )
void gemm_RVV_6x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(6));
}

// gemm_RVV_6x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 6] @DRAM
// )
void gemm_RVV_6x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(6));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(6));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(6));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(6));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(6));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(6));
}

// gemm_RVV_6x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 6] @DRAM
// )
void gemm_RVV_6x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(6));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(6));
}

// gemm_RVV_6x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 6] @DRAM
// )
void gemm_RVV_6x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(6));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(6));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(6));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(6));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(6));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(6));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(6));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(6));
}

// gemm_RVV_6x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 6] @DRAM
// )
void gemm_RVV_6x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(6));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(6));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(6));
}

// gemm_RVV_6x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 6] @DRAM
// )
void gemm_RVV_6x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(6));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(6));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(6));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(6));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(6));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(6));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(6));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(6));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(6));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(6));
}

// gemm_RVV_6x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 6] @DRAM
// )
void gemm_RVV_6x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(6));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(6));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(6));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(6));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(6));
}

// gemm_RVV_6x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 6] @DRAM
// )
void gemm_RVV_6x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(6));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(6));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(6));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(6));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(6));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(6));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(6));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(6));
C_reg2_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(6));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(6));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(6));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(6));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(6));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(6));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(6));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(6));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(6));
}

// gemm_RVV_6x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 6] @DRAM
// )
void gemm_RVV_6x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
}

// gemm_RVV_6x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 6] @DRAM
// )
void gemm_RVV_6x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
}

// gemm_RVV_6x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 6] @DRAM
// )
void gemm_RVV_6x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
}

// gemm_RVV_6x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 6] @DRAM
// )
void gemm_RVV_6x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
}

// gemm_RVV_6x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 6] @DRAM
// )
void gemm_RVV_6x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
}

// gemm_RVV_6x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 6] @DRAM
// )
void gemm_RVV_6x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
}

// gemm_RVV_6x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 6] @DRAM
// )
void gemm_RVV_6x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
}

// gemm_RVV_6x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 6] @DRAM
// )
void gemm_RVV_6x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
}

// gemm_RVV_6x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 6] @DRAM
// )
void gemm_RVV_6x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
}

// gemm_RVV_6x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 6] @DRAM
// )
void gemm_RVV_6x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
}

// gemm_RVV_6x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 6] @DRAM
// )
void gemm_RVV_6x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
}

// gemm_RVV_6x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 6] @DRAM
// )
void gemm_RVV_6x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
}

// gemm_RVV_6x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 6] @DRAM
// )
void gemm_RVV_6x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
}

// gemm_RVV_6x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 6] @DRAM
// )
void gemm_RVV_6x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(6));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(6));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(6));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(6));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(6));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(6));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(6));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
}

// gemm_RVV_6x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 6] @DRAM
// )
void gemm_RVV_6x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
}

// gemm_RVV_6x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 6] @DRAM
// )
void gemm_RVV_6x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(6));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(6));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
}

// gemm_RVV_6x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 6] @DRAM
// )
void gemm_RVV_6x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
}

// gemm_RVV_6x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 6] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 6] @DRAM
// )
void gemm_RVV_6x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(6));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(6));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(6));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(6));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(6));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(6));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(6));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(6));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(6));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (6));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (6));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (6));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (6));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (6));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (6));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (6));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(6));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(6));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(6));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(6));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(6));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(6));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(6));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(6));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(6));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(6));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(6));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(6));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(6));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(6));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(6));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(6));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(6));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(6));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(6));
}

// gemm_RVV_7x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 7] @DRAM
// )
void gemm_RVV_7x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
}

// gemm_RVV_7x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 7] @DRAM
// )
void gemm_RVV_7x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(7));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(7));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(7));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(7));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
}

// gemm_RVV_7x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 7] @DRAM
// )
void gemm_RVV_7x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(7));
}

// gemm_RVV_7x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 7] @DRAM
// )
void gemm_RVV_7x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(7));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(7));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(7));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(7));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(7));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(7));
}

// gemm_RVV_7x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 7] @DRAM
// )
void gemm_RVV_7x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(7));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(7));
}

// gemm_RVV_7x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 7] @DRAM
// )
void gemm_RVV_7x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(7));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(7));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(7));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(7));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(7));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(7));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(7));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(7));
}

// gemm_RVV_7x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 7] @DRAM
// )
void gemm_RVV_7x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(7));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(7));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(7));
}

// gemm_RVV_7x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 7] @DRAM
// )
void gemm_RVV_7x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(7));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(7));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(7));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(7));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(7));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(7));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(7));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(7));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(7));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(7));
}

// gemm_RVV_7x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 7] @DRAM
// )
void gemm_RVV_7x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(7));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(7));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(7));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(7));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(7));
}

// gemm_RVV_7x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 7] @DRAM
// )
void gemm_RVV_7x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(7));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(7));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(7));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(7));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(7));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(7));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(7));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(7));
C_reg2_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(7));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(7));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(7));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(7));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(7));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(7));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(7));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(7));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(7));
}

// gemm_RVV_7x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 7] @DRAM
// )
void gemm_RVV_7x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
}

// gemm_RVV_7x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 7] @DRAM
// )
void gemm_RVV_7x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
}

// gemm_RVV_7x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 7] @DRAM
// )
void gemm_RVV_7x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
}

// gemm_RVV_7x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 7] @DRAM
// )
void gemm_RVV_7x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
}

// gemm_RVV_7x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 7] @DRAM
// )
void gemm_RVV_7x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
}

// gemm_RVV_7x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 7] @DRAM
// )
void gemm_RVV_7x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
}

// gemm_RVV_7x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 7] @DRAM
// )
void gemm_RVV_7x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
}

// gemm_RVV_7x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 7] @DRAM
// )
void gemm_RVV_7x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
}

// gemm_RVV_7x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 7] @DRAM
// )
void gemm_RVV_7x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
}

// gemm_RVV_7x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 7] @DRAM
// )
void gemm_RVV_7x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
}

// gemm_RVV_7x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 7] @DRAM
// )
void gemm_RVV_7x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
}

// gemm_RVV_7x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 7] @DRAM
// )
void gemm_RVV_7x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
}

// gemm_RVV_7x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 7] @DRAM
// )
void gemm_RVV_7x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
}

// gemm_RVV_7x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 7] @DRAM
// )
void gemm_RVV_7x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(7));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(7));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(7));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(7));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(7));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(7));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(7));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
}

// gemm_RVV_7x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 7] @DRAM
// )
void gemm_RVV_7x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
}

// gemm_RVV_7x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 7] @DRAM
// )
void gemm_RVV_7x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(7));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(7));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
}

// gemm_RVV_7x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 7] @DRAM
// )
void gemm_RVV_7x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
}

// gemm_RVV_7x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 7] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 7] @DRAM
// )
void gemm_RVV_7x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(7));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(7));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(7));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(7));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(7));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(7));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(7));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(7));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(7));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (7));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (7));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (7));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (7));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (7));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (7));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (7));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(7));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(7));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(7));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(7));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(7));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(7));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(7));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(7));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(7));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(7));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(7));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(7));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(7));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(7));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(7));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(7));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(7));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(7));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(7));
}

// gemm_RVV_8x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 8] @DRAM
// )
void gemm_RVV_8x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
}

// gemm_RVV_8x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 8] @DRAM
// )
void gemm_RVV_8x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
}

// gemm_RVV_8x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 8] @DRAM
// )
void gemm_RVV_8x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(8));
}

// gemm_RVV_8x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 8] @DRAM
// )
void gemm_RVV_8x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(8));
}

// gemm_RVV_8x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 8] @DRAM
// )
void gemm_RVV_8x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(8));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(8));
}

// gemm_RVV_8x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 8] @DRAM
// )
void gemm_RVV_8x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(8));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(8));
}

// gemm_RVV_8x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 8] @DRAM
// )
void gemm_RVV_8x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(8));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(8));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(8));
}

// gemm_RVV_8x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 8] @DRAM
// )
void gemm_RVV_8x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(8));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(8));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(8));
}

// gemm_RVV_8x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 8] @DRAM
// )
void gemm_RVV_8x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(8));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(8));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(8));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(8));
}

// gemm_RVV_8x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 8] @DRAM
// )
void gemm_RVV_8x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
vfloat16m1_t C_reg2_1;
vfloat16m1_t C_reg2_2;
vfloat16m1_t C_reg2_3;
vfloat16m1_t C_reg2_4;
vfloat16m1_t C_reg2_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
  C_reg2_1 = __riscv_vfmacc_vv_f16m1(C_reg2_1, A_reg, B_reg2_1,(8));
  C_reg2_2 = __riscv_vfmacc_vv_f16m1(C_reg2_2, A_reg, B_reg2_2,(8));
  C_reg2_3 = __riscv_vfmacc_vv_f16m1(C_reg2_3, A_reg, B_reg2_3,(8));
  C_reg2_4 = __riscv_vfmacc_vv_f16m1(C_reg2_4, A_reg, B_reg2_4,(8));
  C_reg2_5 = __riscv_vfmacc_vv_f16m1(C_reg2_5, A_reg, B_reg2_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5,(8));
}

// gemm_RVV_8x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 8] @DRAM
// )
void gemm_RVV_8x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
}

// gemm_RVV_8x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 8] @DRAM
// )
void gemm_RVV_8x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
}

// gemm_RVV_8x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 8] @DRAM
// )
void gemm_RVV_8x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
}

// gemm_RVV_8x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 8] @DRAM
// )
void gemm_RVV_8x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
}

// gemm_RVV_8x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 8] @DRAM
// )
void gemm_RVV_8x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
}

// gemm_RVV_8x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 8] @DRAM
// )
void gemm_RVV_8x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
}

// gemm_RVV_8x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 8] @DRAM
// )
void gemm_RVV_8x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
}

// gemm_RVV_8x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 8] @DRAM
// )
void gemm_RVV_8x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
}

// gemm_RVV_8x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 8] @DRAM
// )
void gemm_RVV_8x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
}

// gemm_RVV_8x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 8] @DRAM
// )
void gemm_RVV_8x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
}

// gemm_RVV_8x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 8] @DRAM
// )
void gemm_RVV_8x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
}

// gemm_RVV_8x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 8] @DRAM
// )
void gemm_RVV_8x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
}

// gemm_RVV_8x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 8] @DRAM
// )
void gemm_RVV_8x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
}

// gemm_RVV_8x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 8] @DRAM
// )
void gemm_RVV_8x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
}

// gemm_RVV_8x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 8] @DRAM
// )
void gemm_RVV_8x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
}

// gemm_RVV_8x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 8] @DRAM
// )
void gemm_RVV_8x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
}

// gemm_RVV_8x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 8] @DRAM
// )
void gemm_RVV_8x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
}

// gemm_RVV_8x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 8] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 8] @DRAM
// )
void gemm_RVV_8x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t C_reg_0;
vfloat16m1_t C_reg_1;
vfloat16m1_t C_reg_2;
vfloat16m1_t C_reg_3;
vfloat16m1_t C_reg_4;
vfloat16m1_t C_reg_5;
vfloat16m1_t C_reg_6;
vfloat16m1_t C_reg_7;
vfloat16m1_t C_reg2_0;
C_reg_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg2_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
vfloat16m1_t A_reg;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t B_reg2_0;
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  C_reg_0 = __riscv_vfmacc_vv_f16m1(C_reg_0, A_reg, B_reg_0,(8));
  C_reg_1 = __riscv_vfmacc_vv_f16m1(C_reg_1, A_reg, B_reg_1,(8));
  C_reg_2 = __riscv_vfmacc_vv_f16m1(C_reg_2, A_reg, B_reg_2,(8));
  C_reg_3 = __riscv_vfmacc_vv_f16m1(C_reg_3, A_reg, B_reg_3,(8));
  C_reg_4 = __riscv_vfmacc_vv_f16m1(C_reg_4, A_reg, B_reg_4,(8));
  C_reg_5 = __riscv_vfmacc_vv_f16m1(C_reg_5, A_reg, B_reg_5,(8));
  C_reg_6 = __riscv_vfmacc_vv_f16m1(C_reg_6, A_reg, B_reg_6,(8));
  C_reg_7 = __riscv_vfmacc_vv_f16m1(C_reg_7, A_reg, B_reg_7,(8));
  C_reg2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0, A_reg, B_reg2_0,(8));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0,(8));
}

// gemm_RVV_9x10_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 9] @DRAM
// )
void gemm_RVV_9x10_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
}

// gemm_RVV_9x10_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 10] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][10, 9] @DRAM
// )
void gemm_RVV_9x10_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(2));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
}

// gemm_RVV_9x11_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 9] @DRAM
// )
void gemm_RVV_9x11_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(1));
}

// gemm_RVV_9x11_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 11] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][11, 9] @DRAM
// )
void gemm_RVV_9x11_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(1));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(3));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(1));
}

// gemm_RVV_9x12_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 9] @DRAM
// )
void gemm_RVV_9x12_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(1));
}

// gemm_RVV_9x12_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 12] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][12, 9] @DRAM
// )
void gemm_RVV_9x12_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(1));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(1));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(4));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(1));
}

// gemm_RVV_9x13_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 9] @DRAM
// )
void gemm_RVV_9x13_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(1));
}

// gemm_RVV_9x13_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 13] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][13, 9] @DRAM
// )
void gemm_RVV_9x13_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(1));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(1));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(1));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(5));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(1));
}

// gemm_RVV_9x14_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 9] @DRAM
// )
void gemm_RVV_9x14_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg2_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg3_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(1));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(1));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(1));
}

// gemm_RVV_9x14_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 14] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][14, 9] @DRAM
// )
void gemm_RVV_9x14_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_reg2_1;
vfloat16m1_t B_reg2_2;
vfloat16m1_t B_reg2_3;
vfloat16m1_t B_reg2_4;
vfloat16m1_t B_reg2_5;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg2_1_0;
vfloat16m1_t C_reg2_2_0;
vfloat16m1_t C_reg2_3_0;
vfloat16m1_t C_reg2_4_0;
vfloat16m1_t C_reg2_5_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
vfloat16m1_t C_reg3_1;
vfloat16m1_t C_reg3_2;
vfloat16m1_t C_reg3_3;
vfloat16m1_t C_reg3_4;
vfloat16m1_t C_reg3_5;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg2_1_0 = __riscv_vle16_v_f16m1(&C[(9) * (ldc)],(8));
C_reg2_2_0 = __riscv_vle16_v_f16m1(&C[(10) * (ldc)],(8));
C_reg2_3_0 = __riscv_vle16_v_f16m1(&C[(11) * (ldc)],(8));
C_reg2_4_0 = __riscv_vle16_v_f16m1(&C[(12) * (ldc)],(8));
C_reg2_5_0 = __riscv_vle16_v_f16m1(&C[(13) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(1));
C_reg3_1 = __riscv_vle16_v_f16m1(&C[(9) * (ldc) + 8],(1));
C_reg3_2 = __riscv_vle16_v_f16m1(&C[(10) * (ldc) + 8],(1));
C_reg3_3 = __riscv_vle16_v_f16m1(&C[(11) * (ldc) + 8],(1));
C_reg3_4 = __riscv_vle16_v_f16m1(&C[(12) * (ldc) + 8],(1));
C_reg3_5 = __riscv_vle16_v_f16m1(&C[(13) * (ldc) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(6));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg2_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg2_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg2_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg2_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg2_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg2_1_0 = __riscv_vfmacc_vv_f16m1(C_reg2_1_0, A_reg_0, B_reg2_1,(8));
  C_reg2_2_0 = __riscv_vfmacc_vv_f16m1(C_reg2_2_0, A_reg_0, B_reg2_2,(8));
  C_reg2_3_0 = __riscv_vfmacc_vv_f16m1(C_reg2_3_0, A_reg_0, B_reg2_3,(8));
  C_reg2_4_0 = __riscv_vfmacc_vv_f16m1(C_reg2_4_0, A_reg_0, B_reg2_4,(8));
  C_reg2_5_0 = __riscv_vfmacc_vv_f16m1(C_reg2_5_0, A_reg_0, B_reg2_5,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
  C_reg3_1 = __riscv_vfmacc_vv_f16m1(C_reg3_1, A_reg1, B_reg2_1,(1));
  C_reg3_2 = __riscv_vfmacc_vv_f16m1(C_reg3_2, A_reg1, B_reg2_2,(1));
  C_reg3_3 = __riscv_vfmacc_vv_f16m1(C_reg3_3, A_reg1, B_reg2_3,(1));
  C_reg3_4 = __riscv_vfmacc_vv_f16m1(C_reg3_4, A_reg1, B_reg2_4,(1));
  C_reg3_5 = __riscv_vfmacc_vv_f16m1(C_reg3_5, A_reg1, B_reg2_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(9) * (ldc)], C_reg2_1_0,(8));
__riscv_vse16_v_f16m1(&C[(10) * (ldc)], C_reg2_2_0,(8));
__riscv_vse16_v_f16m1(&C[(11) * (ldc)], C_reg2_3_0,(8));
__riscv_vse16_v_f16m1(&C[(12) * (ldc)], C_reg2_4_0,(8));
__riscv_vse16_v_f16m1(&C[(13) * (ldc)], C_reg2_5_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
__riscv_vse16_v_f16m1(&C[(9) * (ldc) + 8], C_reg3_1,(1));
__riscv_vse16_v_f16m1(&C[(10) * (ldc) + 8], C_reg3_2,(1));
__riscv_vse16_v_f16m1(&C[(11) * (ldc) + 8], C_reg3_3,(1));
__riscv_vse16_v_f16m1(&C[(12) * (ldc) + 8], C_reg3_4,(1));
__riscv_vse16_v_f16m1(&C[(13) * (ldc) + 8], C_reg3_5,(1));
}

// gemm_RVV_9x1_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 9] @DRAM
// )
void gemm_RVV_9x1_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
}

// gemm_RVV_9x1_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][1, 9] @DRAM
// )
void gemm_RVV_9x1_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_reg_0_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
}

// gemm_RVV_9x2_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 9] @DRAM
// )
void gemm_RVV_9x2_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
}

// gemm_RVV_9x2_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][2, 9] @DRAM
// )
void gemm_RVV_9x2_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
}

// gemm_RVV_9x3_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 9] @DRAM
// )
void gemm_RVV_9x3_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
}

// gemm_RVV_9x3_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][3, 9] @DRAM
// )
void gemm_RVV_9x3_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
}

// gemm_RVV_9x4_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 9] @DRAM
// )
void gemm_RVV_9x4_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
}

// gemm_RVV_9x4_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][4, 9] @DRAM
// )
void gemm_RVV_9x4_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
}

// gemm_RVV_9x5_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 9] @DRAM
// )
void gemm_RVV_9x5_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(1));
}

// gemm_RVV_9x5_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][5, 9] @DRAM
// )
void gemm_RVV_9x5_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(1));
}

// gemm_RVV_9x6_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 9] @DRAM
// )
void gemm_RVV_9x6_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(1));
}

// gemm_RVV_9x6_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][6, 9] @DRAM
// )
void gemm_RVV_9x6_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(1));
}

// gemm_RVV_9x7_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 9] @DRAM
// )
void gemm_RVV_9x7_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(1));
}

// gemm_RVV_9x7_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][7, 9] @DRAM
// )
void gemm_RVV_9x7_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(1));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(1));
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(1));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(1));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(1));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(1));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(1));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(1));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(1));
  B_reg_0 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb)],(8));
  B_reg_1 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 1],(8));
  B_reg_2 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 2],(8));
  B_reg_3 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 3],(8));
  B_reg_4 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 4],(8));
  B_reg_5 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 5],(8));
  B_reg_6 = __riscv_vfmv_v_f_f16m1(B[(k) * (ldb) + 6],(8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(1));
}

// gemm_RVV_9x8_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 9] @DRAM
// )
void gemm_RVV_9x8_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_regt_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(1));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(1));
}

// gemm_RVV_9x8_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][8, 9] @DRAM
// )
void gemm_RVV_9x8_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t A_reg_0;
vfloat16m1_t A_regt;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t C_regt_0;
vfloat16m1_t C_regt_1;
vfloat16m1_t C_regt_2;
vfloat16m1_t C_regt_3;
vfloat16m1_t C_regt_4;
vfloat16m1_t C_regt_5;
vfloat16m1_t C_regt_6;
vfloat16m1_t C_regt_7;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_regt_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_regt_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_regt_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_regt_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_regt_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_regt_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(1));
C_regt_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(1));
C_regt_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_regt = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_regt_0 = __riscv_vfmacc_vv_f16m1(C_regt_0, A_regt, B_reg_0,(1));
  C_regt_1 = __riscv_vfmacc_vv_f16m1(C_regt_1, A_regt, B_reg_1,(1));
  C_regt_2 = __riscv_vfmacc_vv_f16m1(C_regt_2, A_regt, B_reg_2,(1));
  C_regt_3 = __riscv_vfmacc_vv_f16m1(C_regt_3, A_regt, B_reg_3,(1));
  C_regt_4 = __riscv_vfmacc_vv_f16m1(C_regt_4, A_regt, B_reg_4,(1));
  C_regt_5 = __riscv_vfmacc_vv_f16m1(C_regt_5, A_regt, B_reg_5,(1));
  C_regt_6 = __riscv_vfmacc_vv_f16m1(C_regt_6, A_regt, B_reg_6,(1));
  C_regt_7 = __riscv_vfmacc_vv_f16m1(C_regt_7, A_regt, B_reg_7,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_regt_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_regt_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_regt_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_regt_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_regt_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_regt_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_regt_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_regt_7,(1));
}

// gemm_RVV_9x9_b0_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 9] @DRAM
// )
void gemm_RVV_9x9_b0_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg1_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_1 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_2 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_3 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_4 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_5 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_6 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg1_7 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
C_reg2_0_0 = __riscv_vfmv_v_f_f16m1(0.0f,(8));
C_reg3_0 = __riscv_vfmv_v_f_f16m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
}

// gemm_RVV_9x9_b1_col_fp16(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f16][KC, 9] @DRAM,
//     B : [f16][KC, 9] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f16][9, 9] @DRAM
// )
void gemm_RVV_9x9_b1_col_fp16( void *ctxt, int_fast32_t KC, const float* alpha, _Float16 * A, int lda, _Float16 * B, int ldb, const float* beta, _Float16 C, int ldc ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat16m1_t B_reg2_0;
vfloat16m1_t B_tmp;
vfloat16m1_t B_reg_0;
vfloat16m1_t B_reg_1;
vfloat16m1_t B_reg_2;
vfloat16m1_t B_reg_3;
vfloat16m1_t B_reg_4;
vfloat16m1_t B_reg_5;
vfloat16m1_t B_reg_6;
vfloat16m1_t B_reg_7;
vfloat16m1_t A_reg1;
vfloat16m1_t A_reg_0;
vfloat16m1_t C_reg_0_0;
vfloat16m1_t C_reg_1_0;
vfloat16m1_t C_reg_2_0;
vfloat16m1_t C_reg_3_0;
vfloat16m1_t C_reg_4_0;
vfloat16m1_t C_reg_5_0;
vfloat16m1_t C_reg_6_0;
vfloat16m1_t C_reg_7_0;
vfloat16m1_t C_reg2_0_0;
vfloat16m1_t C_reg1_0;
vfloat16m1_t C_reg1_1;
vfloat16m1_t C_reg1_2;
vfloat16m1_t C_reg1_3;
vfloat16m1_t C_reg1_4;
vfloat16m1_t C_reg1_5;
vfloat16m1_t C_reg1_6;
vfloat16m1_t C_reg1_7;
vfloat16m1_t C_reg3_0;
C_reg_0_0 = __riscv_vle16_v_f16m1(&C[0],(8));
C_reg_1_0 = __riscv_vle16_v_f16m1(&C[ldc],(8));
C_reg_2_0 = __riscv_vle16_v_f16m1(&C[(2) * (ldc)],(8));
C_reg_3_0 = __riscv_vle16_v_f16m1(&C[(3) * (ldc)],(8));
C_reg_4_0 = __riscv_vle16_v_f16m1(&C[(4) * (ldc)],(8));
C_reg_5_0 = __riscv_vle16_v_f16m1(&C[(5) * (ldc)],(8));
C_reg_6_0 = __riscv_vle16_v_f16m1(&C[(6) * (ldc)],(8));
C_reg_7_0 = __riscv_vle16_v_f16m1(&C[(7) * (ldc)],(8));
C_reg1_0 = __riscv_vle16_v_f16m1(&C[8],(1));
C_reg1_1 = __riscv_vle16_v_f16m1(&C[ldc + 8],(1));
C_reg1_2 = __riscv_vle16_v_f16m1(&C[(2) * (ldc) + 8],(1));
C_reg1_3 = __riscv_vle16_v_f16m1(&C[(3) * (ldc) + 8],(1));
C_reg1_4 = __riscv_vle16_v_f16m1(&C[(4) * (ldc) + 8],(1));
C_reg1_5 = __riscv_vle16_v_f16m1(&C[(5) * (ldc) + 8],(1));
C_reg1_6 = __riscv_vle16_v_f16m1(&C[(6) * (ldc) + 8],(1));
C_reg1_7 = __riscv_vle16_v_f16m1(&C[(7) * (ldc) + 8],(1));
C_reg2_0_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc)],(8));
C_reg3_0 = __riscv_vle16_v_f16m1(&C[(8) * (ldc) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (1));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (1));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (1));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (1));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (1));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (1));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (1));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb)],(8));
  B_reg_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_reg_1 = __riscv_vrgather_vx_f16m1(B_tmp, (1), (8));
  B_reg_2 = __riscv_vrgather_vx_f16m1(B_tmp, (2), (8));
  B_reg_3 = __riscv_vrgather_vx_f16m1(B_tmp, (3), (8));
  B_reg_4 = __riscv_vrgather_vx_f16m1(B_tmp, (4), (8));
  B_reg_5 = __riscv_vrgather_vx_f16m1(B_tmp, (5), (8));
  B_reg_6 = __riscv_vrgather_vx_f16m1(B_tmp, (6), (8));
  B_reg_7 = __riscv_vrgather_vx_f16m1(B_tmp, (7), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (8));
  B_tmp = __riscv_vle16_v_f16m1(&B[(k) * (ldb) + 8],(1));
  B_reg2_0 = __riscv_vrgather_vx_f16m1(B_tmp, (0), (1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  A_reg_0 = __riscv_vle16_v_f16m1(&A[(k) * (lda)],(8));
  A_reg1 = __riscv_vle16_v_f16m1(&A[(k) * (lda) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vv_f16m1(C_reg_0_0, A_reg_0, B_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vv_f16m1(C_reg_1_0, A_reg_0, B_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vv_f16m1(C_reg_2_0, A_reg_0, B_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vv_f16m1(C_reg_3_0, A_reg_0, B_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vv_f16m1(C_reg_4_0, A_reg_0, B_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vv_f16m1(C_reg_5_0, A_reg_0, B_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vv_f16m1(C_reg_6_0, A_reg_0, B_reg_6,(8));
  C_reg_7_0 = __riscv_vfmacc_vv_f16m1(C_reg_7_0, A_reg_0, B_reg_7,(8));
  C_reg1_0 = __riscv_vfmacc_vv_f16m1(C_reg1_0, A_reg1, B_reg_0,(1));
  C_reg1_1 = __riscv_vfmacc_vv_f16m1(C_reg1_1, A_reg1, B_reg_1,(1));
  C_reg1_2 = __riscv_vfmacc_vv_f16m1(C_reg1_2, A_reg1, B_reg_2,(1));
  C_reg1_3 = __riscv_vfmacc_vv_f16m1(C_reg1_3, A_reg1, B_reg_3,(1));
  C_reg1_4 = __riscv_vfmacc_vv_f16m1(C_reg1_4, A_reg1, B_reg_4,(1));
  C_reg1_5 = __riscv_vfmacc_vv_f16m1(C_reg1_5, A_reg1, B_reg_5,(1));
  C_reg1_6 = __riscv_vfmacc_vv_f16m1(C_reg1_6, A_reg1, B_reg_6,(1));
  C_reg1_7 = __riscv_vfmacc_vv_f16m1(C_reg1_7, A_reg1, B_reg_7,(1));
  C_reg2_0_0 = __riscv_vfmacc_vv_f16m1(C_reg2_0_0, A_reg_0, B_reg2_0,(8));
  C_reg3_0 = __riscv_vfmacc_vv_f16m1(C_reg3_0, A_reg1, B_reg2_0,(1));
}
__riscv_vse16_v_f16m1(&C[0], C_reg_0_0,(8));
__riscv_vse16_v_f16m1(&C[ldc], C_reg_1_0,(8));
__riscv_vse16_v_f16m1(&C[(2) * (ldc)], C_reg_2_0,(8));
__riscv_vse16_v_f16m1(&C[(3) * (ldc)], C_reg_3_0,(8));
__riscv_vse16_v_f16m1(&C[(4) * (ldc)], C_reg_4_0,(8));
__riscv_vse16_v_f16m1(&C[(5) * (ldc)], C_reg_5_0,(8));
__riscv_vse16_v_f16m1(&C[(6) * (ldc)], C_reg_6_0,(8));
__riscv_vse16_v_f16m1(&C[(7) * (ldc)], C_reg_7_0,(8));
__riscv_vse16_v_f16m1(&C[8], C_reg1_0,(1));
__riscv_vse16_v_f16m1(&C[ldc + 8], C_reg1_1,(1));
__riscv_vse16_v_f16m1(&C[(2) * (ldc) + 8], C_reg1_2,(1));
__riscv_vse16_v_f16m1(&C[(3) * (ldc) + 8], C_reg1_3,(1));
__riscv_vse16_v_f16m1(&C[(4) * (ldc) + 8], C_reg1_4,(1));
__riscv_vse16_v_f16m1(&C[(5) * (ldc) + 8], C_reg1_5,(1));
__riscv_vse16_v_f16m1(&C[(6) * (ldc) + 8], C_reg1_6,(1));
__riscv_vse16_v_f16m1(&C[(7) * (ldc) + 8], C_reg1_7,(1));
__riscv_vse16_v_f16m1(&C[(8) * (ldc)], C_reg2_0_0,(8));
__riscv_vse16_v_f16m1(&C[(8) * (ldc) + 8], C_reg3_0,(1));
}


/* relying on the following instruction..."
rvv_broadcast_8xf16(dst,src,vl)
{dst_data} = __riscv_vfmv_v_f_f16m1({src_data},{vl});
*/

/* relying on the following instruction..."
rvv_broadcast_8xf16_0(dst,vl)
{dst_data} = __riscv_vfmv_v_f_f16m1(0.0f,{vl});
*/

/* relying on the following instruction..."
rvv_gather_8xf16(dst,src,imm,vl)
{dst_data} = __riscv_vrgather_vx_f16m1({src_data}, {imm}, {vl});
*/

/* relying on the following instruction..."
rvv_vfmacc_8xf16_8xf16(dst,lhs,rhs,vl)
{dst_data} = __riscv_vfmacc_vv_f16m1({dst_data}, {lhs_data}, {rhs_data},{vl});
*/

/* relying on the following instruction..."
rvv_vld_8xf16(dst,src,vl)
{dst_data} = __riscv_vle16_v_f16m1(&{src_data},{vl});
*/

/* relying on the following instruction..."
rvv_vst_8xf16(dst,src,vl)
__riscv_vse16_v_f16m1(&{dst_data}, {src_data},{vl});
*/
