#include "kernels_RVV_48x8_fp32.h"



#include <stdio.h>
#include <stdlib.h>

#include <riscv_vector.h>


// gemm_RVV_10x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 10] @DRAM
// )
void gemm_RVV_10x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
}

// gemm_RVV_10x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 10] @DRAM
// )
void gemm_RVV_10x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
}

// gemm_RVV_10x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 10] @DRAM
// )
void gemm_RVV_10x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
}

// gemm_RVV_10x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 10] @DRAM
// )
void gemm_RVV_10x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
}

// gemm_RVV_10x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 10] @DRAM
// )
void gemm_RVV_10x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
}

// gemm_RVV_10x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 10] @DRAM
// )
void gemm_RVV_10x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
}

// gemm_RVV_10x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 10] @DRAM
// )
void gemm_RVV_10x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
}

// gemm_RVV_10x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 10] @DRAM
// )
void gemm_RVV_10x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
}

// gemm_RVV_10x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 10] @DRAM
// )
void gemm_RVV_10x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(2));
}

// gemm_RVV_10x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 10] @DRAM
// )
void gemm_RVV_10x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(2));
}

// gemm_RVV_10x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 10] @DRAM
// )
void gemm_RVV_10x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(2));
}

// gemm_RVV_10x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 10] @DRAM
// )
void gemm_RVV_10x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(2));
}

// gemm_RVV_10x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 10] @DRAM
// )
void gemm_RVV_10x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(2));
}

// gemm_RVV_10x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 10] @DRAM
// )
void gemm_RVV_10x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(2));
}

// gemm_RVV_10x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 10] @DRAM
// )
void gemm_RVV_10x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(2));
}

// gemm_RVV_10x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 10] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 10] @DRAM
// )
void gemm_RVV_10x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(2));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(2));
}

// gemm_RVV_11x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 11] @DRAM
// )
void gemm_RVV_11x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
}

// gemm_RVV_11x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 11] @DRAM
// )
void gemm_RVV_11x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
}

// gemm_RVV_11x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 11] @DRAM
// )
void gemm_RVV_11x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
}

// gemm_RVV_11x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 11] @DRAM
// )
void gemm_RVV_11x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
}

// gemm_RVV_11x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 11] @DRAM
// )
void gemm_RVV_11x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
}

// gemm_RVV_11x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 11] @DRAM
// )
void gemm_RVV_11x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
}

// gemm_RVV_11x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 11] @DRAM
// )
void gemm_RVV_11x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
}

// gemm_RVV_11x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 11] @DRAM
// )
void gemm_RVV_11x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
}

// gemm_RVV_11x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 11] @DRAM
// )
void gemm_RVV_11x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(3));
}

// gemm_RVV_11x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 11] @DRAM
// )
void gemm_RVV_11x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(3));
}

// gemm_RVV_11x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 11] @DRAM
// )
void gemm_RVV_11x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(3));
}

// gemm_RVV_11x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 11] @DRAM
// )
void gemm_RVV_11x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(3));
}

// gemm_RVV_11x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 11] @DRAM
// )
void gemm_RVV_11x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(3));
}

// gemm_RVV_11x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 11] @DRAM
// )
void gemm_RVV_11x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(3));
}

// gemm_RVV_11x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 11] @DRAM
// )
void gemm_RVV_11x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(3));
}

// gemm_RVV_11x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 11] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 11] @DRAM
// )
void gemm_RVV_11x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(3));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(3));
}

// gemm_RVV_12x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 12] @DRAM
// )
void gemm_RVV_12x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
}

// gemm_RVV_12x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 12] @DRAM
// )
void gemm_RVV_12x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
}

// gemm_RVV_12x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 12] @DRAM
// )
void gemm_RVV_12x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
}

// gemm_RVV_12x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 12] @DRAM
// )
void gemm_RVV_12x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
}

// gemm_RVV_12x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 12] @DRAM
// )
void gemm_RVV_12x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
}

// gemm_RVV_12x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 12] @DRAM
// )
void gemm_RVV_12x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
}

// gemm_RVV_12x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 12] @DRAM
// )
void gemm_RVV_12x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
}

// gemm_RVV_12x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 12] @DRAM
// )
void gemm_RVV_12x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
}

// gemm_RVV_12x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 12] @DRAM
// )
void gemm_RVV_12x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(4));
}

// gemm_RVV_12x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 12] @DRAM
// )
void gemm_RVV_12x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(4));
}

// gemm_RVV_12x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 12] @DRAM
// )
void gemm_RVV_12x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(4));
}

// gemm_RVV_12x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 12] @DRAM
// )
void gemm_RVV_12x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(4));
}

// gemm_RVV_12x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 12] @DRAM
// )
void gemm_RVV_12x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(4));
}

// gemm_RVV_12x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 12] @DRAM
// )
void gemm_RVV_12x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(4));
}

// gemm_RVV_12x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 12] @DRAM
// )
void gemm_RVV_12x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(4));
}

// gemm_RVV_12x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 12] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 12] @DRAM
// )
void gemm_RVV_12x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(4));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(4));
}

// gemm_RVV_13x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 13] @DRAM
// )
void gemm_RVV_13x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
}

// gemm_RVV_13x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 13] @DRAM
// )
void gemm_RVV_13x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
}

// gemm_RVV_13x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 13] @DRAM
// )
void gemm_RVV_13x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
}

// gemm_RVV_13x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 13] @DRAM
// )
void gemm_RVV_13x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
}

// gemm_RVV_13x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 13] @DRAM
// )
void gemm_RVV_13x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
}

// gemm_RVV_13x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 13] @DRAM
// )
void gemm_RVV_13x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
}

// gemm_RVV_13x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 13] @DRAM
// )
void gemm_RVV_13x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
}

// gemm_RVV_13x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 13] @DRAM
// )
void gemm_RVV_13x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
}

// gemm_RVV_13x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 13] @DRAM
// )
void gemm_RVV_13x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(5));
}

// gemm_RVV_13x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 13] @DRAM
// )
void gemm_RVV_13x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(5));
}

// gemm_RVV_13x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 13] @DRAM
// )
void gemm_RVV_13x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(5));
}

// gemm_RVV_13x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 13] @DRAM
// )
void gemm_RVV_13x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(5));
}

// gemm_RVV_13x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 13] @DRAM
// )
void gemm_RVV_13x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(5));
}

// gemm_RVV_13x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 13] @DRAM
// )
void gemm_RVV_13x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(5));
}

// gemm_RVV_13x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 13] @DRAM
// )
void gemm_RVV_13x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(5));
}

// gemm_RVV_13x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 13] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 13] @DRAM
// )
void gemm_RVV_13x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(5));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(5));
}

// gemm_RVV_14x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 14] @DRAM
// )
void gemm_RVV_14x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
}

// gemm_RVV_14x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 14] @DRAM
// )
void gemm_RVV_14x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
}

// gemm_RVV_14x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 14] @DRAM
// )
void gemm_RVV_14x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
}

// gemm_RVV_14x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 14] @DRAM
// )
void gemm_RVV_14x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
}

// gemm_RVV_14x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 14] @DRAM
// )
void gemm_RVV_14x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
}

// gemm_RVV_14x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 14] @DRAM
// )
void gemm_RVV_14x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
}

// gemm_RVV_14x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 14] @DRAM
// )
void gemm_RVV_14x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
}

// gemm_RVV_14x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 14] @DRAM
// )
void gemm_RVV_14x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
}

// gemm_RVV_14x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 14] @DRAM
// )
void gemm_RVV_14x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(6));
}

// gemm_RVV_14x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 14] @DRAM
// )
void gemm_RVV_14x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(6));
}

// gemm_RVV_14x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 14] @DRAM
// )
void gemm_RVV_14x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(6));
}

// gemm_RVV_14x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 14] @DRAM
// )
void gemm_RVV_14x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(6));
}

// gemm_RVV_14x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 14] @DRAM
// )
void gemm_RVV_14x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(6));
}

// gemm_RVV_14x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 14] @DRAM
// )
void gemm_RVV_14x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(6));
}

// gemm_RVV_14x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 14] @DRAM
// )
void gemm_RVV_14x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(6));
}

// gemm_RVV_14x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 14] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 14] @DRAM
// )
void gemm_RVV_14x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(6));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(6));
}

// gemm_RVV_15x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 15] @DRAM
// )
void gemm_RVV_15x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
}

// gemm_RVV_15x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 15] @DRAM
// )
void gemm_RVV_15x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
}

// gemm_RVV_15x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 15] @DRAM
// )
void gemm_RVV_15x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
}

// gemm_RVV_15x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 15] @DRAM
// )
void gemm_RVV_15x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
}

// gemm_RVV_15x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 15] @DRAM
// )
void gemm_RVV_15x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
}

// gemm_RVV_15x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 15] @DRAM
// )
void gemm_RVV_15x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
}

// gemm_RVV_15x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 15] @DRAM
// )
void gemm_RVV_15x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
}

// gemm_RVV_15x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 15] @DRAM
// )
void gemm_RVV_15x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
}

// gemm_RVV_15x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 15] @DRAM
// )
void gemm_RVV_15x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(7));
}

// gemm_RVV_15x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 15] @DRAM
// )
void gemm_RVV_15x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(7));
}

// gemm_RVV_15x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 15] @DRAM
// )
void gemm_RVV_15x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(7));
}

// gemm_RVV_15x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 15] @DRAM
// )
void gemm_RVV_15x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(7));
}

// gemm_RVV_15x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 15] @DRAM
// )
void gemm_RVV_15x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(7));
}

// gemm_RVV_15x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 15] @DRAM
// )
void gemm_RVV_15x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(7));
}

// gemm_RVV_15x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 15] @DRAM
// )
void gemm_RVV_15x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(7));
}

// gemm_RVV_15x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 15] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 15] @DRAM
// )
void gemm_RVV_15x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(7));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(7));
}

// gemm_RVV_16x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 16] @DRAM
// )
void gemm_RVV_16x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
}

// gemm_RVV_16x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 16] @DRAM
// )
void gemm_RVV_16x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
}

// gemm_RVV_16x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 16] @DRAM
// )
void gemm_RVV_16x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
}

// gemm_RVV_16x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 16] @DRAM
// )
void gemm_RVV_16x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
}

// gemm_RVV_16x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 16] @DRAM
// )
void gemm_RVV_16x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
}

// gemm_RVV_16x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 16] @DRAM
// )
void gemm_RVV_16x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
}

// gemm_RVV_16x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 16] @DRAM
// )
void gemm_RVV_16x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
}

// gemm_RVV_16x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 16] @DRAM
// )
void gemm_RVV_16x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
}

// gemm_RVV_16x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 16] @DRAM
// )
void gemm_RVV_16x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
}

// gemm_RVV_16x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 16] @DRAM
// )
void gemm_RVV_16x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
}

// gemm_RVV_16x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 16] @DRAM
// )
void gemm_RVV_16x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
}

// gemm_RVV_16x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 16] @DRAM
// )
void gemm_RVV_16x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
}

// gemm_RVV_16x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 16] @DRAM
// )
void gemm_RVV_16x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
}

// gemm_RVV_16x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 16] @DRAM
// )
void gemm_RVV_16x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
}

// gemm_RVV_16x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 16] @DRAM
// )
void gemm_RVV_16x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
}

// gemm_RVV_16x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 16] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 16] @DRAM
// )
void gemm_RVV_16x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
}

// gemm_RVV_17x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 17] @DRAM
// )
void gemm_RVV_17x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
}

// gemm_RVV_17x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 17] @DRAM
// )
void gemm_RVV_17x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
}

// gemm_RVV_17x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 17] @DRAM
// )
void gemm_RVV_17x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
}

// gemm_RVV_17x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 17] @DRAM
// )
void gemm_RVV_17x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
}

// gemm_RVV_17x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 17] @DRAM
// )
void gemm_RVV_17x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
}

// gemm_RVV_17x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 17] @DRAM
// )
void gemm_RVV_17x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
}

// gemm_RVV_17x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 17] @DRAM
// )
void gemm_RVV_17x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
}

// gemm_RVV_17x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 17] @DRAM
// )
void gemm_RVV_17x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
}

// gemm_RVV_17x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 17] @DRAM
// )
void gemm_RVV_17x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(1));
}

// gemm_RVV_17x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 17] @DRAM
// )
void gemm_RVV_17x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(1));
}

// gemm_RVV_17x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 17] @DRAM
// )
void gemm_RVV_17x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(1));
}

// gemm_RVV_17x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 17] @DRAM
// )
void gemm_RVV_17x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(1));
}

// gemm_RVV_17x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 17] @DRAM
// )
void gemm_RVV_17x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(1));
}

// gemm_RVV_17x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 17] @DRAM
// )
void gemm_RVV_17x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(1));
}

// gemm_RVV_17x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 17] @DRAM
// )
void gemm_RVV_17x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(1));
}

// gemm_RVV_17x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 17] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 17] @DRAM
// )
void gemm_RVV_17x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(1));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(1));
}

// gemm_RVV_18x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 18] @DRAM
// )
void gemm_RVV_18x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
}

// gemm_RVV_18x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 18] @DRAM
// )
void gemm_RVV_18x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
}

// gemm_RVV_18x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 18] @DRAM
// )
void gemm_RVV_18x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
}

// gemm_RVV_18x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 18] @DRAM
// )
void gemm_RVV_18x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
}

// gemm_RVV_18x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 18] @DRAM
// )
void gemm_RVV_18x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
}

// gemm_RVV_18x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 18] @DRAM
// )
void gemm_RVV_18x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
}

// gemm_RVV_18x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 18] @DRAM
// )
void gemm_RVV_18x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
}

// gemm_RVV_18x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 18] @DRAM
// )
void gemm_RVV_18x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
}

// gemm_RVV_18x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 18] @DRAM
// )
void gemm_RVV_18x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(2));
}

// gemm_RVV_18x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 18] @DRAM
// )
void gemm_RVV_18x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(2));
}

// gemm_RVV_18x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 18] @DRAM
// )
void gemm_RVV_18x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(2));
}

// gemm_RVV_18x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 18] @DRAM
// )
void gemm_RVV_18x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(2));
}

// gemm_RVV_18x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 18] @DRAM
// )
void gemm_RVV_18x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(2));
}

// gemm_RVV_18x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 18] @DRAM
// )
void gemm_RVV_18x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(2));
}

// gemm_RVV_18x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 18] @DRAM
// )
void gemm_RVV_18x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(2));
}

// gemm_RVV_18x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 18] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 18] @DRAM
// )
void gemm_RVV_18x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(2));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(2));
}

// gemm_RVV_19x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 19] @DRAM
// )
void gemm_RVV_19x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
}

// gemm_RVV_19x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 19] @DRAM
// )
void gemm_RVV_19x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
}

// gemm_RVV_19x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 19] @DRAM
// )
void gemm_RVV_19x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
}

// gemm_RVV_19x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 19] @DRAM
// )
void gemm_RVV_19x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
}

// gemm_RVV_19x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 19] @DRAM
// )
void gemm_RVV_19x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
}

// gemm_RVV_19x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 19] @DRAM
// )
void gemm_RVV_19x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
}

// gemm_RVV_19x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 19] @DRAM
// )
void gemm_RVV_19x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
}

// gemm_RVV_19x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 19] @DRAM
// )
void gemm_RVV_19x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
}

// gemm_RVV_19x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 19] @DRAM
// )
void gemm_RVV_19x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(3));
}

// gemm_RVV_19x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 19] @DRAM
// )
void gemm_RVV_19x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(3));
}

// gemm_RVV_19x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 19] @DRAM
// )
void gemm_RVV_19x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(3));
}

// gemm_RVV_19x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 19] @DRAM
// )
void gemm_RVV_19x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(3));
}

// gemm_RVV_19x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 19] @DRAM
// )
void gemm_RVV_19x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(3));
}

// gemm_RVV_19x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 19] @DRAM
// )
void gemm_RVV_19x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(3));
}

// gemm_RVV_19x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 19] @DRAM
// )
void gemm_RVV_19x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(3));
}

// gemm_RVV_19x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 19] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 19] @DRAM
// )
void gemm_RVV_19x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(3));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(3));
}

// gemm_RVV_1x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 1] @DRAM
// )
void gemm_RVV_1x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
}

// gemm_RVV_1x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 1] @DRAM
// )
void gemm_RVV_1x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
}

// gemm_RVV_1x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 1] @DRAM
// )
void gemm_RVV_1x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
}

// gemm_RVV_1x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 1] @DRAM
// )
void gemm_RVV_1x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
}

// gemm_RVV_1x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 1] @DRAM
// )
void gemm_RVV_1x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
}

// gemm_RVV_1x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 1] @DRAM
// )
void gemm_RVV_1x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
}

// gemm_RVV_1x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 1] @DRAM
// )
void gemm_RVV_1x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
}

// gemm_RVV_1x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 1] @DRAM
// )
void gemm_RVV_1x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
}

// gemm_RVV_1x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 1] @DRAM
// )
void gemm_RVV_1x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
}

// gemm_RVV_1x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 1] @DRAM
// )
void gemm_RVV_1x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
}

// gemm_RVV_1x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 1] @DRAM
// )
void gemm_RVV_1x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
}

// gemm_RVV_1x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 1] @DRAM
// )
void gemm_RVV_1x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
}

// gemm_RVV_1x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 1] @DRAM
// )
void gemm_RVV_1x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
}

// gemm_RVV_1x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 1] @DRAM
// )
void gemm_RVV_1x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
}

// gemm_RVV_1x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 1] @DRAM
// )
void gemm_RVV_1x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
}

// gemm_RVV_1x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 1] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 1] @DRAM
// )
void gemm_RVV_1x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(1));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(1));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(1));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(1));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(1));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(1));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(1));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(1));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(1));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(1));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(1));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(1));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(1));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(1));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(1));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(1));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(1));
}

// gemm_RVV_20x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 20] @DRAM
// )
void gemm_RVV_20x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
}

// gemm_RVV_20x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 20] @DRAM
// )
void gemm_RVV_20x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
}

// gemm_RVV_20x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 20] @DRAM
// )
void gemm_RVV_20x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
}

// gemm_RVV_20x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 20] @DRAM
// )
void gemm_RVV_20x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
}

// gemm_RVV_20x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 20] @DRAM
// )
void gemm_RVV_20x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
}

// gemm_RVV_20x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 20] @DRAM
// )
void gemm_RVV_20x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
}

// gemm_RVV_20x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 20] @DRAM
// )
void gemm_RVV_20x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
}

// gemm_RVV_20x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 20] @DRAM
// )
void gemm_RVV_20x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
}

// gemm_RVV_20x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 20] @DRAM
// )
void gemm_RVV_20x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(4));
}

// gemm_RVV_20x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 20] @DRAM
// )
void gemm_RVV_20x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(4));
}

// gemm_RVV_20x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 20] @DRAM
// )
void gemm_RVV_20x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(4));
}

// gemm_RVV_20x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 20] @DRAM
// )
void gemm_RVV_20x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(4));
}

// gemm_RVV_20x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 20] @DRAM
// )
void gemm_RVV_20x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(4));
}

// gemm_RVV_20x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 20] @DRAM
// )
void gemm_RVV_20x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(4));
}

// gemm_RVV_20x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 20] @DRAM
// )
void gemm_RVV_20x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(4));
}

// gemm_RVV_20x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 20] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 20] @DRAM
// )
void gemm_RVV_20x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(4));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(4));
}

// gemm_RVV_21x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 21] @DRAM
// )
void gemm_RVV_21x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
}

// gemm_RVV_21x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 21] @DRAM
// )
void gemm_RVV_21x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
}

// gemm_RVV_21x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 21] @DRAM
// )
void gemm_RVV_21x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
}

// gemm_RVV_21x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 21] @DRAM
// )
void gemm_RVV_21x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
}

// gemm_RVV_21x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 21] @DRAM
// )
void gemm_RVV_21x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
}

// gemm_RVV_21x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 21] @DRAM
// )
void gemm_RVV_21x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
}

// gemm_RVV_21x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 21] @DRAM
// )
void gemm_RVV_21x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
}

// gemm_RVV_21x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 21] @DRAM
// )
void gemm_RVV_21x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
}

// gemm_RVV_21x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 21] @DRAM
// )
void gemm_RVV_21x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(5));
}

// gemm_RVV_21x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 21] @DRAM
// )
void gemm_RVV_21x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(5));
}

// gemm_RVV_21x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 21] @DRAM
// )
void gemm_RVV_21x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(5));
}

// gemm_RVV_21x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 21] @DRAM
// )
void gemm_RVV_21x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(5));
}

// gemm_RVV_21x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 21] @DRAM
// )
void gemm_RVV_21x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(5));
}

// gemm_RVV_21x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 21] @DRAM
// )
void gemm_RVV_21x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(5));
}

// gemm_RVV_21x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 21] @DRAM
// )
void gemm_RVV_21x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(5));
}

// gemm_RVV_21x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 21] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 21] @DRAM
// )
void gemm_RVV_21x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(5));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(5));
}

// gemm_RVV_22x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 22] @DRAM
// )
void gemm_RVV_22x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
}

// gemm_RVV_22x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 22] @DRAM
// )
void gemm_RVV_22x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
}

// gemm_RVV_22x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 22] @DRAM
// )
void gemm_RVV_22x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
}

// gemm_RVV_22x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 22] @DRAM
// )
void gemm_RVV_22x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
}

// gemm_RVV_22x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 22] @DRAM
// )
void gemm_RVV_22x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
}

// gemm_RVV_22x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 22] @DRAM
// )
void gemm_RVV_22x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
}

// gemm_RVV_22x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 22] @DRAM
// )
void gemm_RVV_22x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
}

// gemm_RVV_22x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 22] @DRAM
// )
void gemm_RVV_22x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
}

// gemm_RVV_22x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 22] @DRAM
// )
void gemm_RVV_22x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(6));
}

// gemm_RVV_22x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 22] @DRAM
// )
void gemm_RVV_22x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(6));
}

// gemm_RVV_22x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 22] @DRAM
// )
void gemm_RVV_22x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(6));
}

// gemm_RVV_22x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 22] @DRAM
// )
void gemm_RVV_22x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(6));
}

// gemm_RVV_22x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 22] @DRAM
// )
void gemm_RVV_22x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(6));
}

// gemm_RVV_22x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 22] @DRAM
// )
void gemm_RVV_22x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(6));
}

// gemm_RVV_22x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 22] @DRAM
// )
void gemm_RVV_22x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(6));
}

// gemm_RVV_22x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 22] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 22] @DRAM
// )
void gemm_RVV_22x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(6));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(6));
}

// gemm_RVV_23x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 23] @DRAM
// )
void gemm_RVV_23x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
}

// gemm_RVV_23x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 23] @DRAM
// )
void gemm_RVV_23x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
}

// gemm_RVV_23x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 23] @DRAM
// )
void gemm_RVV_23x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
}

// gemm_RVV_23x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 23] @DRAM
// )
void gemm_RVV_23x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
}

// gemm_RVV_23x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 23] @DRAM
// )
void gemm_RVV_23x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
}

// gemm_RVV_23x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 23] @DRAM
// )
void gemm_RVV_23x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
}

// gemm_RVV_23x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 23] @DRAM
// )
void gemm_RVV_23x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
}

// gemm_RVV_23x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 23] @DRAM
// )
void gemm_RVV_23x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
}

// gemm_RVV_23x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 23] @DRAM
// )
void gemm_RVV_23x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(7));
}

// gemm_RVV_23x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 23] @DRAM
// )
void gemm_RVV_23x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(7));
}

// gemm_RVV_23x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 23] @DRAM
// )
void gemm_RVV_23x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(7));
}

// gemm_RVV_23x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 23] @DRAM
// )
void gemm_RVV_23x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(7));
}

// gemm_RVV_23x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 23] @DRAM
// )
void gemm_RVV_23x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(7));
}

// gemm_RVV_23x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 23] @DRAM
// )
void gemm_RVV_23x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(7));
}

// gemm_RVV_23x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 23] @DRAM
// )
void gemm_RVV_23x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(7));
}

// gemm_RVV_23x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 23] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 23] @DRAM
// )
void gemm_RVV_23x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[16],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(7));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_regt_7,(7));
}

// gemm_RVV_24x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 24] @DRAM
// )
void gemm_RVV_24x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
}

// gemm_RVV_24x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 24] @DRAM
// )
void gemm_RVV_24x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
}

// gemm_RVV_24x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 24] @DRAM
// )
void gemm_RVV_24x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
}

// gemm_RVV_24x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 24] @DRAM
// )
void gemm_RVV_24x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
}

// gemm_RVV_24x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 24] @DRAM
// )
void gemm_RVV_24x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
}

// gemm_RVV_24x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 24] @DRAM
// )
void gemm_RVV_24x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
}

// gemm_RVV_24x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 24] @DRAM
// )
void gemm_RVV_24x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
}

// gemm_RVV_24x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 24] @DRAM
// )
void gemm_RVV_24x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
}

// gemm_RVV_24x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 24] @DRAM
// )
void gemm_RVV_24x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
}

// gemm_RVV_24x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 24] @DRAM
// )
void gemm_RVV_24x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
}

// gemm_RVV_24x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 24] @DRAM
// )
void gemm_RVV_24x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
}

// gemm_RVV_24x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 24] @DRAM
// )
void gemm_RVV_24x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
}

// gemm_RVV_24x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 24] @DRAM
// )
void gemm_RVV_24x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
}

// gemm_RVV_24x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 24] @DRAM
// )
void gemm_RVV_24x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
}

// gemm_RVV_24x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 24] @DRAM
// )
void gemm_RVV_24x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
}

// gemm_RVV_24x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 24] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 24] @DRAM
// )
void gemm_RVV_24x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
}

// gemm_RVV_25x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 25] @DRAM
// )
void gemm_RVV_25x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
}

// gemm_RVV_25x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 25] @DRAM
// )
void gemm_RVV_25x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
}

// gemm_RVV_25x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 25] @DRAM
// )
void gemm_RVV_25x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
}

// gemm_RVV_25x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 25] @DRAM
// )
void gemm_RVV_25x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
}

// gemm_RVV_25x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 25] @DRAM
// )
void gemm_RVV_25x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
}

// gemm_RVV_25x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 25] @DRAM
// )
void gemm_RVV_25x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
}

// gemm_RVV_25x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 25] @DRAM
// )
void gemm_RVV_25x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
}

// gemm_RVV_25x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 25] @DRAM
// )
void gemm_RVV_25x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
}

// gemm_RVV_25x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 25] @DRAM
// )
void gemm_RVV_25x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(1));
}

// gemm_RVV_25x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 25] @DRAM
// )
void gemm_RVV_25x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(1));
}

// gemm_RVV_25x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 25] @DRAM
// )
void gemm_RVV_25x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(1));
}

// gemm_RVV_25x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 25] @DRAM
// )
void gemm_RVV_25x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(1));
}

// gemm_RVV_25x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 25] @DRAM
// )
void gemm_RVV_25x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(1));
}

// gemm_RVV_25x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 25] @DRAM
// )
void gemm_RVV_25x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(1));
}

// gemm_RVV_25x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 25] @DRAM
// )
void gemm_RVV_25x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(1));
}

// gemm_RVV_25x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 25] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 25] @DRAM
// )
void gemm_RVV_25x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(1));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(1));
}

// gemm_RVV_26x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 26] @DRAM
// )
void gemm_RVV_26x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
}

// gemm_RVV_26x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 26] @DRAM
// )
void gemm_RVV_26x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
}

// gemm_RVV_26x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 26] @DRAM
// )
void gemm_RVV_26x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
}

// gemm_RVV_26x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 26] @DRAM
// )
void gemm_RVV_26x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
}

// gemm_RVV_26x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 26] @DRAM
// )
void gemm_RVV_26x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
}

// gemm_RVV_26x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 26] @DRAM
// )
void gemm_RVV_26x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
}

// gemm_RVV_26x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 26] @DRAM
// )
void gemm_RVV_26x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
}

// gemm_RVV_26x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 26] @DRAM
// )
void gemm_RVV_26x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
}

// gemm_RVV_26x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 26] @DRAM
// )
void gemm_RVV_26x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(2));
}

// gemm_RVV_26x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 26] @DRAM
// )
void gemm_RVV_26x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(2));
}

// gemm_RVV_26x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 26] @DRAM
// )
void gemm_RVV_26x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(2));
}

// gemm_RVV_26x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 26] @DRAM
// )
void gemm_RVV_26x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(2));
}

// gemm_RVV_26x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 26] @DRAM
// )
void gemm_RVV_26x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(2));
}

// gemm_RVV_26x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 26] @DRAM
// )
void gemm_RVV_26x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(2));
}

// gemm_RVV_26x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 26] @DRAM
// )
void gemm_RVV_26x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(2));
}

// gemm_RVV_26x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 26] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 26] @DRAM
// )
void gemm_RVV_26x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(2));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(2));
}

// gemm_RVV_27x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 27] @DRAM
// )
void gemm_RVV_27x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
}

// gemm_RVV_27x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 27] @DRAM
// )
void gemm_RVV_27x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
}

// gemm_RVV_27x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 27] @DRAM
// )
void gemm_RVV_27x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
}

// gemm_RVV_27x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 27] @DRAM
// )
void gemm_RVV_27x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
}

// gemm_RVV_27x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 27] @DRAM
// )
void gemm_RVV_27x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
}

// gemm_RVV_27x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 27] @DRAM
// )
void gemm_RVV_27x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
}

// gemm_RVV_27x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 27] @DRAM
// )
void gemm_RVV_27x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
}

// gemm_RVV_27x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 27] @DRAM
// )
void gemm_RVV_27x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
}

// gemm_RVV_27x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 27] @DRAM
// )
void gemm_RVV_27x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(3));
}

// gemm_RVV_27x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 27] @DRAM
// )
void gemm_RVV_27x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(3));
}

// gemm_RVV_27x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 27] @DRAM
// )
void gemm_RVV_27x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(3));
}

// gemm_RVV_27x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 27] @DRAM
// )
void gemm_RVV_27x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(3));
}

// gemm_RVV_27x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 27] @DRAM
// )
void gemm_RVV_27x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(3));
}

// gemm_RVV_27x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 27] @DRAM
// )
void gemm_RVV_27x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(3));
}

// gemm_RVV_27x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 27] @DRAM
// )
void gemm_RVV_27x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(3));
}

// gemm_RVV_27x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 27] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 27] @DRAM
// )
void gemm_RVV_27x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(3));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(3));
}

// gemm_RVV_28x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 28] @DRAM
// )
void gemm_RVV_28x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
}

// gemm_RVV_28x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 28] @DRAM
// )
void gemm_RVV_28x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
}

// gemm_RVV_28x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 28] @DRAM
// )
void gemm_RVV_28x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
}

// gemm_RVV_28x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 28] @DRAM
// )
void gemm_RVV_28x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
}

// gemm_RVV_28x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 28] @DRAM
// )
void gemm_RVV_28x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
}

// gemm_RVV_28x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 28] @DRAM
// )
void gemm_RVV_28x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
}

// gemm_RVV_28x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 28] @DRAM
// )
void gemm_RVV_28x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
}

// gemm_RVV_28x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 28] @DRAM
// )
void gemm_RVV_28x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
}

// gemm_RVV_28x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 28] @DRAM
// )
void gemm_RVV_28x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(4));
}

// gemm_RVV_28x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 28] @DRAM
// )
void gemm_RVV_28x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(4));
}

// gemm_RVV_28x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 28] @DRAM
// )
void gemm_RVV_28x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(4));
}

// gemm_RVV_28x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 28] @DRAM
// )
void gemm_RVV_28x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(4));
}

// gemm_RVV_28x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 28] @DRAM
// )
void gemm_RVV_28x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(4));
}

// gemm_RVV_28x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 28] @DRAM
// )
void gemm_RVV_28x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(4));
}

// gemm_RVV_28x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 28] @DRAM
// )
void gemm_RVV_28x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(4));
}

// gemm_RVV_28x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 28] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 28] @DRAM
// )
void gemm_RVV_28x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(4));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(4));
}

// gemm_RVV_29x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 29] @DRAM
// )
void gemm_RVV_29x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
}

// gemm_RVV_29x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 29] @DRAM
// )
void gemm_RVV_29x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
}

// gemm_RVV_29x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 29] @DRAM
// )
void gemm_RVV_29x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
}

// gemm_RVV_29x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 29] @DRAM
// )
void gemm_RVV_29x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
}

// gemm_RVV_29x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 29] @DRAM
// )
void gemm_RVV_29x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
}

// gemm_RVV_29x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 29] @DRAM
// )
void gemm_RVV_29x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
}

// gemm_RVV_29x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 29] @DRAM
// )
void gemm_RVV_29x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
}

// gemm_RVV_29x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 29] @DRAM
// )
void gemm_RVV_29x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
}

// gemm_RVV_29x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 29] @DRAM
// )
void gemm_RVV_29x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(5));
}

// gemm_RVV_29x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 29] @DRAM
// )
void gemm_RVV_29x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(5));
}

// gemm_RVV_29x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 29] @DRAM
// )
void gemm_RVV_29x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(5));
}

// gemm_RVV_29x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 29] @DRAM
// )
void gemm_RVV_29x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(5));
}

// gemm_RVV_29x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 29] @DRAM
// )
void gemm_RVV_29x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(5));
}

// gemm_RVV_29x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 29] @DRAM
// )
void gemm_RVV_29x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(5));
}

// gemm_RVV_29x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 29] @DRAM
// )
void gemm_RVV_29x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(5));
}

// gemm_RVV_29x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 29] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 29] @DRAM
// )
void gemm_RVV_29x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(5));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(5));
}

// gemm_RVV_2x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 2] @DRAM
// )
void gemm_RVV_2x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
}

// gemm_RVV_2x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 2] @DRAM
// )
void gemm_RVV_2x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
}

// gemm_RVV_2x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 2] @DRAM
// )
void gemm_RVV_2x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
}

// gemm_RVV_2x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 2] @DRAM
// )
void gemm_RVV_2x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
}

// gemm_RVV_2x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 2] @DRAM
// )
void gemm_RVV_2x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
}

// gemm_RVV_2x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 2] @DRAM
// )
void gemm_RVV_2x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
}

// gemm_RVV_2x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 2] @DRAM
// )
void gemm_RVV_2x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
}

// gemm_RVV_2x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 2] @DRAM
// )
void gemm_RVV_2x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
}

// gemm_RVV_2x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 2] @DRAM
// )
void gemm_RVV_2x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
}

// gemm_RVV_2x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 2] @DRAM
// )
void gemm_RVV_2x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
}

// gemm_RVV_2x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 2] @DRAM
// )
void gemm_RVV_2x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
}

// gemm_RVV_2x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 2] @DRAM
// )
void gemm_RVV_2x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
}

// gemm_RVV_2x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 2] @DRAM
// )
void gemm_RVV_2x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
}

// gemm_RVV_2x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 2] @DRAM
// )
void gemm_RVV_2x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
}

// gemm_RVV_2x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 2] @DRAM
// )
void gemm_RVV_2x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
}

// gemm_RVV_2x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 2] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 2] @DRAM
// )
void gemm_RVV_2x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(2));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(2));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(2));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(2));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(2));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(2));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(2));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(2));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(2));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(2));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(2));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(2));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(2));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(2));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(2));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(2));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(2));
}

// gemm_RVV_30x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 30] @DRAM
// )
void gemm_RVV_30x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
}

// gemm_RVV_30x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 30] @DRAM
// )
void gemm_RVV_30x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
}

// gemm_RVV_30x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 30] @DRAM
// )
void gemm_RVV_30x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
}

// gemm_RVV_30x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 30] @DRAM
// )
void gemm_RVV_30x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
}

// gemm_RVV_30x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 30] @DRAM
// )
void gemm_RVV_30x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
}

// gemm_RVV_30x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 30] @DRAM
// )
void gemm_RVV_30x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
}

// gemm_RVV_30x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 30] @DRAM
// )
void gemm_RVV_30x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
}

// gemm_RVV_30x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 30] @DRAM
// )
void gemm_RVV_30x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
}

// gemm_RVV_30x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 30] @DRAM
// )
void gemm_RVV_30x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(6));
}

// gemm_RVV_30x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 30] @DRAM
// )
void gemm_RVV_30x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(6));
}

// gemm_RVV_30x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 30] @DRAM
// )
void gemm_RVV_30x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(6));
}

// gemm_RVV_30x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 30] @DRAM
// )
void gemm_RVV_30x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(6));
}

// gemm_RVV_30x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 30] @DRAM
// )
void gemm_RVV_30x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(6));
}

// gemm_RVV_30x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 30] @DRAM
// )
void gemm_RVV_30x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(6));
}

// gemm_RVV_30x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 30] @DRAM
// )
void gemm_RVV_30x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(6));
}

// gemm_RVV_30x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 30] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 30] @DRAM
// )
void gemm_RVV_30x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(6));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(6));
}

// gemm_RVV_31x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 31] @DRAM
// )
void gemm_RVV_31x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
}

// gemm_RVV_31x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 31] @DRAM
// )
void gemm_RVV_31x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
}

// gemm_RVV_31x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 31] @DRAM
// )
void gemm_RVV_31x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
}

// gemm_RVV_31x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 31] @DRAM
// )
void gemm_RVV_31x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
}

// gemm_RVV_31x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 31] @DRAM
// )
void gemm_RVV_31x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
}

// gemm_RVV_31x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 31] @DRAM
// )
void gemm_RVV_31x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
}

// gemm_RVV_31x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 31] @DRAM
// )
void gemm_RVV_31x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
}

// gemm_RVV_31x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 31] @DRAM
// )
void gemm_RVV_31x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
}

// gemm_RVV_31x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 31] @DRAM
// )
void gemm_RVV_31x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(7));
}

// gemm_RVV_31x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 31] @DRAM
// )
void gemm_RVV_31x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(7));
}

// gemm_RVV_31x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 31] @DRAM
// )
void gemm_RVV_31x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(7));
}

// gemm_RVV_31x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 31] @DRAM
// )
void gemm_RVV_31x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(7));
}

// gemm_RVV_31x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 31] @DRAM
// )
void gemm_RVV_31x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(7));
}

// gemm_RVV_31x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 31] @DRAM
// )
void gemm_RVV_31x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(7));
}

// gemm_RVV_31x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 31] @DRAM
// )
void gemm_RVV_31x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(7));
}

// gemm_RVV_31x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 31] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 31] @DRAM
// )
void gemm_RVV_31x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[24],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(7));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_regt_7,(7));
}

// gemm_RVV_32x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 32] @DRAM
// )
void gemm_RVV_32x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
}

// gemm_RVV_32x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 32] @DRAM
// )
void gemm_RVV_32x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
}

// gemm_RVV_32x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 32] @DRAM
// )
void gemm_RVV_32x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
}

// gemm_RVV_32x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 32] @DRAM
// )
void gemm_RVV_32x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
}

// gemm_RVV_32x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 32] @DRAM
// )
void gemm_RVV_32x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
}

// gemm_RVV_32x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 32] @DRAM
// )
void gemm_RVV_32x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
}

// gemm_RVV_32x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 32] @DRAM
// )
void gemm_RVV_32x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
}

// gemm_RVV_32x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 32] @DRAM
// )
void gemm_RVV_32x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
}

// gemm_RVV_32x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 32] @DRAM
// )
void gemm_RVV_32x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
}

// gemm_RVV_32x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 32] @DRAM
// )
void gemm_RVV_32x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
}

// gemm_RVV_32x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 32] @DRAM
// )
void gemm_RVV_32x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
}

// gemm_RVV_32x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 32] @DRAM
// )
void gemm_RVV_32x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
}

// gemm_RVV_32x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 32] @DRAM
// )
void gemm_RVV_32x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
}

// gemm_RVV_32x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 32] @DRAM
// )
void gemm_RVV_32x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
}

// gemm_RVV_32x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 32] @DRAM
// )
void gemm_RVV_32x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
}

// gemm_RVV_32x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 32] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 32] @DRAM
// )
void gemm_RVV_32x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
}

// gemm_RVV_33x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 33] @DRAM
// )
void gemm_RVV_33x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
}

// gemm_RVV_33x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 33] @DRAM
// )
void gemm_RVV_33x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
}

// gemm_RVV_33x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 33] @DRAM
// )
void gemm_RVV_33x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
}

// gemm_RVV_33x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 33] @DRAM
// )
void gemm_RVV_33x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
}

// gemm_RVV_33x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 33] @DRAM
// )
void gemm_RVV_33x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
}

// gemm_RVV_33x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 33] @DRAM
// )
void gemm_RVV_33x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
}

// gemm_RVV_33x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 33] @DRAM
// )
void gemm_RVV_33x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
}

// gemm_RVV_33x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 33] @DRAM
// )
void gemm_RVV_33x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
}

// gemm_RVV_33x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 33] @DRAM
// )
void gemm_RVV_33x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(1));
}

// gemm_RVV_33x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 33] @DRAM
// )
void gemm_RVV_33x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(1));
}

// gemm_RVV_33x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 33] @DRAM
// )
void gemm_RVV_33x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(1));
}

// gemm_RVV_33x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 33] @DRAM
// )
void gemm_RVV_33x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(1));
}

// gemm_RVV_33x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 33] @DRAM
// )
void gemm_RVV_33x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(1));
}

// gemm_RVV_33x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 33] @DRAM
// )
void gemm_RVV_33x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(1));
}

// gemm_RVV_33x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 33] @DRAM
// )
void gemm_RVV_33x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(1));
}

// gemm_RVV_33x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 33] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 33] @DRAM
// )
void gemm_RVV_33x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(1));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(1));
}

// gemm_RVV_34x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 34] @DRAM
// )
void gemm_RVV_34x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
}

// gemm_RVV_34x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 34] @DRAM
// )
void gemm_RVV_34x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
}

// gemm_RVV_34x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 34] @DRAM
// )
void gemm_RVV_34x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
}

// gemm_RVV_34x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 34] @DRAM
// )
void gemm_RVV_34x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
}

// gemm_RVV_34x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 34] @DRAM
// )
void gemm_RVV_34x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
}

// gemm_RVV_34x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 34] @DRAM
// )
void gemm_RVV_34x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
}

// gemm_RVV_34x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 34] @DRAM
// )
void gemm_RVV_34x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
}

// gemm_RVV_34x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 34] @DRAM
// )
void gemm_RVV_34x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
}

// gemm_RVV_34x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 34] @DRAM
// )
void gemm_RVV_34x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(2));
}

// gemm_RVV_34x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 34] @DRAM
// )
void gemm_RVV_34x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(2));
}

// gemm_RVV_34x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 34] @DRAM
// )
void gemm_RVV_34x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(2));
}

// gemm_RVV_34x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 34] @DRAM
// )
void gemm_RVV_34x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(2));
}

// gemm_RVV_34x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 34] @DRAM
// )
void gemm_RVV_34x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(2));
}

// gemm_RVV_34x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 34] @DRAM
// )
void gemm_RVV_34x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(2));
}

// gemm_RVV_34x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 34] @DRAM
// )
void gemm_RVV_34x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(2));
}

// gemm_RVV_34x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 34] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 34] @DRAM
// )
void gemm_RVV_34x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(2));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(2));
}

// gemm_RVV_35x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 35] @DRAM
// )
void gemm_RVV_35x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
}

// gemm_RVV_35x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 35] @DRAM
// )
void gemm_RVV_35x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
}

// gemm_RVV_35x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 35] @DRAM
// )
void gemm_RVV_35x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
}

// gemm_RVV_35x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 35] @DRAM
// )
void gemm_RVV_35x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
}

// gemm_RVV_35x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 35] @DRAM
// )
void gemm_RVV_35x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
}

// gemm_RVV_35x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 35] @DRAM
// )
void gemm_RVV_35x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
}

// gemm_RVV_35x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 35] @DRAM
// )
void gemm_RVV_35x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
}

// gemm_RVV_35x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 35] @DRAM
// )
void gemm_RVV_35x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
}

// gemm_RVV_35x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 35] @DRAM
// )
void gemm_RVV_35x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(3));
}

// gemm_RVV_35x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 35] @DRAM
// )
void gemm_RVV_35x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(3));
}

// gemm_RVV_35x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 35] @DRAM
// )
void gemm_RVV_35x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(3));
}

// gemm_RVV_35x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 35] @DRAM
// )
void gemm_RVV_35x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(3));
}

// gemm_RVV_35x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 35] @DRAM
// )
void gemm_RVV_35x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(3));
}

// gemm_RVV_35x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 35] @DRAM
// )
void gemm_RVV_35x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(3));
}

// gemm_RVV_35x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 35] @DRAM
// )
void gemm_RVV_35x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(3));
}

// gemm_RVV_35x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 35] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 35] @DRAM
// )
void gemm_RVV_35x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(3));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(3));
}

// gemm_RVV_36x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 36] @DRAM
// )
void gemm_RVV_36x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
}

// gemm_RVV_36x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 36] @DRAM
// )
void gemm_RVV_36x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
}

// gemm_RVV_36x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 36] @DRAM
// )
void gemm_RVV_36x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
}

// gemm_RVV_36x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 36] @DRAM
// )
void gemm_RVV_36x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
}

// gemm_RVV_36x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 36] @DRAM
// )
void gemm_RVV_36x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
}

// gemm_RVV_36x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 36] @DRAM
// )
void gemm_RVV_36x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
}

// gemm_RVV_36x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 36] @DRAM
// )
void gemm_RVV_36x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
}

// gemm_RVV_36x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 36] @DRAM
// )
void gemm_RVV_36x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
}

// gemm_RVV_36x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 36] @DRAM
// )
void gemm_RVV_36x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(4));
}

// gemm_RVV_36x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 36] @DRAM
// )
void gemm_RVV_36x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(4));
}

// gemm_RVV_36x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 36] @DRAM
// )
void gemm_RVV_36x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(4));
}

// gemm_RVV_36x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 36] @DRAM
// )
void gemm_RVV_36x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(4));
}

// gemm_RVV_36x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 36] @DRAM
// )
void gemm_RVV_36x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(4));
}

// gemm_RVV_36x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 36] @DRAM
// )
void gemm_RVV_36x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(4));
}

// gemm_RVV_36x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 36] @DRAM
// )
void gemm_RVV_36x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(4));
}

// gemm_RVV_36x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 36] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 36] @DRAM
// )
void gemm_RVV_36x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(4));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(4));
}

// gemm_RVV_37x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 37] @DRAM
// )
void gemm_RVV_37x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
}

// gemm_RVV_37x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 37] @DRAM
// )
void gemm_RVV_37x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
}

// gemm_RVV_37x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 37] @DRAM
// )
void gemm_RVV_37x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
}

// gemm_RVV_37x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 37] @DRAM
// )
void gemm_RVV_37x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
}

// gemm_RVV_37x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 37] @DRAM
// )
void gemm_RVV_37x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
}

// gemm_RVV_37x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 37] @DRAM
// )
void gemm_RVV_37x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
}

// gemm_RVV_37x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 37] @DRAM
// )
void gemm_RVV_37x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
}

// gemm_RVV_37x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 37] @DRAM
// )
void gemm_RVV_37x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
}

// gemm_RVV_37x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 37] @DRAM
// )
void gemm_RVV_37x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(5));
}

// gemm_RVV_37x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 37] @DRAM
// )
void gemm_RVV_37x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(5));
}

// gemm_RVV_37x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 37] @DRAM
// )
void gemm_RVV_37x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(5));
}

// gemm_RVV_37x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 37] @DRAM
// )
void gemm_RVV_37x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(5));
}

// gemm_RVV_37x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 37] @DRAM
// )
void gemm_RVV_37x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(5));
}

// gemm_RVV_37x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 37] @DRAM
// )
void gemm_RVV_37x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(5));
}

// gemm_RVV_37x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 37] @DRAM
// )
void gemm_RVV_37x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(5));
}

// gemm_RVV_37x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 37] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 37] @DRAM
// )
void gemm_RVV_37x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(5));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(5));
}

// gemm_RVV_38x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 38] @DRAM
// )
void gemm_RVV_38x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
}

// gemm_RVV_38x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 38] @DRAM
// )
void gemm_RVV_38x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
}

// gemm_RVV_38x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 38] @DRAM
// )
void gemm_RVV_38x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
}

// gemm_RVV_38x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 38] @DRAM
// )
void gemm_RVV_38x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
}

// gemm_RVV_38x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 38] @DRAM
// )
void gemm_RVV_38x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
}

// gemm_RVV_38x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 38] @DRAM
// )
void gemm_RVV_38x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
}

// gemm_RVV_38x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 38] @DRAM
// )
void gemm_RVV_38x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
}

// gemm_RVV_38x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 38] @DRAM
// )
void gemm_RVV_38x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
}

// gemm_RVV_38x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 38] @DRAM
// )
void gemm_RVV_38x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(6));
}

// gemm_RVV_38x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 38] @DRAM
// )
void gemm_RVV_38x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(6));
}

// gemm_RVV_38x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 38] @DRAM
// )
void gemm_RVV_38x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(6));
}

// gemm_RVV_38x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 38] @DRAM
// )
void gemm_RVV_38x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(6));
}

// gemm_RVV_38x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 38] @DRAM
// )
void gemm_RVV_38x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(6));
}

// gemm_RVV_38x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 38] @DRAM
// )
void gemm_RVV_38x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(6));
}

// gemm_RVV_38x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 38] @DRAM
// )
void gemm_RVV_38x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(6));
}

// gemm_RVV_38x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 38] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 38] @DRAM
// )
void gemm_RVV_38x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(6));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(6));
}

// gemm_RVV_39x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 39] @DRAM
// )
void gemm_RVV_39x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
}

// gemm_RVV_39x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 39] @DRAM
// )
void gemm_RVV_39x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
}

// gemm_RVV_39x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 39] @DRAM
// )
void gemm_RVV_39x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
}

// gemm_RVV_39x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 39] @DRAM
// )
void gemm_RVV_39x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
}

// gemm_RVV_39x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 39] @DRAM
// )
void gemm_RVV_39x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
}

// gemm_RVV_39x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 39] @DRAM
// )
void gemm_RVV_39x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
}

// gemm_RVV_39x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 39] @DRAM
// )
void gemm_RVV_39x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
}

// gemm_RVV_39x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 39] @DRAM
// )
void gemm_RVV_39x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
}

// gemm_RVV_39x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 39] @DRAM
// )
void gemm_RVV_39x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(7));
}

// gemm_RVV_39x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 39] @DRAM
// )
void gemm_RVV_39x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(7));
}

// gemm_RVV_39x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 39] @DRAM
// )
void gemm_RVV_39x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(7));
}

// gemm_RVV_39x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 39] @DRAM
// )
void gemm_RVV_39x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(7));
}

// gemm_RVV_39x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 39] @DRAM
// )
void gemm_RVV_39x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(7));
}

// gemm_RVV_39x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 39] @DRAM
// )
void gemm_RVV_39x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(7));
}

// gemm_RVV_39x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 39] @DRAM
// )
void gemm_RVV_39x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(7));
}

// gemm_RVV_39x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 39] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 39] @DRAM
// )
void gemm_RVV_39x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[32],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(7));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_regt_7,(7));
}

// gemm_RVV_3x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 3] @DRAM
// )
void gemm_RVV_3x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
}

// gemm_RVV_3x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 3] @DRAM
// )
void gemm_RVV_3x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
}

// gemm_RVV_3x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 3] @DRAM
// )
void gemm_RVV_3x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
}

// gemm_RVV_3x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 3] @DRAM
// )
void gemm_RVV_3x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
}

// gemm_RVV_3x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 3] @DRAM
// )
void gemm_RVV_3x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
}

// gemm_RVV_3x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 3] @DRAM
// )
void gemm_RVV_3x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
}

// gemm_RVV_3x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 3] @DRAM
// )
void gemm_RVV_3x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
}

// gemm_RVV_3x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 3] @DRAM
// )
void gemm_RVV_3x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
}

// gemm_RVV_3x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 3] @DRAM
// )
void gemm_RVV_3x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
}

// gemm_RVV_3x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 3] @DRAM
// )
void gemm_RVV_3x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
}

// gemm_RVV_3x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 3] @DRAM
// )
void gemm_RVV_3x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
}

// gemm_RVV_3x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 3] @DRAM
// )
void gemm_RVV_3x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
}

// gemm_RVV_3x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 3] @DRAM
// )
void gemm_RVV_3x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
}

// gemm_RVV_3x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 3] @DRAM
// )
void gemm_RVV_3x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
}

// gemm_RVV_3x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 3] @DRAM
// )
void gemm_RVV_3x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
}

// gemm_RVV_3x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 3] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 3] @DRAM
// )
void gemm_RVV_3x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(3));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(3));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(3));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(3));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(3));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(3));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(3));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(3));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(3));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(3));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(3));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(3));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(3));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(3));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(3));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(3));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(3));
}

// gemm_RVV_40x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 40] @DRAM
// )
void gemm_RVV_40x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
}

// gemm_RVV_40x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 40] @DRAM
// )
void gemm_RVV_40x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
}

// gemm_RVV_40x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 40] @DRAM
// )
void gemm_RVV_40x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
}

// gemm_RVV_40x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 40] @DRAM
// )
void gemm_RVV_40x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
}

// gemm_RVV_40x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 40] @DRAM
// )
void gemm_RVV_40x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
}

// gemm_RVV_40x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 40] @DRAM
// )
void gemm_RVV_40x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
}

// gemm_RVV_40x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 40] @DRAM
// )
void gemm_RVV_40x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
}

// gemm_RVV_40x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 40] @DRAM
// )
void gemm_RVV_40x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
}

// gemm_RVV_40x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 40] @DRAM
// )
void gemm_RVV_40x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
}

// gemm_RVV_40x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 40] @DRAM
// )
void gemm_RVV_40x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
}

// gemm_RVV_40x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 40] @DRAM
// )
void gemm_RVV_40x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
}

// gemm_RVV_40x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 40] @DRAM
// )
void gemm_RVV_40x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
}

// gemm_RVV_40x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 40] @DRAM
// )
void gemm_RVV_40x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
}

// gemm_RVV_40x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 40] @DRAM
// )
void gemm_RVV_40x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
}

// gemm_RVV_40x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 40] @DRAM
// )
void gemm_RVV_40x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
}

// gemm_RVV_40x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 40] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 40] @DRAM
// )
void gemm_RVV_40x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_reg_7_4 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
}

// gemm_RVV_41x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 41] @DRAM
// )
void gemm_RVV_41x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
}

// gemm_RVV_41x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 41] @DRAM
// )
void gemm_RVV_41x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
}

// gemm_RVV_41x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 41] @DRAM
// )
void gemm_RVV_41x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
}

// gemm_RVV_41x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 41] @DRAM
// )
void gemm_RVV_41x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
}

// gemm_RVV_41x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 41] @DRAM
// )
void gemm_RVV_41x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
}

// gemm_RVV_41x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 41] @DRAM
// )
void gemm_RVV_41x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
}

// gemm_RVV_41x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 41] @DRAM
// )
void gemm_RVV_41x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
}

// gemm_RVV_41x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 41] @DRAM
// )
void gemm_RVV_41x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
}

// gemm_RVV_41x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 41] @DRAM
// )
void gemm_RVV_41x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(1));
}

// gemm_RVV_41x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 41] @DRAM
// )
void gemm_RVV_41x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(1));
}

// gemm_RVV_41x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 41] @DRAM
// )
void gemm_RVV_41x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(1));
}

// gemm_RVV_41x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 41] @DRAM
// )
void gemm_RVV_41x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(1));
}

// gemm_RVV_41x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 41] @DRAM
// )
void gemm_RVV_41x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(1));
}

// gemm_RVV_41x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 41] @DRAM
// )
void gemm_RVV_41x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(1));
}

// gemm_RVV_41x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 41] @DRAM
// )
void gemm_RVV_41x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(1));
}

// gemm_RVV_41x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 41] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 41] @DRAM
// )
void gemm_RVV_41x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_reg_7_4 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(1));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(1));
}

// gemm_RVV_42x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 42] @DRAM
// )
void gemm_RVV_42x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
}

// gemm_RVV_42x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 42] @DRAM
// )
void gemm_RVV_42x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
}

// gemm_RVV_42x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 42] @DRAM
// )
void gemm_RVV_42x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
}

// gemm_RVV_42x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 42] @DRAM
// )
void gemm_RVV_42x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
}

// gemm_RVV_42x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 42] @DRAM
// )
void gemm_RVV_42x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
}

// gemm_RVV_42x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 42] @DRAM
// )
void gemm_RVV_42x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
}

// gemm_RVV_42x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 42] @DRAM
// )
void gemm_RVV_42x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
}

// gemm_RVV_42x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 42] @DRAM
// )
void gemm_RVV_42x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
}

// gemm_RVV_42x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 42] @DRAM
// )
void gemm_RVV_42x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(2));
}

// gemm_RVV_42x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 42] @DRAM
// )
void gemm_RVV_42x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(2));
}

// gemm_RVV_42x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 42] @DRAM
// )
void gemm_RVV_42x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(2));
}

// gemm_RVV_42x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 42] @DRAM
// )
void gemm_RVV_42x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(2));
}

// gemm_RVV_42x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 42] @DRAM
// )
void gemm_RVV_42x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(2));
}

// gemm_RVV_42x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 42] @DRAM
// )
void gemm_RVV_42x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(2));
}

// gemm_RVV_42x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 42] @DRAM
// )
void gemm_RVV_42x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(2));
}

// gemm_RVV_42x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 42] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 42] @DRAM
// )
void gemm_RVV_42x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_reg_7_4 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(2));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(2));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(2));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(2));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(2));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(2));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(2));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40],(2));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(2));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(2));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(2));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(2));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(2));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(2));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(2));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(2));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(2));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(2));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(2));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(2));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(2));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(2));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(2));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(2));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(2));
}

// gemm_RVV_43x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 43] @DRAM
// )
void gemm_RVV_43x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
}

// gemm_RVV_43x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 43] @DRAM
// )
void gemm_RVV_43x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
}

// gemm_RVV_43x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 43] @DRAM
// )
void gemm_RVV_43x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
}

// gemm_RVV_43x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 43] @DRAM
// )
void gemm_RVV_43x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
}

// gemm_RVV_43x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 43] @DRAM
// )
void gemm_RVV_43x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
}

// gemm_RVV_43x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 43] @DRAM
// )
void gemm_RVV_43x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
}

// gemm_RVV_43x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 43] @DRAM
// )
void gemm_RVV_43x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
}

// gemm_RVV_43x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 43] @DRAM
// )
void gemm_RVV_43x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
}

// gemm_RVV_43x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 43] @DRAM
// )
void gemm_RVV_43x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(3));
}

// gemm_RVV_43x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 43] @DRAM
// )
void gemm_RVV_43x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(3));
}

// gemm_RVV_43x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 43] @DRAM
// )
void gemm_RVV_43x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(3));
}

// gemm_RVV_43x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 43] @DRAM
// )
void gemm_RVV_43x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(3));
}

// gemm_RVV_43x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 43] @DRAM
// )
void gemm_RVV_43x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(3));
}

// gemm_RVV_43x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 43] @DRAM
// )
void gemm_RVV_43x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(3));
}

// gemm_RVV_43x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 43] @DRAM
// )
void gemm_RVV_43x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(3));
}

// gemm_RVV_43x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 43] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 43] @DRAM
// )
void gemm_RVV_43x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_reg_7_4 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(3));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(3));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(3));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(3));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(3));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(3));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(3));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40],(3));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(3));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(3));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(3));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(3));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(3));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(3));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(3));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(3));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(3));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(3));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(3));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(3));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(3));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(3));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(3));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(3));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(3));
}

// gemm_RVV_44x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 44] @DRAM
// )
void gemm_RVV_44x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
}

// gemm_RVV_44x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 44] @DRAM
// )
void gemm_RVV_44x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
}

// gemm_RVV_44x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 44] @DRAM
// )
void gemm_RVV_44x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
}

// gemm_RVV_44x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 44] @DRAM
// )
void gemm_RVV_44x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
}

// gemm_RVV_44x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 44] @DRAM
// )
void gemm_RVV_44x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
}

// gemm_RVV_44x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 44] @DRAM
// )
void gemm_RVV_44x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
}

// gemm_RVV_44x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 44] @DRAM
// )
void gemm_RVV_44x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
}

// gemm_RVV_44x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 44] @DRAM
// )
void gemm_RVV_44x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
}

// gemm_RVV_44x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 44] @DRAM
// )
void gemm_RVV_44x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(4));
}

// gemm_RVV_44x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 44] @DRAM
// )
void gemm_RVV_44x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(4));
}

// gemm_RVV_44x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 44] @DRAM
// )
void gemm_RVV_44x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(4));
}

// gemm_RVV_44x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 44] @DRAM
// )
void gemm_RVV_44x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(4));
}

// gemm_RVV_44x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 44] @DRAM
// )
void gemm_RVV_44x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(4));
}

// gemm_RVV_44x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 44] @DRAM
// )
void gemm_RVV_44x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(4));
}

// gemm_RVV_44x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 44] @DRAM
// )
void gemm_RVV_44x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(4));
}

// gemm_RVV_44x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 44] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 44] @DRAM
// )
void gemm_RVV_44x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_reg_7_4 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(4));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(4));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(4));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(4));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(4));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(4));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(4));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40],(4));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(4));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(4));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(4));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(4));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(4));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(4));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(4));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(4));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(4));
}

// gemm_RVV_45x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 45] @DRAM
// )
void gemm_RVV_45x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
}

// gemm_RVV_45x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 45] @DRAM
// )
void gemm_RVV_45x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
}

// gemm_RVV_45x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 45] @DRAM
// )
void gemm_RVV_45x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
}

// gemm_RVV_45x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 45] @DRAM
// )
void gemm_RVV_45x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
}

// gemm_RVV_45x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 45] @DRAM
// )
void gemm_RVV_45x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
}

// gemm_RVV_45x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 45] @DRAM
// )
void gemm_RVV_45x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
}

// gemm_RVV_45x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 45] @DRAM
// )
void gemm_RVV_45x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
}

// gemm_RVV_45x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 45] @DRAM
// )
void gemm_RVV_45x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
}

// gemm_RVV_45x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 45] @DRAM
// )
void gemm_RVV_45x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(5));
}

// gemm_RVV_45x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 45] @DRAM
// )
void gemm_RVV_45x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(5));
}

// gemm_RVV_45x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 45] @DRAM
// )
void gemm_RVV_45x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(5));
}

// gemm_RVV_45x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 45] @DRAM
// )
void gemm_RVV_45x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(5));
}

// gemm_RVV_45x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 45] @DRAM
// )
void gemm_RVV_45x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(5));
}

// gemm_RVV_45x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 45] @DRAM
// )
void gemm_RVV_45x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(5));
}

// gemm_RVV_45x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 45] @DRAM
// )
void gemm_RVV_45x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(5));
}

// gemm_RVV_45x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 45] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 45] @DRAM
// )
void gemm_RVV_45x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_reg_7_4 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(5));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(5));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(5));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(5));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(5));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(5));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(5));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40],(5));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(5));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(5));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(5));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(5));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(5));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(5));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(5));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(5));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(5));
}

// gemm_RVV_46x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 46] @DRAM
// )
void gemm_RVV_46x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
}

// gemm_RVV_46x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 46] @DRAM
// )
void gemm_RVV_46x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
}

// gemm_RVV_46x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 46] @DRAM
// )
void gemm_RVV_46x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
}

// gemm_RVV_46x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 46] @DRAM
// )
void gemm_RVV_46x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
}

// gemm_RVV_46x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 46] @DRAM
// )
void gemm_RVV_46x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
}

// gemm_RVV_46x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 46] @DRAM
// )
void gemm_RVV_46x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
}

// gemm_RVV_46x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 46] @DRAM
// )
void gemm_RVV_46x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
}

// gemm_RVV_46x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 46] @DRAM
// )
void gemm_RVV_46x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
}

// gemm_RVV_46x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 46] @DRAM
// )
void gemm_RVV_46x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(6));
}

// gemm_RVV_46x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 46] @DRAM
// )
void gemm_RVV_46x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(6));
}

// gemm_RVV_46x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 46] @DRAM
// )
void gemm_RVV_46x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(6));
}

// gemm_RVV_46x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 46] @DRAM
// )
void gemm_RVV_46x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(6));
}

// gemm_RVV_46x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 46] @DRAM
// )
void gemm_RVV_46x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(6));
}

// gemm_RVV_46x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 46] @DRAM
// )
void gemm_RVV_46x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(6));
}

// gemm_RVV_46x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 46] @DRAM
// )
void gemm_RVV_46x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(6));
}

// gemm_RVV_46x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 46] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 46] @DRAM
// )
void gemm_RVV_46x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_reg_7_4 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(6));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(6));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(6));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(6));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(6));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(6));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(6));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40],(6));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(6));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(6));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(6));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(6));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(6));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(6));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(6));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(6));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(6));
}

// gemm_RVV_47x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 47] @DRAM
// )
void gemm_RVV_47x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
}

// gemm_RVV_47x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 47] @DRAM
// )
void gemm_RVV_47x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
}

// gemm_RVV_47x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 47] @DRAM
// )
void gemm_RVV_47x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
}

// gemm_RVV_47x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 47] @DRAM
// )
void gemm_RVV_47x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
}

// gemm_RVV_47x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 47] @DRAM
// )
void gemm_RVV_47x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
}

// gemm_RVV_47x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 47] @DRAM
// )
void gemm_RVV_47x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
}

// gemm_RVV_47x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 47] @DRAM
// )
void gemm_RVV_47x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
}

// gemm_RVV_47x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 47] @DRAM
// )
void gemm_RVV_47x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
}

// gemm_RVV_47x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 47] @DRAM
// )
void gemm_RVV_47x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(7));
}

// gemm_RVV_47x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 47] @DRAM
// )
void gemm_RVV_47x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(7));
}

// gemm_RVV_47x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 47] @DRAM
// )
void gemm_RVV_47x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(7));
}

// gemm_RVV_47x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 47] @DRAM
// )
void gemm_RVV_47x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(7));
}

// gemm_RVV_47x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 47] @DRAM
// )
void gemm_RVV_47x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(7));
}

// gemm_RVV_47x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 47] @DRAM
// )
void gemm_RVV_47x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(7));
}

// gemm_RVV_47x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 47] @DRAM
// )
void gemm_RVV_47x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(7));
}

// gemm_RVV_47x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 47] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 47] @DRAM
// )
void gemm_RVV_47x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_reg_7_4 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[40],(7));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(7));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(7));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(7));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(7));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(7));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(7));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40],(7));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(7));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(7));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(7));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(7));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(7));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(7));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(7));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(7));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_regt_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_regt_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_regt_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_regt_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_regt_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_regt_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_regt_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_regt_7,(7));
}

// gemm_RVV_48x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 48] @DRAM
// )
void gemm_RVV_48x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
}

// gemm_RVV_48x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 48] @DRAM
// )
void gemm_RVV_48x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_0_5 = __riscv_vle32_v_f32m1(&C.data[40],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
}

// gemm_RVV_48x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 48] @DRAM
// )
void gemm_RVV_48x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
}

// gemm_RVV_48x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 48] @DRAM
// )
void gemm_RVV_48x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_0_5 = __riscv_vle32_v_f32m1(&C.data[40],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_1_5 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
}

// gemm_RVV_48x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 48] @DRAM
// )
void gemm_RVV_48x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
}

// gemm_RVV_48x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 48] @DRAM
// )
void gemm_RVV_48x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_0_5 = __riscv_vle32_v_f32m1(&C.data[40],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_1_5 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_2_5 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
}

// gemm_RVV_48x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 48] @DRAM
// )
void gemm_RVV_48x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
}

// gemm_RVV_48x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 48] @DRAM
// )
void gemm_RVV_48x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_0_5 = __riscv_vle32_v_f32m1(&C.data[40],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_1_5 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_2_5 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_3_5 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
}

// gemm_RVV_48x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 48] @DRAM
// )
void gemm_RVV_48x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_4_5;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_4_5 = __riscv_vfmacc_vf_f32m1(C_reg_4_5, B.data[(k) * (B.strides[0]) + 4], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_reg_4_5,(8));
}

// gemm_RVV_48x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 48] @DRAM
// )
void gemm_RVV_48x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_4_5;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_0_5 = __riscv_vle32_v_f32m1(&C.data[40],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_1_5 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_2_5 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_3_5 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_4_5 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_4_5 = __riscv_vfmacc_vf_f32m1(C_reg_4_5, B.data[(k) * (B.strides[0]) + 4], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_reg_4_5,(8));
}

// gemm_RVV_48x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 48] @DRAM
// )
void gemm_RVV_48x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_4_5;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_5_5;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_4_5 = __riscv_vfmacc_vf_f32m1(C_reg_4_5, B.data[(k) * (B.strides[0]) + 4], A_reg_5,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_5_5 = __riscv_vfmacc_vf_f32m1(C_reg_5_5, B.data[(k) * (B.strides[0]) + 5], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_reg_4_5,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_reg_5_5,(8));
}

// gemm_RVV_48x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 48] @DRAM
// )
void gemm_RVV_48x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_4_5;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_5_5;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_0_5 = __riscv_vle32_v_f32m1(&C.data[40],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_1_5 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_2_5 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_3_5 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_4_5 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_5_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_4_5 = __riscv_vfmacc_vf_f32m1(C_reg_4_5, B.data[(k) * (B.strides[0]) + 4], A_reg_5,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_5_5 = __riscv_vfmacc_vf_f32m1(C_reg_5_5, B.data[(k) * (B.strides[0]) + 5], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_reg_4_5,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_reg_5_5,(8));
}

// gemm_RVV_48x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 48] @DRAM
// )
void gemm_RVV_48x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_4_5;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_5_5;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_6_5;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_4_5 = __riscv_vfmacc_vf_f32m1(C_reg_4_5, B.data[(k) * (B.strides[0]) + 4], A_reg_5,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_5_5 = __riscv_vfmacc_vf_f32m1(C_reg_5_5, B.data[(k) * (B.strides[0]) + 5], A_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_6_5 = __riscv_vfmacc_vf_f32m1(C_reg_6_5, B.data[(k) * (B.strides[0]) + 6], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_reg_4_5,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_reg_5_5,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_reg_6_5,(8));
}

// gemm_RVV_48x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 48] @DRAM
// )
void gemm_RVV_48x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_4_5;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_5_5;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_6_5;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_0_5 = __riscv_vle32_v_f32m1(&C.data[40],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_1_5 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_2_5 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_3_5 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_4_5 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_5_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_6_5 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_4_5 = __riscv_vfmacc_vf_f32m1(C_reg_4_5, B.data[(k) * (B.strides[0]) + 4], A_reg_5,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_5_5 = __riscv_vfmacc_vf_f32m1(C_reg_5_5, B.data[(k) * (B.strides[0]) + 5], A_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_6_5 = __riscv_vfmacc_vf_f32m1(C_reg_6_5, B.data[(k) * (B.strides[0]) + 6], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_reg_4_5,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_reg_5_5,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_reg_6_5,(8));
}

// gemm_RVV_48x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 48] @DRAM
// )
void gemm_RVV_48x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_4_5;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_5_5;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_6_5;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
vfloat32m1_t C_reg_7_5;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_0_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_4_5 = __riscv_vfmacc_vf_f32m1(C_reg_4_5, B.data[(k) * (B.strides[0]) + 4], A_reg_5,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_5_5 = __riscv_vfmacc_vf_f32m1(C_reg_5_5, B.data[(k) * (B.strides[0]) + 5], A_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_6_5 = __riscv_vfmacc_vf_f32m1(C_reg_6_5, B.data[(k) * (B.strides[0]) + 6], A_reg_5,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_reg_7_5 = __riscv_vfmacc_vf_f32m1(C_reg_7_5, B.data[(k) * (B.strides[0]) + 7], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_reg_4_5,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_reg_5_5,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_reg_6_5,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_reg_7_5,(8));
}

// gemm_RVV_48x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 48] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 48] @DRAM
// )
void gemm_RVV_48x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_reg_1;
vfloat32m1_t A_reg_2;
vfloat32m1_t A_reg_3;
vfloat32m1_t A_reg_4;
vfloat32m1_t A_reg_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_0_1;
vfloat32m1_t C_reg_0_2;
vfloat32m1_t C_reg_0_3;
vfloat32m1_t C_reg_0_4;
vfloat32m1_t C_reg_0_5;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_1_1;
vfloat32m1_t C_reg_1_2;
vfloat32m1_t C_reg_1_3;
vfloat32m1_t C_reg_1_4;
vfloat32m1_t C_reg_1_5;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_2_1;
vfloat32m1_t C_reg_2_2;
vfloat32m1_t C_reg_2_3;
vfloat32m1_t C_reg_2_4;
vfloat32m1_t C_reg_2_5;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_3_1;
vfloat32m1_t C_reg_3_2;
vfloat32m1_t C_reg_3_3;
vfloat32m1_t C_reg_3_4;
vfloat32m1_t C_reg_3_5;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_4_1;
vfloat32m1_t C_reg_4_2;
vfloat32m1_t C_reg_4_3;
vfloat32m1_t C_reg_4_4;
vfloat32m1_t C_reg_4_5;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_5_1;
vfloat32m1_t C_reg_5_2;
vfloat32m1_t C_reg_5_3;
vfloat32m1_t C_reg_5_4;
vfloat32m1_t C_reg_5_5;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_6_1;
vfloat32m1_t C_reg_6_2;
vfloat32m1_t C_reg_6_3;
vfloat32m1_t C_reg_6_4;
vfloat32m1_t C_reg_6_5;
vfloat32m1_t C_reg_7_0;
vfloat32m1_t C_reg_7_1;
vfloat32m1_t C_reg_7_2;
vfloat32m1_t C_reg_7_3;
vfloat32m1_t C_reg_7_4;
vfloat32m1_t C_reg_7_5;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_0_1 = __riscv_vle32_v_f32m1(&C.data[8],(8));
C_reg_0_2 = __riscv_vle32_v_f32m1(&C.data[16],(8));
C_reg_0_3 = __riscv_vle32_v_f32m1(&C.data[24],(8));
C_reg_0_4 = __riscv_vle32_v_f32m1(&C.data[32],(8));
C_reg_0_5 = __riscv_vle32_v_f32m1(&C.data[40],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_1_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(8));
C_reg_1_2 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 16],(8));
C_reg_1_3 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 24],(8));
C_reg_1_4 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 32],(8));
C_reg_1_5 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 40],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_2_1 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(8));
C_reg_2_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16],(8));
C_reg_2_3 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24],(8));
C_reg_2_4 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32],(8));
C_reg_2_5 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_3_1 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(8));
C_reg_3_2 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16],(8));
C_reg_3_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24],(8));
C_reg_3_4 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32],(8));
C_reg_3_5 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_4_1 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(8));
C_reg_4_2 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16],(8));
C_reg_4_3 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24],(8));
C_reg_4_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32],(8));
C_reg_4_5 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_5_1 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(8));
C_reg_5_2 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16],(8));
C_reg_5_3 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24],(8));
C_reg_5_4 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32],(8));
C_reg_5_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_6_1 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(8));
C_reg_6_2 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16],(8));
C_reg_6_3 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24],(8));
C_reg_6_4 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32],(8));
C_reg_6_5 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_reg_7_1 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(8));
C_reg_7_2 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16],(8));
C_reg_7_3 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24],(8));
C_reg_7_4 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32],(8));
C_reg_7_5 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40],(8));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_reg_1 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(8));
  A_reg_2 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 16],(8));
  A_reg_3 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 24],(8));
  A_reg_4 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 32],(8));
  A_reg_5 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 40],(8));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_0_1 = __riscv_vfmacc_vf_f32m1(C_reg_0_1, B.data[(k) * (B.strides[0])], A_reg_1,(8));
  C_reg_0_2 = __riscv_vfmacc_vf_f32m1(C_reg_0_2, B.data[(k) * (B.strides[0])], A_reg_2,(8));
  C_reg_0_3 = __riscv_vfmacc_vf_f32m1(C_reg_0_3, B.data[(k) * (B.strides[0])], A_reg_3,(8));
  C_reg_0_4 = __riscv_vfmacc_vf_f32m1(C_reg_0_4, B.data[(k) * (B.strides[0])], A_reg_4,(8));
  C_reg_0_5 = __riscv_vfmacc_vf_f32m1(C_reg_0_5, B.data[(k) * (B.strides[0])], A_reg_5,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_1_1 = __riscv_vfmacc_vf_f32m1(C_reg_1_1, B.data[(k) * (B.strides[0]) + 1], A_reg_1,(8));
  C_reg_1_2 = __riscv_vfmacc_vf_f32m1(C_reg_1_2, B.data[(k) * (B.strides[0]) + 1], A_reg_2,(8));
  C_reg_1_3 = __riscv_vfmacc_vf_f32m1(C_reg_1_3, B.data[(k) * (B.strides[0]) + 1], A_reg_3,(8));
  C_reg_1_4 = __riscv_vfmacc_vf_f32m1(C_reg_1_4, B.data[(k) * (B.strides[0]) + 1], A_reg_4,(8));
  C_reg_1_5 = __riscv_vfmacc_vf_f32m1(C_reg_1_5, B.data[(k) * (B.strides[0]) + 1], A_reg_5,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_2_1 = __riscv_vfmacc_vf_f32m1(C_reg_2_1, B.data[(k) * (B.strides[0]) + 2], A_reg_1,(8));
  C_reg_2_2 = __riscv_vfmacc_vf_f32m1(C_reg_2_2, B.data[(k) * (B.strides[0]) + 2], A_reg_2,(8));
  C_reg_2_3 = __riscv_vfmacc_vf_f32m1(C_reg_2_3, B.data[(k) * (B.strides[0]) + 2], A_reg_3,(8));
  C_reg_2_4 = __riscv_vfmacc_vf_f32m1(C_reg_2_4, B.data[(k) * (B.strides[0]) + 2], A_reg_4,(8));
  C_reg_2_5 = __riscv_vfmacc_vf_f32m1(C_reg_2_5, B.data[(k) * (B.strides[0]) + 2], A_reg_5,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_3_1 = __riscv_vfmacc_vf_f32m1(C_reg_3_1, B.data[(k) * (B.strides[0]) + 3], A_reg_1,(8));
  C_reg_3_2 = __riscv_vfmacc_vf_f32m1(C_reg_3_2, B.data[(k) * (B.strides[0]) + 3], A_reg_2,(8));
  C_reg_3_3 = __riscv_vfmacc_vf_f32m1(C_reg_3_3, B.data[(k) * (B.strides[0]) + 3], A_reg_3,(8));
  C_reg_3_4 = __riscv_vfmacc_vf_f32m1(C_reg_3_4, B.data[(k) * (B.strides[0]) + 3], A_reg_4,(8));
  C_reg_3_5 = __riscv_vfmacc_vf_f32m1(C_reg_3_5, B.data[(k) * (B.strides[0]) + 3], A_reg_5,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_4_1 = __riscv_vfmacc_vf_f32m1(C_reg_4_1, B.data[(k) * (B.strides[0]) + 4], A_reg_1,(8));
  C_reg_4_2 = __riscv_vfmacc_vf_f32m1(C_reg_4_2, B.data[(k) * (B.strides[0]) + 4], A_reg_2,(8));
  C_reg_4_3 = __riscv_vfmacc_vf_f32m1(C_reg_4_3, B.data[(k) * (B.strides[0]) + 4], A_reg_3,(8));
  C_reg_4_4 = __riscv_vfmacc_vf_f32m1(C_reg_4_4, B.data[(k) * (B.strides[0]) + 4], A_reg_4,(8));
  C_reg_4_5 = __riscv_vfmacc_vf_f32m1(C_reg_4_5, B.data[(k) * (B.strides[0]) + 4], A_reg_5,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_5_1 = __riscv_vfmacc_vf_f32m1(C_reg_5_1, B.data[(k) * (B.strides[0]) + 5], A_reg_1,(8));
  C_reg_5_2 = __riscv_vfmacc_vf_f32m1(C_reg_5_2, B.data[(k) * (B.strides[0]) + 5], A_reg_2,(8));
  C_reg_5_3 = __riscv_vfmacc_vf_f32m1(C_reg_5_3, B.data[(k) * (B.strides[0]) + 5], A_reg_3,(8));
  C_reg_5_4 = __riscv_vfmacc_vf_f32m1(C_reg_5_4, B.data[(k) * (B.strides[0]) + 5], A_reg_4,(8));
  C_reg_5_5 = __riscv_vfmacc_vf_f32m1(C_reg_5_5, B.data[(k) * (B.strides[0]) + 5], A_reg_5,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_6_1 = __riscv_vfmacc_vf_f32m1(C_reg_6_1, B.data[(k) * (B.strides[0]) + 6], A_reg_1,(8));
  C_reg_6_2 = __riscv_vfmacc_vf_f32m1(C_reg_6_2, B.data[(k) * (B.strides[0]) + 6], A_reg_2,(8));
  C_reg_6_3 = __riscv_vfmacc_vf_f32m1(C_reg_6_3, B.data[(k) * (B.strides[0]) + 6], A_reg_3,(8));
  C_reg_6_4 = __riscv_vfmacc_vf_f32m1(C_reg_6_4, B.data[(k) * (B.strides[0]) + 6], A_reg_4,(8));
  C_reg_6_5 = __riscv_vfmacc_vf_f32m1(C_reg_6_5, B.data[(k) * (B.strides[0]) + 6], A_reg_5,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_reg_7_1 = __riscv_vfmacc_vf_f32m1(C_reg_7_1, B.data[(k) * (B.strides[0]) + 7], A_reg_1,(8));
  C_reg_7_2 = __riscv_vfmacc_vf_f32m1(C_reg_7_2, B.data[(k) * (B.strides[0]) + 7], A_reg_2,(8));
  C_reg_7_3 = __riscv_vfmacc_vf_f32m1(C_reg_7_3, B.data[(k) * (B.strides[0]) + 7], A_reg_3,(8));
  C_reg_7_4 = __riscv_vfmacc_vf_f32m1(C_reg_7_4, B.data[(k) * (B.strides[0]) + 7], A_reg_4,(8));
  C_reg_7_5 = __riscv_vfmacc_vf_f32m1(C_reg_7_5, B.data[(k) * (B.strides[0]) + 7], A_reg_5,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_reg_0_1,(8));
__riscv_vse32_v_f32m1(&C.data[16], C_reg_0_2,(8));
__riscv_vse32_v_f32m1(&C.data[24], C_reg_0_3,(8));
__riscv_vse32_v_f32m1(&C.data[32], C_reg_0_4,(8));
__riscv_vse32_v_f32m1(&C.data[40], C_reg_0_5,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_reg_1_1,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 16], C_reg_1_2,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 24], C_reg_1_3,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 32], C_reg_1_4,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 40], C_reg_1_5,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_reg_2_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 16], C_reg_2_2,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 24], C_reg_2_3,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 32], C_reg_2_4,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 40], C_reg_2_5,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_reg_3_1,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 16], C_reg_3_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 24], C_reg_3_3,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 32], C_reg_3_4,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 40], C_reg_3_5,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_reg_4_1,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 16], C_reg_4_2,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 24], C_reg_4_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 32], C_reg_4_4,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 40], C_reg_4_5,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_reg_5_1,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 16], C_reg_5_2,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 24], C_reg_5_3,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 32], C_reg_5_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 40], C_reg_5_5,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_reg_6_1,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 16], C_reg_6_2,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 24], C_reg_6_3,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 32], C_reg_6_4,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 40], C_reg_6_5,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_reg_7_1,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 16], C_reg_7_2,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 24], C_reg_7_3,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 32], C_reg_7_4,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 40], C_reg_7_5,(8));
}

// gemm_RVV_4x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 4] @DRAM
// )
void gemm_RVV_4x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
}

// gemm_RVV_4x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 4] @DRAM
// )
void gemm_RVV_4x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
}

// gemm_RVV_4x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 4] @DRAM
// )
void gemm_RVV_4x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
}

// gemm_RVV_4x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 4] @DRAM
// )
void gemm_RVV_4x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
}

// gemm_RVV_4x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 4] @DRAM
// )
void gemm_RVV_4x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
}

// gemm_RVV_4x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 4] @DRAM
// )
void gemm_RVV_4x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
}

// gemm_RVV_4x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 4] @DRAM
// )
void gemm_RVV_4x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
}

// gemm_RVV_4x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 4] @DRAM
// )
void gemm_RVV_4x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
}

// gemm_RVV_4x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 4] @DRAM
// )
void gemm_RVV_4x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
}

// gemm_RVV_4x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 4] @DRAM
// )
void gemm_RVV_4x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
}

// gemm_RVV_4x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 4] @DRAM
// )
void gemm_RVV_4x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
}

// gemm_RVV_4x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 4] @DRAM
// )
void gemm_RVV_4x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
}

// gemm_RVV_4x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 4] @DRAM
// )
void gemm_RVV_4x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
}

// gemm_RVV_4x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 4] @DRAM
// )
void gemm_RVV_4x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
}

// gemm_RVV_4x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 4] @DRAM
// )
void gemm_RVV_4x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
}

// gemm_RVV_4x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 4] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 4] @DRAM
// )
void gemm_RVV_4x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(4));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(4));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(4));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(4));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(4));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(4));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(4));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(4));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(4));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(4));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(4));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(4));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(4));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(4));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(4));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(4));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(4));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(4));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(4));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(4));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(4));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(4));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(4));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(4));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(4));
}

// gemm_RVV_5x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 5] @DRAM
// )
void gemm_RVV_5x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
}

// gemm_RVV_5x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 5] @DRAM
// )
void gemm_RVV_5x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
}

// gemm_RVV_5x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 5] @DRAM
// )
void gemm_RVV_5x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
}

// gemm_RVV_5x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 5] @DRAM
// )
void gemm_RVV_5x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(5));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
}

// gemm_RVV_5x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 5] @DRAM
// )
void gemm_RVV_5x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
}

// gemm_RVV_5x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 5] @DRAM
// )
void gemm_RVV_5x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(5));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(5));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
}

// gemm_RVV_5x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 5] @DRAM
// )
void gemm_RVV_5x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
}

// gemm_RVV_5x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 5] @DRAM
// )
void gemm_RVV_5x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(5));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(5));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(5));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
}

// gemm_RVV_5x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 5] @DRAM
// )
void gemm_RVV_5x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(5));
}

// gemm_RVV_5x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 5] @DRAM
// )
void gemm_RVV_5x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(5));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(5));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(5));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(5));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(5));
}

// gemm_RVV_5x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 5] @DRAM
// )
void gemm_RVV_5x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(5));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(5));
}

// gemm_RVV_5x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 5] @DRAM
// )
void gemm_RVV_5x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(5));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(5));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(5));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(5));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(5));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(5));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(5));
}

// gemm_RVV_5x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 5] @DRAM
// )
void gemm_RVV_5x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(5));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(5));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(5));
}

// gemm_RVV_5x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 5] @DRAM
// )
void gemm_RVV_5x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(5));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(5));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(5));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(5));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(5));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(5));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(5));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(5));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(5));
}

// gemm_RVV_5x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 5] @DRAM
// )
void gemm_RVV_5x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(5));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(5));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(5));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(5));
}

// gemm_RVV_5x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 5] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 5] @DRAM
// )
void gemm_RVV_5x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(5));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(5));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(5));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(5));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(5));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(5));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(5));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(5));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(5));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(5));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(5));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(5));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(5));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(5));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(5));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(5));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(5));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(5));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(5));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(5));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(5));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(5));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(5));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(5));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(5));
}

// gemm_RVV_6x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 6] @DRAM
// )
void gemm_RVV_6x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
}

// gemm_RVV_6x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 6] @DRAM
// )
void gemm_RVV_6x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
}

// gemm_RVV_6x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 6] @DRAM
// )
void gemm_RVV_6x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
}

// gemm_RVV_6x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 6] @DRAM
// )
void gemm_RVV_6x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(6));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
}

// gemm_RVV_6x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 6] @DRAM
// )
void gemm_RVV_6x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
}

// gemm_RVV_6x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 6] @DRAM
// )
void gemm_RVV_6x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(6));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(6));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
}

// gemm_RVV_6x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 6] @DRAM
// )
void gemm_RVV_6x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
}

// gemm_RVV_6x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 6] @DRAM
// )
void gemm_RVV_6x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(6));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(6));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(6));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
}

// gemm_RVV_6x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 6] @DRAM
// )
void gemm_RVV_6x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(6));
}

// gemm_RVV_6x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 6] @DRAM
// )
void gemm_RVV_6x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(6));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(6));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(6));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(6));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(6));
}

// gemm_RVV_6x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 6] @DRAM
// )
void gemm_RVV_6x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(6));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(6));
}

// gemm_RVV_6x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 6] @DRAM
// )
void gemm_RVV_6x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(6));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(6));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(6));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(6));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(6));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(6));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(6));
}

// gemm_RVV_6x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 6] @DRAM
// )
void gemm_RVV_6x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(6));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(6));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(6));
}

// gemm_RVV_6x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 6] @DRAM
// )
void gemm_RVV_6x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(6));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(6));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(6));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(6));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(6));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(6));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(6));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(6));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(6));
}

// gemm_RVV_6x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 6] @DRAM
// )
void gemm_RVV_6x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(6));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(6));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(6));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(6));
}

// gemm_RVV_6x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 6] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 6] @DRAM
// )
void gemm_RVV_6x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(6));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(6));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(6));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(6));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(6));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(6));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(6));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(6));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(6));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(6));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(6));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(6));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(6));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(6));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(6));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(6));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(6));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(6));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(6));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(6));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(6));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(6));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(6));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(6));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(6));
}

// gemm_RVV_7x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 7] @DRAM
// )
void gemm_RVV_7x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
}

// gemm_RVV_7x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 7] @DRAM
// )
void gemm_RVV_7x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
}

// gemm_RVV_7x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 7] @DRAM
// )
void gemm_RVV_7x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
}

// gemm_RVV_7x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 7] @DRAM
// )
void gemm_RVV_7x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(7));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
}

// gemm_RVV_7x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 7] @DRAM
// )
void gemm_RVV_7x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
}

// gemm_RVV_7x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 7] @DRAM
// )
void gemm_RVV_7x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(7));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(7));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
}

// gemm_RVV_7x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 7] @DRAM
// )
void gemm_RVV_7x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
}

// gemm_RVV_7x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 7] @DRAM
// )
void gemm_RVV_7x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(7));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(7));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(7));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
}

// gemm_RVV_7x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 7] @DRAM
// )
void gemm_RVV_7x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(7));
}

// gemm_RVV_7x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 7] @DRAM
// )
void gemm_RVV_7x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(7));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(7));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(7));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(7));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(7));
}

// gemm_RVV_7x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 7] @DRAM
// )
void gemm_RVV_7x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(7));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(7));
}

// gemm_RVV_7x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 7] @DRAM
// )
void gemm_RVV_7x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(7));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(7));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(7));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(7));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(7));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(7));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(7));
}

// gemm_RVV_7x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 7] @DRAM
// )
void gemm_RVV_7x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(7));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(7));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(7));
}

// gemm_RVV_7x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 7] @DRAM
// )
void gemm_RVV_7x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(7));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(7));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(7));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(7));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(7));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(7));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(7));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(7));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(7));
}

// gemm_RVV_7x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 7] @DRAM
// )
void gemm_RVV_7x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(7));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(7));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(7));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(7));
}

// gemm_RVV_7x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 7] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 7] @DRAM
// )
void gemm_RVV_7x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(7));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(7));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(7));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(7));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(7));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(7));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(7));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(7));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(7));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(7));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(7));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(7));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(7));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(7));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(7));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(7));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(7));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(7));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(7));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(7));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(7));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(7));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(7));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(7));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(7));
}

// gemm_RVV_8x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 8] @DRAM
// )
void gemm_RVV_8x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
}

// gemm_RVV_8x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 8] @DRAM
// )
void gemm_RVV_8x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
}

// gemm_RVV_8x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 8] @DRAM
// )
void gemm_RVV_8x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
}

// gemm_RVV_8x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 8] @DRAM
// )
void gemm_RVV_8x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
}

// gemm_RVV_8x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 8] @DRAM
// )
void gemm_RVV_8x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
}

// gemm_RVV_8x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 8] @DRAM
// )
void gemm_RVV_8x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
}

// gemm_RVV_8x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 8] @DRAM
// )
void gemm_RVV_8x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
}

// gemm_RVV_8x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 8] @DRAM
// )
void gemm_RVV_8x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
}

// gemm_RVV_8x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 8] @DRAM
// )
void gemm_RVV_8x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(8));
}

// gemm_RVV_8x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 8] @DRAM
// )
void gemm_RVV_8x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(8));
}

// gemm_RVV_8x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 8] @DRAM
// )
void gemm_RVV_8x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(8));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(8));
}

// gemm_RVV_8x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 8] @DRAM
// )
void gemm_RVV_8x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(8));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(8));
}

// gemm_RVV_8x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 8] @DRAM
// )
void gemm_RVV_8x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(8));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(8));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(8));
}

// gemm_RVV_8x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 8] @DRAM
// )
void gemm_RVV_8x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(8));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(8));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(8));
}

// gemm_RVV_8x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 8] @DRAM
// )
void gemm_RVV_8x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(8));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(8));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(8));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(8));
}

// gemm_RVV_8x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 8] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 8] @DRAM
// )
void gemm_RVV_8x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t C_reg_0;
vfloat32m1_t C_reg_1;
vfloat32m1_t C_reg_2;
vfloat32m1_t C_reg_3;
vfloat32m1_t C_reg_4;
vfloat32m1_t C_reg_5;
vfloat32m1_t C_reg_6;
vfloat32m1_t C_reg_7;
C_reg_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
vfloat32m1_t A_reg;
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  C_reg_0 = __riscv_vfmacc_vf_f32m1(C_reg_0, B.data[(k) * (B.strides[0])], A_reg,(8));
  C_reg_1 = __riscv_vfmacc_vf_f32m1(C_reg_1, B.data[(k) * (B.strides[0]) + 1], A_reg,(8));
  C_reg_2 = __riscv_vfmacc_vf_f32m1(C_reg_2, B.data[(k) * (B.strides[0]) + 2], A_reg,(8));
  C_reg_3 = __riscv_vfmacc_vf_f32m1(C_reg_3, B.data[(k) * (B.strides[0]) + 3], A_reg,(8));
  C_reg_4 = __riscv_vfmacc_vf_f32m1(C_reg_4, B.data[(k) * (B.strides[0]) + 4], A_reg,(8));
  C_reg_5 = __riscv_vfmacc_vf_f32m1(C_reg_5, B.data[(k) * (B.strides[0]) + 5], A_reg,(8));
  C_reg_6 = __riscv_vfmacc_vf_f32m1(C_reg_6, B.data[(k) * (B.strides[0]) + 6], A_reg,(8));
  C_reg_7 = __riscv_vfmacc_vf_f32m1(C_reg_7, B.data[(k) * (B.strides[0]) + 7], A_reg,(8));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7,(8));
}

// gemm_RVV_9x1_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 9] @DRAM
// )
void gemm_RVV_9x1_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
}

// gemm_RVV_9x1_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 1] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][1, 9] @DRAM
// )
void gemm_RVV_9x1_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_reg_0_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
}

// gemm_RVV_9x2_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 9] @DRAM
// )
void gemm_RVV_9x2_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
}

// gemm_RVV_9x2_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 2] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][2, 9] @DRAM
// )
void gemm_RVV_9x2_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
}

// gemm_RVV_9x3_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 9] @DRAM
// )
void gemm_RVV_9x3_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
}

// gemm_RVV_9x3_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 3] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][3, 9] @DRAM
// )
void gemm_RVV_9x3_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
}

// gemm_RVV_9x4_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 9] @DRAM
// )
void gemm_RVV_9x4_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
}

// gemm_RVV_9x4_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 4] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][4, 9] @DRAM
// )
void gemm_RVV_9x4_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
}

// gemm_RVV_9x5_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 9] @DRAM
// )
void gemm_RVV_9x5_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(1));
}

// gemm_RVV_9x5_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 5] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][5, 9] @DRAM
// )
void gemm_RVV_9x5_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(1));
}

// gemm_RVV_9x6_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 9] @DRAM
// )
void gemm_RVV_9x6_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(1));
}

// gemm_RVV_9x6_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 6] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][6, 9] @DRAM
// )
void gemm_RVV_9x6_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(1));
}

// gemm_RVV_9x7_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 9] @DRAM
// )
void gemm_RVV_9x7_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(1));
}

// gemm_RVV_9x7_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 7] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][7, 9] @DRAM
// )
void gemm_RVV_9x7_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(1));
}

// gemm_RVV_9x8_b0_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 9] @DRAM
// )
void gemm_RVV_9x8_b0_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_1_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_2_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_3_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_4_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_5_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_6_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_reg_7_0 = __riscv_vfmv_v_f_f32m1(0.0f,(8));
C_regt_0 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_1 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_2 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_3 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_4 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_5 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_6 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
C_regt_7 = __riscv_vfmv_v_f_f32m1(0.0f,(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(1));
}

// gemm_RVV_9x8_b1_col_fp32(
//     KC : size,
//     alpha : f32[1] @DRAM,
//     A : [f32][KC, 9] @DRAM,
//     B : [f32][KC, 8] @DRAM,
//     beta : f32[1] @DRAM,
//     C : [f32][8, 9] @DRAM
// )
void gemm_RVV_9x8_b1_col_fp32( void *ctxt, int_fast32_t KC, const float* alpha, struct exo_win_2f32c A, struct exo_win_2f32c B, const float* beta, struct exo_win_2f32 C ) {
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
vfloat32m1_t A_reg_0;
vfloat32m1_t A_regt;
vfloat32m1_t C_regt_0;
vfloat32m1_t C_regt_1;
vfloat32m1_t C_regt_2;
vfloat32m1_t C_regt_3;
vfloat32m1_t C_regt_4;
vfloat32m1_t C_regt_5;
vfloat32m1_t C_regt_6;
vfloat32m1_t C_regt_7;
vfloat32m1_t C_reg_0_0;
vfloat32m1_t C_reg_1_0;
vfloat32m1_t C_reg_2_0;
vfloat32m1_t C_reg_3_0;
vfloat32m1_t C_reg_4_0;
vfloat32m1_t C_reg_5_0;
vfloat32m1_t C_reg_6_0;
vfloat32m1_t C_reg_7_0;
C_reg_0_0 = __riscv_vle32_v_f32m1(&C.data[0],(8));
C_reg_1_0 = __riscv_vle32_v_f32m1(&C.data[C.strides[0]],(8));
C_reg_2_0 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0])],(8));
C_reg_3_0 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0])],(8));
C_reg_4_0 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0])],(8));
C_reg_5_0 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0])],(8));
C_reg_6_0 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0])],(8));
C_reg_7_0 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0])],(8));
C_regt_0 = __riscv_vle32_v_f32m1(&C.data[8],(1));
C_regt_1 = __riscv_vle32_v_f32m1(&C.data[C.strides[0] + 8],(1));
C_regt_2 = __riscv_vle32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8],(1));
C_regt_3 = __riscv_vle32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8],(1));
C_regt_4 = __riscv_vle32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8],(1));
C_regt_5 = __riscv_vle32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8],(1));
C_regt_6 = __riscv_vle32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8],(1));
C_regt_7 = __riscv_vle32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8],(1));
for (int_fast32_t k = 0; k < KC; k++) {
  A_reg_0 = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0])],(8));
  A_regt = __riscv_vle32_v_f32m1(&A.data[(k) * (A.strides[0]) + 8],(1));
  C_reg_0_0 = __riscv_vfmacc_vf_f32m1(C_reg_0_0, B.data[(k) * (B.strides[0])], A_reg_0,(8));
  C_reg_1_0 = __riscv_vfmacc_vf_f32m1(C_reg_1_0, B.data[(k) * (B.strides[0]) + 1], A_reg_0,(8));
  C_reg_2_0 = __riscv_vfmacc_vf_f32m1(C_reg_2_0, B.data[(k) * (B.strides[0]) + 2], A_reg_0,(8));
  C_reg_3_0 = __riscv_vfmacc_vf_f32m1(C_reg_3_0, B.data[(k) * (B.strides[0]) + 3], A_reg_0,(8));
  C_reg_4_0 = __riscv_vfmacc_vf_f32m1(C_reg_4_0, B.data[(k) * (B.strides[0]) + 4], A_reg_0,(8));
  C_reg_5_0 = __riscv_vfmacc_vf_f32m1(C_reg_5_0, B.data[(k) * (B.strides[0]) + 5], A_reg_0,(8));
  C_reg_6_0 = __riscv_vfmacc_vf_f32m1(C_reg_6_0, B.data[(k) * (B.strides[0]) + 6], A_reg_0,(8));
  C_reg_7_0 = __riscv_vfmacc_vf_f32m1(C_reg_7_0, B.data[(k) * (B.strides[0]) + 7], A_reg_0,(8));
  C_regt_0 = __riscv_vfmacc_vf_f32m1(C_regt_0, B.data[(k) * (B.strides[0])], A_regt,(1));
  C_regt_1 = __riscv_vfmacc_vf_f32m1(C_regt_1, B.data[(k) * (B.strides[0]) + 1], A_regt,(1));
  C_regt_2 = __riscv_vfmacc_vf_f32m1(C_regt_2, B.data[(k) * (B.strides[0]) + 2], A_regt,(1));
  C_regt_3 = __riscv_vfmacc_vf_f32m1(C_regt_3, B.data[(k) * (B.strides[0]) + 3], A_regt,(1));
  C_regt_4 = __riscv_vfmacc_vf_f32m1(C_regt_4, B.data[(k) * (B.strides[0]) + 4], A_regt,(1));
  C_regt_5 = __riscv_vfmacc_vf_f32m1(C_regt_5, B.data[(k) * (B.strides[0]) + 5], A_regt,(1));
  C_regt_6 = __riscv_vfmacc_vf_f32m1(C_regt_6, B.data[(k) * (B.strides[0]) + 6], A_regt,(1));
  C_regt_7 = __riscv_vfmacc_vf_f32m1(C_regt_7, B.data[(k) * (B.strides[0]) + 7], A_regt,(1));
}
__riscv_vse32_v_f32m1(&C.data[0], C_reg_0_0,(8));
__riscv_vse32_v_f32m1(&C.data[C.strides[0]], C_reg_1_0,(8));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0])], C_reg_2_0,(8));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0])], C_reg_3_0,(8));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0])], C_reg_4_0,(8));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0])], C_reg_5_0,(8));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0])], C_reg_6_0,(8));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0])], C_reg_7_0,(8));
__riscv_vse32_v_f32m1(&C.data[8], C_regt_0,(1));
__riscv_vse32_v_f32m1(&C.data[C.strides[0] + 8], C_regt_1,(1));
__riscv_vse32_v_f32m1(&C.data[(2) * (C.strides[0]) + 8], C_regt_2,(1));
__riscv_vse32_v_f32m1(&C.data[(3) * (C.strides[0]) + 8], C_regt_3,(1));
__riscv_vse32_v_f32m1(&C.data[(4) * (C.strides[0]) + 8], C_regt_4,(1));
__riscv_vse32_v_f32m1(&C.data[(5) * (C.strides[0]) + 8], C_regt_5,(1));
__riscv_vse32_v_f32m1(&C.data[(6) * (C.strides[0]) + 8], C_regt_6,(1));
__riscv_vse32_v_f32m1(&C.data[(7) * (C.strides[0]) + 8], C_regt_7,(1));
}


/* relying on the following instruction..."
rvv_broadcast_8xf32_0(dst,vl)
{dst_data} = __riscv_vfmv_v_f_f32m1(0.0f,{vl});
*/

/* relying on the following instruction..."
rvv_vfmacc_8xf32_1xf32(dst,lhs,rhs,vl)
{dst_data} = __riscv_vfmacc_vf_f32m1({dst_data}, {rhs_data}, {lhs_data},{vl});
*/

/* relying on the following instruction..."
rvv_vld_8xf32(dst,src,vl)
{dst_data} = __riscv_vle32_v_f32m1(&{src_data},{vl});
*/

/* relying on the following instruction..."
rvv_vst_8xf32(dst,src,vl)
__riscv_vse32_v_f32m1(&{dst_data}, {src_data},{vl});
*/
